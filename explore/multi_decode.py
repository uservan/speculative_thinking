import __init__
import torch
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer, Qwen2ForCausalLM
import json
from utils.data_utils import load_data,save_all_results, read_saved_results, save_results
from utils.utils import *
from tqdm import tqdm
import time
import argparse
import pickle
from generate import generate_with_partial_kv
import copy
from concurrent.futures import ThreadPoolExecutor, as_completed

def sentiment_analysis(text, positive_words, negative_words):
    positive_count = 0
    negative_count = 0
    last_pos_index = -1
    last_neg_index = -1

    text = text.lower()  # è½¬æ¢ä¸ºå°å†™ä»¥ç¡®ä¿åŒ¹é…ä¸åŒºåˆ†å¤§å°å†™
    for word in positive_words:
        if word in text:
            positive_count += text.count(word)  # ç»Ÿè®¡å‡ºç°æ¬¡æ•°
            last_pos_index = max(last_pos_index, text.rfind(word))  # è®°å½•æœ€åå‡ºç°çš„ä½ç½®

    for word in negative_words:
        if word in text:
            negative_count += text.count(word)  # ç»Ÿè®¡å‡ºç°æ¬¡æ•°
            last_neg_index = max(last_neg_index, text.rfind(word))  # è®°å½•æœ€åå‡ºç°çš„ä½ç½®

    if positive_count > negative_count:
        return 1  # æ­£å‘
    elif negative_count > positive_count:
        return -1  # è´Ÿå‘
    elif positive_count == negative_count and positive_count > 0:
        return -1 # 1 if last_pos_index > last_neg_index else -1  # å–æœ€é åçš„è¯ç±»åˆ«
    else:
        return 0  # ä¸­ç«‹

def load_train_data(choose_data_name):
    split = 'train' if choose_data_name not in ['MATH500'] else 'test'
    choose_data_name = dataset_names[choose_data_name]
    choose_data = load_data(choose_data_name, 'huggingface')
    choose_data = choose_data[split]
    if 'MATH500' in choose_data_name or 'aime' in choose_data_name:
        new_column_names = {"problem": "question"}
    choose_data = choose_data.rename_columns(new_column_names)
    return choose_data

def contains_keywords(text, keywords):
    return any(keyword in text.lower() for keyword in keywords)

def sent_judge(spe_sent, tgt_sent):
    correct_flag = False
    # if spe_sent * tgt_sent <= 0: correct_flag=True
    if spe_sent<0: 
        if spe_sent * tgt_sent <= 0: correct_flag=True
    if spe_sent >0: 
        if spe_sent * tgt_sent < 0: correct_flag=True
    return correct_flag

def speculative_generate(
    speculative_model, target_model, tokenizer, input_ids, help_think_word_ids,
    max_tokens=100, max_target_tokens=10, temperature=1.0, top_k=50, top_p=0.95
):
    try_correct_num = 0
    spec_kv=None
    tgt_kv=None
    device = input_ids.device
    prompt_len = input_ids.shape[1]
    generated_ids = input_ids  # ç›´æ¥å­˜å‚¨ token ids
    correct_tokens = []
    begin = False
    while generated_ids.shape[1] < max_tokens:  # **ä¸å†æ‰‹åŠ¨æ£€æŸ¥ max_tokens**
        if not begin:
            # **Step 1: Speculative Model é€ä¸ªç”Ÿæˆ 1 ä¸ª token**
            new_ids, spec_kv = generate_with_partial_kv(
                speculative_model, tokenizer, generated_ids, spec_kv,
                max_new_tokens=1, temperature=temperature, top_k=top_k, top_p=top_p
            )
            # **æ›´æ–° token åºåˆ—**
            generated_ids = new_ids # torch.cat([generated_ids, new_ids[:, -1:]], dim=-1)
            # **Step 2: ä»…è§£ç æ–° token å¹¶æ£€æŸ¥æ˜¯å¦è§¦å‘ Target Model**
            decoded_text = tokenizer.decode(new_ids[0, -1:], skip_special_tokens=True)
            
        if begin or any(trigger in decoded_text for trigger in TRIGGER_TOKENS):
            if begin or help_think_word_ids is None:
                cache_generated_ids = generated_ids
            else:
                cache_generated_ids = torch.cat([generated_ids, help_think_word_ids], dim=-1)

            spe_new_ids, spec_kv_candidate = generate_with_partial_kv(
                speculative_model, tokenizer, cache_generated_ids, copy.deepcopy(spec_kv),
                max_new_tokens=max_target_tokens, temperature=temperature, top_k=top_k, top_p=top_p
            )
            spe_decoded_text = tokenizer.decode(spe_new_ids[0,-max_target_tokens:], skip_special_tokens=True)
            spe_sent = sentiment_analysis(spe_decoded_text, TARGET_VALIDATION_KEYWORDS['positive'], TARGET_VALIDATION_KEYWORDS['negative'])
            if spe_sent != 0:
                try_correct_num = try_correct_num+1
                # **Step 3: ç›®æ ‡æ¨¡å‹ç”Ÿæˆ max_target_tokens ä¸ª token**
                tgt_new_ids, tgt_kv_candidate = generate_with_partial_kv(
                    target_model, tokenizer, cache_generated_ids, copy.deepcopy(tgt_kv),
                    max_new_tokens=max_target_tokens, temperature=temperature, top_k=top_k, top_p=top_p
                )
                # **è§£ç  Target Model ç”Ÿæˆçš„æ–‡æœ¬**
                tgt_decoded_text = tokenizer.decode(tgt_new_ids[0,-max_target_tokens:], skip_special_tokens=True)
                if tgt_decoded_text != spe_decoded_text:
                    tgt_sent = sentiment_analysis(tgt_decoded_text, TARGET_VALIDATION_KEYWORDS['positive'], TARGET_VALIDATION_KEYWORDS['negative'])
                    correct_flag = sent_judge(spe_sent, tgt_sent)

                if correct_flag:
                    correct_tokens.append({
                        'pos': cache_generated_ids.shape[1]-prompt_len, 
                        'traget':tgt_decoded_text, 'speculative':spe_decoded_text})
                    generated_ids = tgt_new_ids # torch.cat([cache_generated_ids, tgt_new_ids[:, :]], dim=-1)  # âœ… æ¥å— Target Model ç»“æœ
                    tgt_kv = tgt_kv_candidate  # âœ… åªæœ‰åœ¨æ¥å— Target Model ç»“æœæ—¶æ‰æ›´æ–° `tgt_kv`
                else:
                    generated_ids = spe_new_ids
                    spec_kv = spec_kv_candidate
            begin = False
        
        if tgt_kv is not None and tgt_kv[0][0].shape[2] > len(generated_ids[0]):
            print("wrong")
            
        # **Step 5: ç»ˆæ­¢æ¡ä»¶**
        # last_token_id = generated_ids[0, -1].item()
        if tokenizer.eos_token_id in generated_ids[0, -max_target_tokens:].tolist():  # âœ… è®© `generate()` å¤„ç†ç»ˆæ­¢
            break

    # **æœ€ç»ˆè§£ç æ–‡æœ¬**
    generated_text = tokenizer.decode(generated_ids[0,prompt_len:], skip_special_tokens=True)
    return generated_text, len(generated_ids[0])-prompt_len, correct_tokens, try_correct_num


def parse_args(args=None):
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset', type=str, default='MATH500')
    parser.add_argument('--target_model', type=str, default='deepseek-7b')
    parser.add_argument('--speculative_model', type=str, default='deepseek-1.5b') 
    parser.add_argument('--speculative_k', type=int, default=15)
    parser.add_argument('--accept_prob', type=float, default=0.01)
    parser.add_argument('--temperature', type=float, default=0.6)
    parser.add_argument('--topp', type=float, default=0.95)
    parser.add_argument('--topk', type=float, default=0)
    parser.add_argument('--max_tokens', type=int, default=1024*32)
    parser.add_argument('--seed', type=int, default=42)
    return parser.parse_args(args)

dataset_names = {
    'MATH500': "qq8933/MATH500",
    'AIME': "AI-MO/aimo-validation-aime"
}

models_names = {
    'deepseek-32b': "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
    'deepseek-1.5b': "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
     'deepseek-7b': "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
    'Qwen-math-1.5b':'Qwen/Qwen2.5-Math-1.5B'
}


if __name__ == '__main__':
    args = parse_args()
    seed_everything(args.seed)
    # accept_prob = args.accept_prob
    # # **åŠ è½½ Hugging Face æ¨¡å‹ï¼ˆæ”¯æŒ KV Cache + å¤š GPUï¼‰**
    # target_model_name = models_names[args.target_model]  
    # target_model = AutoModelForCausalLM.from_pretrained(
    #     target_model_name, torch_dtype=torch.float16,device_map="auto", low_cpu_mem_usage=True, attn_implementation="flash_attention_2",
    # )
    # tokenizer = AutoTokenizer.from_pretrained(target_model_name)
    # speculative_model = None
    # if args.speculative_model:
    #     speculative_model_name = models_names[args.speculative_model] 
    #     speculative_model = AutoModelForCausalLM.from_pretrained(
    #         speculative_model_name,  torch_dtype=torch.float16, device_map="auto", low_cpu_mem_usage=True,attn_implementation="flash_attention_2",
    #     )
   

# **è§¦å‘ Token & å…³é”®è¯**
TRIGGER_TOKENS = {"\n\n"}  # é‡åˆ°è¿™äº› token è§¦å‘ Target Model
TARGET_VALIDATION_KEYWORDS = {"negative":["wait", "alternatively", 'hold on','another', 'double-check','verify', 'think again', 'recap'],
                              "positive":['yeah','yes', 'alright','final answer']}  # ç›®æ ‡å…³é”®å­—
help_think_word = None # '\n\n'
system_prompt = None
# è®¾å¤‡é€‰æ‹©
device = "cuda" if torch.cuda.is_available() else "cpu"

# åŠ è½½ Tokenizer å’Œæ¨¡å‹
target_model_name = models_names[args.target_model]
speculative_model_name = models_names[args.speculative_model]

tokenizer = AutoTokenizer.from_pretrained(target_model_name)
if tokenizer.pad_token_id is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id

target_model = AutoModelForCausalLM.from_pretrained(target_model_name, torch_dtype=torch.float16, device_map="auto")
speculative_model = AutoModelForCausalLM.from_pretrained(speculative_model_name, torch_dtype=torch.float16, device_map="auto")

help_think_word_ids = None if help_think_word is None else tokenizer([help_think_word], return_tensors="pt").input_ids.to(device)

datasets = args.dataset.split(',')
for dataset in datasets:
    math500_dataset = load_train_data(dataset)
    output_file = f"./results/{dataset}_{args.target_model}_{args.speculative_model}_3.json"

    results = read_saved_results(output_file)
    start_idx = len(results)  # ä»æœªå®Œæˆéƒ¨åˆ†ç»§ç»­
    remaining_data = math500_dataset.select(range(start_idx, len(math500_dataset)))

    def process_sample(idx, sample):
        """å¤„ç†å•ä¸ªæ ·æœ¬ï¼Œå¹¶è¿”å›ç´¢å¼•ï¼Œç¡®ä¿é¡ºåº"""
        question, answer = sample["question"], sample["answer"]
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({
            "role": "user",
            "content": "Please reason step by step, and put your final answer within \\boxed{{}}. " + question + ' <think>\n'
        })
        input_ids = tokenizer.apply_chat_template(messages, return_tensors="pt").to(device)

        start_time = time.time()
        generated_text, num_tokens, correct_tokens, try_correct_num = speculative_generate(
            speculative_model, target_model, tokenizer, input_ids, help_think_word_ids,
            max_target_tokens=args.speculative_k, max_tokens=args.max_tokens
        )
        end_time = time.time()

        return {
            "index": idx,  # è®°å½•åŸå§‹ç´¢å¼•ï¼Œç¡®ä¿æ’åº
            "question": question,
            "generated_text": generated_text,
            "answer": answer,
            "corrected_tokens": correct_tokens,
            "generation_time": end_time - start_time,
            "num_tokens": num_tokens,
            "try_correct_num": try_correct_num
        }

    # çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
    max_workers = min(4, os.cpu_count())  # é™åˆ¶æœ€å¤§çº¿ç¨‹æ•°ï¼Œé˜²æ­¢è¶…è½½
    print(f"ğŸš€ è‡ªåŠ¨åˆ†é…çº¿ç¨‹æ•°: {max_workers}")
    # max_workers = 8  # æ ¹æ®æ˜¾å­˜æƒ…å†µè°ƒæ•´
    results_list = []  # ç”¨äºå­˜å‚¨è¿”å›çš„ future ç»“æœ
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(process_sample, idx, sample): idx for idx, sample in enumerate(remaining_data)}

        for future in tqdm(as_completed(futures), total=len(futures)):
            try:
                result = future.result()
                results_list.append(result)  # å…ˆå­˜å‚¨ï¼Œä¿è¯ç»“æœå®Œæ•´
            except Exception as e:
                print(f"Error processing sample {futures[future]}: {e}")

    # **ç¡®ä¿æœ€ç»ˆç»“æœæŒ‰ç´¢å¼•æ’åº**
    results_list = sorted(results_list, key=lambda x: x["index"])

    # **æŒ‰é¡ºåºå­˜å…¥ results å¹¶ä¿å­˜**
    for res in results_list:
        results.append(res)
        save_results(output_file, res)

print("æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")