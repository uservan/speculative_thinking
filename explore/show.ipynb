{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wxy320/miniconda3/envs/multio1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import __init__\n",
    "from utils.data_utils import read_saved_results\n",
    "from utils.qwen_math_parser import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_math_correctness(ref, generation):\n",
    "    if not find_box(generation): return False\n",
    "    answer = strip_answer_string(ref)\n",
    "    pred = extract_answer(generation)\n",
    "    pred = strip_answer_string(pred)\n",
    "    return math_equal(pred, answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek_32b_path = '/home/wxy320/ondemand/program/speculative_thinking/results/MATH500_deepseek-32b_None_0.01.json'\n",
    "deepseek_1b_path = '/home/wxy320/ondemand/program/speculative_thinking/results/MATH500_deepseek-1.5b_None_0.01.json'\n",
    "\n",
    "results_32b = read_saved_results(deepseek_32b_path)\n",
    "results_1b = read_saved_results(deepseek_1b_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 统计词汇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词汇分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "\n",
    "# 目标关键词\n",
    "keywords = {\"wait\", \"hmm\", \"alternatively\"}\n",
    "\n",
    "# 存储关键词前 3 个单词\n",
    "collected_words = {keyword: Counter() for keyword in keywords}\n",
    "\n",
    "# 遍历所有结果\n",
    "for result in results_32b:\n",
    "    text = result['generated_answer'].lower()  # 直接处理原始文本\n",
    "    \n",
    "    # 以关键词进行分割\n",
    "    for keyword in keywords:\n",
    "        parts = re.split(rf'\\b{keyword}\\b', text, flags=re.IGNORECASE)  # \\b 确保匹配完整单词\n",
    "        # 遍历找到的分割点\n",
    "        for i in range(1, len(parts)):  # 从1开始，因为0是文本开头\n",
    "            before_keyword = parts[i - 1]  # 关键词前的文本\n",
    "            token_ids = tokenizer.encode(before_keyword)\n",
    "            decoded_tokens = [tokenizer.decode([t]) for t in token_ids]\n",
    "            \n",
    "            # 取最后 3 个 token 进行统计\n",
    "            for token in decoded_tokens[-2:]:\n",
    "                collected_words[keyword][token] += 1  # 统计出现次数\n",
    "\n",
    "# # **转换 Counter 为普通字典**，方便 JSON 序列化\n",
    "# collected_words_dict = {keyword: dict(counts) for keyword, counts in collected_words.items()}\n",
    "\n",
    "# # **写入 JSON 文件**\n",
    "# output_file = \"collected_tokens_1b.json\"\n",
    "# with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(collected_words_dict, f, indent=4, ensure_ascii=False)  # `ensure_ascii=False` 处理非 ASCII 字符\n",
    "\n",
    "# print(f\"统计结果已保存至 {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait :  Counter({'<｜begin▁of▁sentence｜>': 4103})\n",
      "hmm:  Counter({'<｜begin▁of▁sentence｜>': 1195})\n",
      "alternatively:  Counter({'<｜begin▁of▁sentence｜>': 1763})\n"
     ]
    }
   ],
   "source": [
    "print('wait : ',collected_words['\\n\\n'])\n",
    "print('hmm: ',collected_words['hmm'])\n",
    "print('alternatively: ', collected_words['alternatively'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储 `\\n\\n` 后 3 个 token 的统计\n",
    "collected_tokens = Counter()\n",
    "\n",
    "# 遍历所有结果\n",
    "for result in results_1b:\n",
    "    text = result['generated_answer'].strip()  # 处理原始文本\n",
    "\n",
    "    # 按 `\\n\\n` 进行分割\n",
    "    sections = re.split(r'\\n\\n+', text)  # `\\n\\n+` 允许匹配多个连续的 `\\n\\n`\n",
    "\n",
    "    # 遍历每个段落，取其后三个 token\n",
    "    for i in range(1, len(sections)):  # 从 1 开始，确保有前一段落\n",
    "        after_break = sections[i].strip()  # `\\n\\n` 后的文本\n",
    "        token_ids = tokenizer.encode(after_break, add_special_tokens=False)  # 仅编码文本部分\n",
    "        decoded_tokens = [tokenizer.decode([t]).lower() for t in token_ids]\n",
    "\n",
    "        # 取 `\\n\\n` 后的 3 个 token 并统计\n",
    "        for token in decoded_tokens[:1]:  # 取前 3 个 token\n",
    "            collected_tokens[token] += 1  # 统计出现次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sorted_tokens = dict(sorted(\n",
    "    {k: t for k, t in collected_tokens.items() if t > 100}.items(), \n",
    "    key=lambda item: item[1],  # 依据 t 进行排序\n",
    "    reverse=True  # 倒序排序\n",
    "))\n",
    "filtered_sorted_tokens\n",
    "\n",
    "# **写入 JSON 文件**\n",
    "output_file = \"collected_tokens_nn_1b.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filtered_sorted_tokens, f, indent=4, ensure_ascii=False)  # `ensure_ascii=False` 处理非 ASCII 字符"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计thoughts数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a test.',\n",
       " 'Wait,Here comes the next part.\\n\\nThis should not be split.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'This is a test.\\n\\nWait,Here comes the next part.\\n\\nThis should not be split.'.strip()  # 处理原始文本\n",
    "\n",
    "keywords = {\"wait\", \"hmm\", \"alternately\", 'yes', 'yeah'}\n",
    "\n",
    "# **按 `\\n\\n` 进行初步分割**\n",
    "sections = re.split(r'\\n\\n+', text)\n",
    "\n",
    "# **存储最终分割的部分**\n",
    "filtered_sections = []\n",
    "temp_section = \"\"\n",
    "\n",
    "for section in sections:\n",
    "    last_20_chars = section[:20].lower()  # 取后 20 个字符并转换为小写\n",
    "    # print(last_20_chars)\n",
    "    if any(keyword in last_20_chars for keyword in keywords):\n",
    "        # 如果当前 `temp_section` 不是空的，则合并存储\n",
    "        if temp_section:\n",
    "            filtered_sections.append(temp_section.strip())\n",
    "        temp_section = section+ \"\\n\\n\"\n",
    "    else:\n",
    "        # 累积无关键词的部分，等待下次匹配\n",
    "        temp_section += section + \"\\n\\n\"\n",
    "\n",
    "# **如果最后 `temp_section` 仍有内容，加入最终结果**\n",
    "if temp_section:\n",
    "    filtered_sections.append(temp_section.strip())\n",
    "filtered_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "\n",
    "# 目标关键词\n",
    "keywords = {\"wait\", \"hmm\", \"alternatively\", 'yes', 'yeah', '**final', 'double-check'}\n",
    "\n",
    "def segement(results_1b):\n",
    "    filtered_sections_all = []\n",
    "    for result in results_1b:\n",
    "        text = result['generated_answer'].strip()  # 处理原始文本\n",
    "\n",
    "        # **按 `\\n\\n` 进行初步分割**\n",
    "        sections = re.split(r'\\n\\n+', text)\n",
    "\n",
    "        # **存储最终分割的部分**\n",
    "        filtered_sections = []\n",
    "        temp_section = \"\"\n",
    "\n",
    "        for section in sections:\n",
    "            last_20_chars = section[:30].lower()  # 取后 20 个字符并转换为小写\n",
    "            # print(last_20_chars)\n",
    "            if any(keyword in last_20_chars for keyword in keywords):\n",
    "                # 如果当前 `temp_section` 不是空的，则合并存储\n",
    "                if temp_section:\n",
    "                    filtered_sections.append(temp_section.strip())\n",
    "                temp_section = section+ \"\\n\\n\"\n",
    "            else:\n",
    "                # 累积无关键词的部分，等待下次匹配\n",
    "                temp_section += section + \"\\n\\n\"\n",
    "\n",
    "        # **如果最后 `temp_section` 仍有内容，加入最终结果**\n",
    "        if temp_section:\n",
    "            filtered_sections.append(temp_section.strip())\n",
    "        # print(filtered_sections)\n",
    "        filtered_sections_all.append({\n",
    "            'right': check_math_correctness(result['answer'], result['generated_answer']), \n",
    "            'question': result['question'],\n",
    "            'filtered_sections':filtered_sections,\n",
    "            'num_tokens': result['num_tokens']\n",
    "        })\n",
    "        # break\n",
    "\n",
    "    return filtered_sections_all\n",
    "\n",
    "def avg_l(s):\n",
    "    return sum(s)/len(s) if len(s)>0 else 0\n",
    "\n",
    "def avg(s):\n",
    "    if len(s) == 0:\n",
    "        return 0, 0  # 避免除零错误\n",
    "    \n",
    "    mean = sum(s) / len(s)  # 计算均值\n",
    "    variance = sum((x - mean) ** 2 for x in s) / len(s)  # 计算方差（无偏估计可用 n-1）\n",
    "    std_dev = math.sqrt(variance)  # 计算标准差\n",
    "    \n",
    "    return mean, std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'right': True,\n",
       "  'question': 'Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\\\theta),$ where $r > 0$ and $0 \\\\le \\\\theta < 2 \\\\pi.$',\n",
       "  'filtered_sections': ['Okay, so I need to convert the rectangular coordinates (0, 3) to polar coordinates. Hmm, I remember that polar coordinates are represented as (r, θ), where r is the distance from the origin to the point, and θ is the angle made with the positive x-axis. \\n\\nFirst, I should recall the formulas for converting from rectangular (x, y) to polar (r, θ). I think the formulas are:\\n- r = √(x² + y²)\\n- θ = arctan(y/x)\\n\\nLet me write that down to keep track:\\nr = √(x² + y²)\\nθ = arctan(y/x)\\n\\nAlright, so given the point (0, 3), x is 0 and y is 3. Let me plug these into the formulas.\\n\\nStarting with r:\\nr = √(0² + 3²) = √(0 + 9) = √9 = 3\\n\\nSo, r is 3. That seems straightforward.\\n\\nNow, for θ:\\nθ = arctan(y/x) = arctan(3/0)',\n",
       "   \"Wait, dividing by zero is undefined. Hmm, arctan(3/0) is arctan(∞). I remember that as x approaches 0 from the positive side, y/x approaches positive infinity, and as x approaches 0 from the negative side, y/x approaches negative infinity. But since our point is (0, 3), x is 0 and y is positive, so we are in the positive y-axis direction.\\n\\nI think arctan(∞) is π/2. Because when the angle is π/2, it's straight up along the y-axis. So, θ should be π/2.\",\n",
       "   'Let me double-check that. So, when x is 0 and y is positive, the point is on the positive y-axis, which is 90 degrees or π/2 radians. That makes sense.\\n\\nSo, putting it all together, the polar coordinates should be (3, π/2).',\n",
       "   \"Wait, but just to make sure I didn't miss anything, let me visualize this. In rectangular coordinates, (0, 3) is 3 units up along the y-axis. In polar coordinates, that should be 3 units from the origin, making a 90-degree angle with the positive x-axis. Yeah, that seems right.\\n\\nI was a bit confused earlier when I saw arctan(3/0), but now I know that 3/0 is infinity, and arctan(∞) is π/2. So, θ is π/2. \\n\\nIs there another way to think about this? Maybe using the unit circle? On the unit circle, (0, 1) corresponds to π/2. So, that point is (0, 1) in polar coordinates, which is (1, π/2). But in our case, the radius is 3, so it's just scaled up by 3. So, (3, π/2) is indeed correct.\\n\\nI don't think I made any mistakes here. I converted the x and y values correctly, calculated r, and figured out the angle θ based on the position of the point. So, yeah, the polar coordinates should be (3, π/2).\",\n",
       "   '**Final Answer**\\nThe polar coordinates are \\\\boxed{(3, \\\\frac{\\\\pi}{2})}.\\n</think>\\n\\nTo convert the rectangular coordinates \\\\((0, 3)\\\\) to polar coordinates, we use the following formulas:\\n- \\\\( r = \\\\sqrt{x^2 + y^2} \\\\)\\n- \\\\( \\\\theta = \\\\arctan\\\\left(\\\\frac{y}{x}\\\\right) \\\\)\\n\\nGiven the point \\\\((0, 3)\\\\), we have \\\\( x = 0 \\\\) and \\\\( y = 3 \\\\).\\n\\nFirst, we calculate \\\\( r \\\\):\\n\\\\[\\nr = \\\\sqrt{0^2 + 3^2} = \\\\sqrt{0 + 9} = \\\\sqrt{9} = 3\\n\\\\]\\n\\nNext, we calculate \\\\( \\\\theta \\\\):\\n\\\\[\\n\\\\theta = \\\\arctan\\\\left(\\\\frac{3}{0}\\\\right)\\n\\\\]\\nSince \\\\( \\\\frac{3}{0} \\\\) is undefined and approaches infinity, we know that \\\\( \\\\arctan(\\\\infty) = \\\\frac{\\\\pi}{2} \\\\). Given that the point \\\\((0, 3)\\\\) is on the positive y-axis, the angle is \\\\( \\\\frac{\\\\pi}{2} \\\\).\\n\\nThus, the polar coordinates are \\\\((3, \\\\frac{\\\\pi}{2})\\\\).\\n\\n\\\\[\\n\\\\boxed{(3, \\\\frac{\\\\pi}{2})}\\n\\\\]'],\n",
       "  'num_tokens': 993},\n",
       " 500,\n",
       " {'right': True,\n",
       "  'question': 'Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\\\theta),$ where $r > 0$ and $0 \\\\le \\\\theta < 2 \\\\pi.$',\n",
       "  'filtered_sections': [\"Okay, so I need to convert the rectangular coordinate (0, 3) to polar coordinates. Hmm, I remember that polar coordinates are represented as (r, θ), where r is the distance from the origin and θ is the angle made with the positive x-axis. Let me try to recall the formulas for the conversion.\\n\\nFirst, the formula for r is the square root of (x squared plus y squared). So, r = √(x² + y²). In this case, x is 0 and y is 3. Plugging those in, I get r = √(0² + 3²) which simplifies to √(0 + 9) = √9 = 3. Okay, so r is 3. That part seems straightforward.\\n\\nNow, for θ, I think the formula is θ = arctangent of (y over x), right? So θ = arctan(y/x). But wait, in this case, x is 0. So, arctan(3/0). Hmm, division by zero. That's undefined. I remember that when x is 0, the point lies on the y-axis, so the angle θ should be either π/2 or 3π/2 depending on the sign of y.\\n\\nSince y is positive here (y = 3), the point is on the positive y-axis. Therefore, the angle θ should be π/2 radians, which is 90 degrees. Let me confirm that. If θ is π/2, then the polar coordinates would be (3, π/2). Does that make sense?\\n\\nLet me visualize the point (0, 3) in the rectangular system. It's located three units above the origin on the y-axis. So, in polar coordinates, it should be 3 units away from the origin at an angle of π/2. Yeah, that seems correct.\",\n",
       "   \"Wait, just to make sure I didn't make a mistake. Sometimes, when dealing with arctangent, especially when x is zero, it's easy to get confused. But since the point is on the y-axis, the angle is either 90 degrees or 270 degrees. Since y is positive, it's 90 degrees, which is π/2 radians. So, I think I'm confident with θ = π/2.\\n\\nSo putting it all together, the polar coordinates are (3, π/2). Let me write that down: (3, π/2). I don't think I need to adjust anything else because r is positive, which is required, and θ is between 0 and 2π, which π/2 satisfies.\",\n",
       "   \"Just to double-check, if I convert back from polar to rectangular coordinates, does it give me (0, 3)? The conversion formulas are x = r cos θ and y = r sin θ. So, x = 3 cos(π/2) and y = 3 sin(π/2). Cos(π/2) is 0, so x is 0. Sin(π/2) is 1, so y is 3. Perfect, that brings us back to (0, 3). So, the conversion is correct.\\n\\nI don't think I missed anything here. The key was recognizing that when x is zero, the angle is either π/2 or 3π/2, and determining the correct one based on the sign of y. Since y is positive, it's π/2. If y were negative, it would be 3π/2. But in this case, it's positive, so θ is π/2.\\n\\nTherefore, the polar coordinates for (0, 3) are (3, π/2). I feel confident about this answer after going through the steps and double-checking the conversion back to rectangular coordinates.\",\n",
       "   '**Final Answer**\\nThe polar coordinates are \\\\boxed{(3, \\\\frac{\\\\pi}{2})}.\\n</think>\\n\\nTo convert the rectangular coordinate \\\\((0, 3)\\\\) to polar coordinates, we start by calculating \\\\(r\\\\) and \\\\(\\\\theta\\\\).\\n\\n1. The formula for \\\\(r\\\\) is:\\n   \\\\[\\n   r = \\\\sqrt{x^2 + y^2}\\n   \\\\]\\n   Substituting \\\\(x = 0\\\\) and \\\\(y = 3\\\\):\\n   \\\\[\\n   r = \\\\sqrt{0^2 + 3^2} = \\\\sqrt{9} = 3\\n   \\\\]\\n\\n2. The formula for \\\\(\\\\theta\\\\) is:\\n   \\\\[\\n   \\\\theta = \\\\arctan\\\\left(\\\\frac{y}{x}\\\\right)\\n   \\\\]\\n   Since \\\\(x = 0\\\\), the point lies on the y-axis. Given \\\\(y\\\\) is positive, the angle \\\\(\\\\theta\\\\) is \\\\(\\\\frac{\\\\pi}{2}\\\\).\\n\\nThus, the polar coordinates are \\\\((3, \\\\frac{\\\\pi}{2})\\\\).\\n\\nTo verify, converting back to rectangular coordinates using \\\\(x = r \\\\cos \\\\theta\\\\) and \\\\(y = r \\\\sin \\\\theta\\\\) gives:\\n   \\\\[\\n   x = 3 \\\\cos\\\\left(\\\\frac{\\\\pi}{2}\\\\right) = 0\\n   \\\\]\\n   \\\\[\\n   y = 3 \\\\sin\\\\left(\\\\frac{\\\\pi}{2}\\\\right) = 3\\n   \\\\]\\nwhich confirms the original point.\\n\\nTherefore, the polar coordinates are \\\\(\\\\boxed{(3, \\\\frac{\\\\pi}{2})}\\\\).'],\n",
       "  'num_tokens': 1151},\n",
       " 500)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sections_all_1b = segement(results_1b)\n",
    "filtered_sections_all_32b = segement(results_32b)\n",
    "filtered_sections_all_1b[0], len(filtered_sections_all_1b), filtered_sections_all_32b[0], len(filtered_sections_all_32b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19905 > 16384). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import __init__\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Qwen2ForCausalLM\n",
    "target_model_name = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-32B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(target_model_name)\n",
    "\n",
    "avg_s, avg_s_l,num_tokens = {'right': [[], []], 'wrong': [[], []]}, {'right': [[], []], 'wrong': [[], []]},{'right': [[], []], 'wrong': [[], []]}\n",
    "\n",
    "for s_1 in filtered_sections_all_1b:\n",
    "    key1 = 'right' if s_1['right'] else 'wrong'\n",
    "    # 统计 filtered_sections 的数量\n",
    "    avg_s[key1][0].append(len(s_1['filtered_sections']))\n",
    "   \n",
    "    # # **使用 tokenizer 计算 token 个数**\n",
    "    token_count_1b = [len(tokenizer.encode(ss, add_special_tokens=False)) for ss in s_1['filtered_sections']]\n",
    "   \n",
    "    # 统计 filtered_sections 的 token 数量\n",
    "    avg_s_l[key1][0].append(avg_l(token_count_1b)) # s_1['num_tokens']/avg_s[key1][0][-1]\n",
    "    num_tokens[key1][0].append(s_1['num_tokens'])\n",
    "   \n",
    "for s_32 in filtered_sections_all_32b:\n",
    "    key32 = 'right' if s_32['right'] else 'wrong'\n",
    "\n",
    "    # 统计 filtered_sections 的数量\n",
    "    avg_s[key32][1].append(len(s_32['filtered_sections']))\n",
    "\n",
    "    # # **使用 tokenizer 计算 token 个数**\n",
    "    token_count_32b = [len(tokenizer.encode(ss, add_special_tokens=False)) for ss in s_32['filtered_sections']]\n",
    "\n",
    "    # 统计 filtered_sections 的 token 数量\n",
    "    avg_s_l[key32][1].append(avg_l(token_count_32b)) # s_32['num_tokens']/avg_s[key32][1][-1]\n",
    "    num_tokens[key32][1].append(s_32['num_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of thoughts\n",
      "right: 1.5b: [5, 12, 10, 5, 24, 7, 9, 20, 6, 15, 13, 14, 8, 17, 21, 17, 7, 21, 9, 7, 20, 11, 10, 9, 39, 4, 70, 15, 6, 10, 7, 7, 10, 4, 3, 4, 3, 14, 8, 7, 11, 28, 8, 4, 2, 3, 3, 5, 29, 8, 3, 10, 15, 13, 6, 6, 38, 12, 5, 5, 37, 5, 3, 4, 22, 10, 12, 9, 6, 12, 36, 5, 5, 8, 3, 41, 5, 6, 4, 48, 6, 115, 6, 20, 11, 10, 76, 8, 5, 7, 7, 17, 3, 29, 4, 3, 8, 6, 5, 60, 3, 10, 22, 8, 6, 44, 11, 62, 16, 6, 4, 4, 3, 9, 40, 80, 71, 5, 18, 9, 10, 31, 10, 72, 22, 7, 23, 59, 132, 10, 15, 9, 9, 17, 16, 69, 64, 65, 5, 123, 5, 4, 30, 1, 12, 23, 3, 14, 51, 145, 4, 5, 13, 7, 6, 17, 46, 8, 4, 7, 23, 25, 11, 5, 4, 47, 18, 4, 19, 7, 6, 21, 4, 7, 2, 22, 13, 15, 9, 16, 4, 6, 22, 5, 16, 130, 32, 42, 4, 8, 15, 7, 5, 104, 7, 12, 9, 8, 1, 16, 48, 6, 4, 7, 4, 4, 36, 6, 3, 10, 7, 10, 5, 5, 14, 5, 10, 4, 4, 17, 11, 14, 14, 13, 13, 7, 20, 10, 8, 3, 4, 9, 5, 25, 55, 58, 12, 16, 8, 9, 8, 6, 69, 35, 7, 15, 8, 18, 8, 6, 13, 186, 27, 21, 4, 5, 30, 7, 5, 103, 5, 23, 4, 4, 20, 17, 17, 4, 10, 16, 113, 80, 4, 11, 13, 8, 5, 5, 8, 6, 7, 6, 4, 4, 10, 57, 12, 111, 39, 10, 5, 20, 5, 36, 7, 11, 15, 12, 18, 10, 13, 10, 17, 7, 16, 106, 94, 8, 4, 4, 8, 14, 66, 15, 9, 3, 19, 10, 8, 18, 7, 10, 6, 40, 19, 192, 3, 2, 14, 29, 5, 20, 1, 6, 111, 8, 6, 7, 9, 4, 4, 13, 54, 6, 9, 11, 30, 8, 4, 5, 6, 24, 5, 4, 42, 27, 32, 4, 10, 6, 9, 8, 9, 10, 10, 40, 6, 10, 9, 19, 6, 9, 5, 8, 37, 7, 8, 19, 76, 18, 18, 3, 6, 6, 41, 10, 6, 32, 9, 4, 4, 20, 6, 31, 21, 6, 21, 6, 70, 10, 9, 4, 27, 3, 35, 5, 5, 80, 26, 32], 32b: [4, 17, 5, 3, 6, 4, 12, 11, 3, 75, 35, 17, 12, 4, 6, 18, 8, 28, 12, 8, 31, 6, 4, 9, 21, 41, 4, 7, 3, 12, 4, 11, 3, 12, 8, 8, 6, 4, 40, 5, 30, 5, 4, 16, 3, 9, 8, 7, 5, 7, 3, 4, 4, 7, 3, 17, 5, 11, 9, 12, 4, 16, 5, 7, 5, 24, 23, 8, 4, 8, 3, 11, 5, 4, 7, 5, 34, 6, 38, 11, 3, 6, 7, 5, 11, 4, 12, 6, 8, 6, 4, 3, 3, 171, 22, 5, 96, 9, 4, 4, 8, 9, 8, 4, 6, 6, 3, 5, 4, 6, 16, 88, 25, 4, 4, 90, 3, 7, 3, 14, 20, 6, 10, 4, 3, 5, 43, 5, 52, 14, 4, 4, 15, 7, 5, 16, 10, 107, 6, 4, 10, 42, 63, 23, 9, 15, 9, 32, 5, 10, 8, 6, 25, 317, 8, 131, 6, 44, 3, 18, 6, 5, 9, 7, 13, 17, 8, 5, 7, 6, 7, 4, 6, 20, 3, 5, 4, 16, 11, 10, 4, 6, 3, 13, 9, 6, 20, 4, 7, 4, 17, 7, 3, 4, 6, 10, 48, 4, 4, 23, 7, 3, 7, 2, 18, 8, 7, 33, 30, 16, 6, 17, 3, 6, 9, 9, 3, 37, 3, 4, 6, 32, 5, 6, 30, 18, 5, 14, 10, 7, 6, 16, 7, 5, 20, 13, 5, 9, 4, 5, 4, 7, 4, 3, 7, 3, 3, 21, 8, 5, 7, 20, 7, 6, 11, 4, 11, 16, 3, 9, 3, 4, 8, 8, 65, 6, 3, 8, 41, 12, 5, 8, 7, 6, 13, 5, 6, 16, 24, 5, 16, 5, 26, 178, 40, 6, 5, 11, 50, 7, 4, 3, 4, 6, 8, 4, 25, 6, 10, 8, 4, 39, 9, 16, 9, 24, 5, 6, 6, 14, 24, 4, 8, 4, 9, 8, 4, 5, 5, 4, 5, 10, 3, 7, 7, 49, 7, 28, 23, 6, 8, 13, 8, 11, 16, 6, 5, 8, 7, 6, 3, 14, 13, 6, 8, 4, 16, 24, 5, 4, 7, 6, 15, 14, 7, 120, 14, 5, 6, 40, 6, 13, 8, 9, 5, 11, 6, 47, 10, 13, 2, 12, 9, 14, 3, 7, 11, 6, 16, 7, 3, 5, 10, 3, 6, 5, 8, 5, 4, 11, 31, 31, 7, 45, 5, 3, 12, 8, 5, 10, 31, 7, 11, 4, 13, 4, 6, 16, 8, 7, 9, 5, 31, 12, 5, 4, 14, 10, 11, 5, 5, 10, 5, 4, 10, 5, 7, 27, 5, 6, 7, 23, 21, 8, 7, 6, 2, 6, 9, 5, 16, 4, 10, 18, 4, 8, 17, 9, 8, 9, 31, 46, 13, 8, 15, 98, 6, 4, 79, 15, 5, 5, 33, 19, 22]\n",
      "wrong: 1.5b: [318, 7, 2, 106, 26, 390, 13, 159, 169, 29, 35, 75, 125, 94, 8, 699, 121, 107, 115, 142, 380, 13, 57, 108, 27, 100, 213, 620, 5, 6, 2, 214, 7, 43, 133, 2, 58, 31, 72, 432, 74, 17, 291, 29, 25, 178, 114, 31, 9, 86, 172, 1336, 14, 4, 20, 151, 23, 141, 4, 198, 43, 133, 119, 98, 121, 37, 11, 189, 3, 2, 233, 1, 86, 1244, 12, 131, 234, 18, 26, 477, 3, 92, 54, 171, 332, 163, 15, 181, 1, 38], 32b: [107, 96, 6, 10, 55, 86, 7, 103, 75, 1357, 208, 9, 136, 5, 55, 139, 1507, 11, 10, 32, 101, 3, 30, 72, 64, 8, 3, 7, 107, 76, 5, 101, 196, 5, 42]\n",
      "length of thoughts\n",
      "right: 1.5b: [198.4, 266.5833333333333, 208.6, 302.8, 191.41666666666666, 180.14285714285714, 249.44444444444446, 147.85, 277.8333333333333, 195.2, 247.0, 119.5, 329.0, 151.1764705882353, 242.0952380952381, 285.94117647058823, 281.2857142857143, 281.04761904761904, 269.3333333333333, 292.85714285714283, 207.55, 177.27272727272728, 210.9, 202.88888888888889, 117.7948717948718, 276.25, 95.81428571428572, 182.2, 335.3333333333333, 250.3, 215.71428571428572, 129.42857142857142, 242.7, 264.5, 267.0, 227.0, 479.3333333333333, 263.5, 247.5, 194.0, 157.45454545454547, 307.64285714285717, 271.625, 263.75, 221.5, 342.6666666666667, 389.0, 255.8, 130.51724137931035, 192.375, 1434.0, 301.6, 280.0, 459.6923076923077, 218.0, 284.6666666666667, 90.94736842105263, 258.5833333333333, 462.4, 332.6, 166.0, 242.4, 279.6666666666667, 397.5, 121.45454545454545, 153.7, 166.66666666666666, 410.44444444444446, 230.66666666666666, 172.91666666666666, 165.47222222222223, 504.8, 213.0, 180.25, 609.3333333333334, 269.8780487804878, 497.6, 171.16666666666666, 292.0, 106.41666666666667, 287.8333333333333, 126.0, 281.6666666666667, 99.25, 256.45454545454544, 188.1, 141.44736842105263, 148.375, 312.6, 234.14285714285714, 375.57142857142856, 214.11764705882354, 422.3333333333333, 102.6896551724138, 390.25, 547.0, 242.125, 293.0, 254.2, 116.95, 274.3333333333333, 200.4, 166.95454545454547, 389.875, 217.5, 271.1363636363636, 283.90909090909093, 132.46774193548387, 249.75, 333.8333333333333, 294.5, 208.75, 646.6666666666666, 163.33333333333334, 141.775, 132.875, 163.19718309859155, 309.4, 136.22222222222223, 232.33333333333334, 222.5, 271.4193548387097, 306.8, 116.69444444444444, 117.5909090909091, 289.57142857142856, 139.95652173913044, 141.25423728813558, 69.1590909090909, 229.6, 186.46666666666667, 247.77777777777777, 302.55555555555554, 146.35294117647058, 180.5625, 60.927536231884055, 144.15625, 93.4, 242.8, 131.47154471544715, 272.2, 396.25, 203.9, 512.0, 178.08333333333334, 192.30434782608697, 488.0, 198.07142857142858, 194.37254901960785, 165.60689655172413, 414.75, 439.8, 282.0769230769231, 248.85714285714286, 214.33333333333334, 172.23529411764707, 198.02173913043478, 248.75, 408.75, 276.0, 273.69565217391306, 162.4, 174.27272727272728, 390.4, 636.75, 219.06382978723406, 124.94444444444444, 441.5, 210.42105263157896, 161.42857142857142, 241.16666666666666, 177.8095238095238, 351.75, 216.14285714285714, 177.0, 157.54545454545453, 239.0, 182.8, 148.77777777777777, 208.875, 545.25, 325.8333333333333, 112.68181818181819, 446.4, 257.0, 124.95384615384616, 121.03125, 79.23809523809524, 225.5, 402.5, 162.06666666666666, 446.2857142857143, 324.8, 104.47115384615384, 202.28571428571428, 224.5, 274.3333333333333, 275.25, 827.0, 201.125, 100.22916666666667, 376.6666666666667, 628.25, 298.85714285714283, 543.0, 375.0, 106.0, 321.5, 244.33333333333334, 149.1, 236.0, 182.0, 258.6, 314.6, 170.07142857142858, 410.2, 174.9, 215.75, 272.75, 146.11764705882354, 201.27272727272728, 259.35714285714283, 171.71428571428572, 203.15384615384616, 260.7692307692308, 396.57142857142856, 155.0, 257.8, 197.625, 494.3333333333333, 195.0, 213.55555555555554, 296.2, 143.56, 106.69090909090909, 126.62068965517241, 212.41666666666666, 211.9375, 433.875, 217.33333333333334, 499.0, 227.83333333333334, 85.76811594202898, 132.74285714285713, 416.57142857142856, 146.93333333333334, 201.375, 206.94444444444446, 369.875, 204.0, 289.46153846153845, 51.53225806451613, 156.62962962962962, 207.76190476190476, 182.75, 268.8, 122.23333333333333, 349.0, 447.2, 130.14563106796118, 235.6, 153.52173913043478, 426.0, 308.75, 147.15, 138.88235294117646, 151.0, 226.75, 251.8, 158.8125, 129.49557522123894, 409.4375, 325.0, 166.1818181818182, 240.07692307692307, 207.375, 393.8, 424.2, 243.0, 225.66666666666666, 223.57142857142858, 241.5, 338.0, 542.25, 194.6, 167.7017543859649, 211.08333333333334, 112.009009009009, 327.1025641025641, 180.3, 275.4, 319.0, 400.8, 142.33333333333334, 347.42857142857144, 220.27272727272728, 316.93333333333334, 169.5, 130.11111111111111, 343.7, 454.38461538461536, 257.8, 213.11764705882354, 193.14285714285714, 209.3125, 135.41509433962264, 125.18085106382979, 283.25, 280.75, 377.0, 336.5, 232.07142857142858, 86.37878787878788, 240.86666666666667, 257.0, 387.6666666666667, 122.57894736842105, 189.4, 232.75, 142.55555555555554, 404.57142857142856, 291.3, 194.83333333333334, 187.275, 195.10526315789474, 81.66145833333333, 444.3333333333333, 203.5, 500.7857142857143, 207.10344827586206, 202.8, 117.4, 295.0, 332.1666666666667, 114.92792792792793, 337.75, 454.6666666666667, 151.0, 780.8888888888889, 299.0, 374.0, 308.84615384615387, 116.24074074074075, 287.8333333333333, 270.77777777777777, 221.9090909090909, 196.16666666666666, 407.25, 280.5, 230.2, 326.3333333333333, 224.04166666666666, 586.8, 293.75, 157.45238095238096, 233.59259259259258, 134.125, 343.25, 267.9, 223.83333333333334, 241.0, 245.375, 257.3333333333333, 189.5, 235.9, 191.625, 283.8333333333333, 212.0, 281.3333333333333, 135.3684210526316, 395.6666666666667, 206.11111111111111, 364.6, 214.75, 219.43243243243242, 224.85714285714286, 159.125, 167.3684210526316, 112.3157894736842, 196.27777777777777, 152.83333333333334, 325.3333333333333, 294.8333333333333, 486.8333333333333, 106.5609756097561, 153.3, 297.8333333333333, 175.8125, 142.66666666666666, 253.0, 322.75, 278.25, 244.0, 143.25806451612902, 349.04761904761904, 244.16666666666666, 151.52380952380952, 331.1666666666667, 148.37142857142857, 442.2, 293.22222222222223, 349.75, 152.62962962962962, 365.3333333333333, 184.4, 446.0, 456.4, 143.55, 196.84615384615384, 177.46875], 32b: [287.5, 305.5882352941176, 326.6, 361.0, 328.3333333333333, 228.5, 253.33333333333334, 197.1818181818182, 394.3333333333333, 124.6, 153.5142857142857, 299.94117647058823, 241.25, 174.25, 232.33333333333334, 266.3888888888889, 289.25, 177.03571428571428, 297.5, 115.375, 294.3225806451613, 317.1666666666667, 483.75, 326.77777777777777, 154.0952380952381, 107.0, 243.0, 191.42857142857142, 473.6666666666667, 213.16666666666666, 197.25, 285.8181818181818, 611.0, 233.5, 155.125, 193.125, 403.6666666666667, 188.25, 138.225, 270.0, 293.93333333333334, 231.4, 300.75, 285.3125, 327.6666666666667, 182.11111111111111, 358.5, 395.7142857142857, 627.6, 369.0, 329.0, 346.75, 263.75, 174.0, 486.6666666666667, 224.41176470588235, 217.8, 526.6363636363636, 401.1111111111111, 364.5833333333333, 633.0, 212.5, 273.4, 280.57142857142856, 368.2, 181.33333333333334, 125.8695652173913, 291.375, 323.25, 155.875, 469.6666666666667, 225.54545454545453, 175.0, 228.5, 535.0, 256.0, 227.8235294117647, 181.5, 96.26315789473684, 247.27272727272728, 733.0, 177.16666666666666, 157.71428571428572, 285.2, 368.90909090909093, 584.75, 190.0, 247.0, 420.0, 258.1666666666667, 330.5, 456.0, 746.3333333333334, 98.84795321637426, 292.54545454545456, 258.2, 213.47916666666666, 567.4444444444445, 269.75, 321.5, 248.25, 349.0, 343.25, 301.0, 338.8333333333333, 269.8333333333333, 793.0, 265.6, 736.5, 432.3333333333333, 273.875, 68.19318181818181, 173.88, 446.5, 297.5, 84.25555555555556, 521.6666666666666, 307.42857142857144, 909.6666666666666, 322.0, 202.5, 338.8333333333333, 151.5, 291.5, 538.0, 295.8, 164.86046511627907, 384.0, 131.0, 378.14285714285717, 447.0, 260.5, 146.2, 297.85714285714283, 263.2, 266.0625, 235.5, 167.98130841121494, 291.8333333333333, 457.25, 291.6, 149.66666666666666, 104.98412698412699, 138.82608695652175, 230.66666666666666, 331.93333333333334, 351.77777777777777, 175.21875, 302.2, 162.2, 382.625, 262.5, 173.04, 48.69716088328076, 162.25, 84.65648854961832, 476.0, 123.4090909090909, 788.0, 267.22222222222223, 307.8333333333333, 304.2, 304.55555555555554, 324.14285714285717, 252.3846153846154, 186.05882352941177, 392.625, 353.8, 304.14285714285717, 325.8333333333333, 262.7142857142857, 332.5, 394.8333333333333, 276.45, 166.0, 356.4, 392.0, 314.75, 470.90909090909093, 192.7, 292.5, 312.1666666666667, 399.0, 474.3076923076923, 377.0, 256.8333333333333, 228.75, 505.25, 226.71428571428572, 450.25, 186.47058823529412, 292.2857142857143, 501.6666666666667, 335.75, 262.8333333333333, 210.0, 139.20833333333334, 290.75, 304.5, 284.30434782608694, 337.2857142857143, 778.6666666666666, 200.0, 369.5, 274.27777777777777, 130.5, 240.28571428571428, 142.75757575757575, 204.53333333333333, 205.6875, 379.0, 167.58823529411765, 281.0, 458.6666666666667, 160.22222222222223, 376.1111111111111, 511.3333333333333, 151.67567567567568, 342.6666666666667, 489.25, 348.1666666666667, 147.59375, 322.4, 272.8333333333333, 254.83333333333334, 279.8888888888889, 226.0, 204.07142857142858, 310.8, 195.28571428571428, 327.6666666666667, 304.25, 264.42857142857144, 325.8, 160.8, 206.46153846153845, 167.0, 165.0, 301.25, 303.6, 264.75, 249.28571428571428, 417.0, 573.3333333333334, 227.28571428571428, 531.3333333333334, 369.6666666666667, 270.7142857142857, 277.25, 404.2, 235.0, 128.95, 203.57142857142858, 275.3333333333333, 278.3636363636364, 611.25, 175.27272727272728, 149.75, 413.3333333333333, 312.8888888888889, 305.3333333333333, 406.25, 233.75, 258.5, 108.56923076923077, 225.66666666666666, 321.6666666666667, 449.125, 226.8048780487805, 221.41666666666666, 580.4, 428.875, 283.7142857142857, 297.3333333333333, 319.0769230769231, 303.6, 437.5, 359.8125, 323.1666666666667, 359.2, 149.5, 241.8, 152.19230769230768, 99.82022471910112, 178.375, 415.5, 237.2, 300.8181818181818, 89.54, 246.85714285714286, 460.25, 280.3333333333333, 231.75, 253.33333333333334, 303.25, 485.0, 242.36, 205.16666666666666, 273.9, 184.375, 355.25, 146.48717948717947, 192.44444444444446, 203.5, 486.0, 292.4166666666667, 195.0, 543.0, 386.6666666666667, 261.0, 182.70833333333334, 379.0, 241.375, 409.0, 139.11111111111111, 343.25, 618.5, 302.8, 215.6, 336.5, 313.8, 133.6, 635.3333333333334, 249.0, 269.14285714285717, 126.24489795918367, 224.57142857142858, 302.9642857142857, 345.2173913043478, 397.8333333333333, 251.625, 371.38461538461536, 292.5, 242.27272727272728, 201.4375, 387.0, 298.0, 429.0, 163.28571428571428, 268.8333333333333, 706.0, 495.64285714285717, 238.69230769230768, 247.33333333333334, 172.5, 192.75, 423.25, 185.375, 545.2, 244.0, 353.0, 315.6666666666667, 157.53333333333333, 150.28571428571428, 311.14285714285717, 99.81666666666666, 224.78571428571428, 416.6, 238.0, 135.625, 286.3333333333333, 141.69230769230768, 367.625, 302.77777777777777, 192.0, 258.90909090909093, 465.3333333333333, 147.17021276595744, 153.7, 229.30769230769232, 188.5, 567.5833333333334, 219.55555555555554, 259.7857142857143, 263.0, 235.42857142857142, 214.54545454545453, 333.0, 269.1875, 390.2857142857143, 482.3333333333333, 395.0, 408.9, 259.6666666666667, 284.3333333333333, 504.8, 273.25, 283.2, 432.0, 180.27272727272728, 239.06451612903226, 172.67741935483872, 199.85714285714286, 227.84444444444443, 215.2, 518.3333333333334, 308.25, 288.625, 156.6, 435.0, 258.741935483871, 461.0, 226.27272727272728, 449.0, 210.84615384615384, 353.5, 312.1666666666667, 293.8125, 239.0, 192.0, 171.55555555555554, 349.4, 206.67741935483872, 218.58333333333334, 266.4, 399.75, 123.14285714285714, 231.8, 195.36363636363637, 301.2, 269.8, 303.2, 332.2, 447.75, 239.8, 468.8, 395.14285714285717, 127.22222222222223, 218.2, 356.6666666666667, 255.85714285714286, 172.56521739130434, 254.9047619047619, 220.0, 235.0, 603.1666666666666, 223.0, 263.1666666666667, 319.77777777777777, 435.0, 248.4375, 279.75, 230.0, 389.05555555555554, 271.0, 313.25, 291.0, 193.44444444444446, 295.5, 341.1111111111111, 113.51612903225806, 124.28260869565217, 184.07692307692307, 169.0, 238.8, 79.12244897959184, 211.33333333333334, 284.25, 130.44303797468353, 406.46666666666664, 392.4, 481.0, 240.27272727272728, 250.10526315789474, 164.5909090909091]\n",
      "wrong: 1.5b: [103.78504672897196, 78.67708333333333, 450.3333333333333, 240.9, 130.72727272727272, 380.86046511627904, 185.57142857142858, 283.18446601941747, 198.84, 24.146647015475313, 82.6826923076923, 173.0, 137.81617647058823, 478.0, 168.78181818181818, 94.56115107913669, 21.739880557398806, 419.3636363636364, 257.4, 219.4375, 131.7128712871287, 369.0, 155.9, 194.30555555555554, 112.78125, 370.25, 448.3333333333333, 286.7142857142857, 196.9626168224299, 235.77631578947367, 604.2, 241.36633663366337, 126.86734693877551, 487.6, 80.07142857142857], 32b: [103.78504672897196, 78.67708333333333, 450.3333333333333, 240.9, 130.72727272727272, 380.86046511627904, 185.57142857142858, 283.18446601941747, 198.84, 24.146647015475313, 82.6826923076923, 173.0, 137.81617647058823, 478.0, 168.78181818181818, 94.56115107913669, 21.739880557398806, 419.3636363636364, 257.4, 219.4375, 131.7128712871287, 369.0, 155.9, 194.30555555555554, 112.78125, 370.25, 448.3333333333333, 286.7142857142857, 196.9626168224299, 235.77631578947367, 604.2, 241.36633663366337, 126.86734693877551, 487.6, 80.07142857142857]\n",
      "avg num of tokens\n",
      "right: 1.5b: [993, 3200, 2087, 1515, 4649, 1262, 2246, 2958, 1669, 2929, 3215, 1674, 2634, 2571, 5085, 4862, 1971, 5904, 2425, 2051, 4156, 1951, 2110, 1827, 4595, 1107, 6708, 2737, 2014, 2504, 1511, 907, 2429, 1059, 802, 910, 1439, 3690, 1981, 1361, 1733, 8618, 2174, 1056, 444, 1029, 1168, 1281, 3787, 1541, 4303, 3018, 4201, 5977, 1314, 1711, 3457, 3107, 2313, 1664, 6146, 1213, 840, 1591, 2673, 1539, 2007, 3695, 1385, 2077, 5962, 2528, 1067, 1443, 1829, 11067, 2489, 1028, 1169, 5110, 1728, 14494, 1693, 1986, 2825, 1882, 10760, 1188, 1564, 1640, 2630, 3642, 1268, 2985, 1564, 1642, 1939, 1761, 1273, 7021, 825, 2005, 3675, 3123, 1306, 11931, 3124, 8214, 3998, 2004, 1179, 837, 1941, 1471, 5672, 10631, 11588, 1548, 2453, 2094, 2226, 8415, 3069, 8404, 2588, 2028, 3220, 8336, 9130, 2297, 2799, 2232, 2725, 2494, 2890, 4205, 9227, 6083, 1215, 16176, 1362, 1586, 6121, 513, 2139, 4425, 1465, 2774, 9919, 24017, 1660, 2201, 3668, 1744, 1288, 2929, 9112, 1992, 1636, 1935, 6297, 4061, 1921, 1956, 2548, 10301, 2251, 1767, 4002, 1133, 1448, 3738, 1408, 1518, 355, 3468, 3108, 2744, 1342, 3344, 2182, 1957, 2480, 2233, 4113, 16247, 3880, 3329, 903, 3221, 2432, 3125, 1625, 10868, 1421, 2695, 2470, 2203, 828, 3220, 4812, 2261, 2514, 2093, 2173, 1502, 3817, 1930, 734, 1492, 1653, 1827, 1294, 1575, 2384, 2052, 1750, 866, 1093, 2485, 2215, 3633, 2406, 2642, 3391, 2777, 3101, 2579, 1582, 1484, 781, 1924, 1482, 3590, 5882, 7345, 2550, 3392, 3472, 1957, 3997, 1368, 5919, 4647, 2917, 2206, 1612, 3728, 2960, 1225, 3764, 9588, 4231, 4368, 735, 1345, 3668, 2444, 2237, 13406, 1179, 3532, 1705, 1236, 2944, 2362, 2570, 908, 2519, 2543, 14635, 32768, 1301, 1831, 3122, 1661, 1970, 2122, 1945, 1359, 1566, 1453, 1355, 2170, 1947, 9560, 2534, 12449, 12761, 1804, 1378, 6381, 2005, 5126, 2433, 2424, 4756, 2036, 2347, 3438, 5908, 2579, 3624, 1353, 3351, 14357, 11770, 2267, 1124, 1509, 2693, 3250, 5702, 3614, 2314, 1166, 2330, 1895, 1864, 2567, 2833, 2914, 1170, 7500, 3708, 15680, 1334, 408, 7012, 6007, 1016, 2349, 296, 1994, 12769, 2703, 2729, 1058, 7029, 1199, 1498, 4016, 6279, 1729, 2438, 2442, 5887, 3260, 1124, 1152, 1959, 5379, 2935, 1178, 6614, 6310, 4294, 1374, 2682, 1345, 2170, 1966, 2318, 1896, 2360, 7669, 1705, 2122, 2536, 2573, 2376, 1858, 1824, 1721, 8127, 1575, 1275, 3184, 8545, 3534, 2754, 977, 1770, 2923, 4370, 1534, 1789, 5627, 1286, 1016, 1292, 5566, 1465, 4444, 7331, 1466, 3184, 1989, 10387, 4423, 2645, 1402, 4123, 1098, 6460, 2231, 2283, 11487, 5122, 5681], 32b: [1151, 5196, 1634, 1084, 1972, 915, 3041, 2173, 1185, 9347, 5374, 5100, 2897, 699, 1395, 4796, 2319, 4962, 3571, 926, 9128, 1906, 1937, 2945, 3247, 4388, 973, 1341, 1422, 2559, 793, 3145, 1834, 2803, 1242, 1547, 2423, 754, 5531, 1351, 8822, 1158, 1204, 4566, 984, 1642, 2869, 2771, 3139, 2584, 988, 1388, 1056, 1219, 1461, 3816, 1090, 5794, 3611, 4376, 2533, 3401, 1368, 1968, 1842, 4353, 2897, 2332, 1294, 1249, 1410, 2482, 876, 915, 3746, 1282, 7748, 1091, 3659, 2721, 2200, 1066, 1110, 1427, 4059, 2340, 2281, 1484, 3361, 1553, 1325, 1369, 2241, 16913, 6439, 1292, 20512, 5109, 1080, 1287, 1987, 3142, 2749, 1206, 2034, 1620, 2381, 1329, 2947, 2596, 4389, 6002, 4348, 1787, 1192, 7589, 1566, 2153, 2730, 4509, 4051, 2034, 1516, 1168, 1615, 1480, 7090, 1921, 6813, 5295, 1790, 1043, 2194, 2086, 1317, 4259, 2356, 17975, 1754, 1831, 2917, 6290, 6615, 3194, 2078, 4980, 3167, 5608, 1513, 1624, 3062, 1576, 4327, 15438, 1300, 11091, 2857, 5433, 2365, 4814, 1848, 1522, 2742, 2270, 3283, 3165, 3142, 1770, 2133, 1956, 1840, 1331, 2372, 5530, 500, 1783, 1569, 5038, 5182, 1928, 1172, 1877, 1198, 6168, 3394, 1542, 4576, 2023, 1590, 1802, 3172, 2047, 1506, 1345, 1578, 2101, 6683, 1164, 1221, 6546, 2364, 2337, 1402, 740, 4939, 1045, 1683, 4715, 6139, 3292, 2279, 2850, 844, 2753, 1443, 3386, 1535, 5613, 1029, 1958, 2090, 4724, 1613, 1639, 7646, 5039, 1132, 2858, 3114, 1368, 1968, 4870, 1853, 1630, 3222, 2685, 836, 1486, 1206, 1520, 1060, 1748, 1669, 1721, 1593, 1595, 1110, 5687, 2220, 2022, 1647, 2581, 1427, 1653, 3064, 2446, 1930, 2397, 1241, 2817, 918, 1628, 1873, 2070, 7061, 1356, 966, 3594, 9305, 2658, 2903, 3432, 1987, 1785, 4152, 1521, 2627, 5758, 7757, 1797, 2393, 1210, 3959, 17772, 7144, 2494, 1187, 3310, 4478, 1729, 1842, 842, 928, 1521, 2427, 1941, 6062, 1233, 2741, 1477, 1422, 5715, 1734, 3257, 4375, 7019, 979, 3259, 2321, 3655, 4386, 1517, 1935, 1637, 1253, 2747, 2475, 1516, 1079, 1347, 1571, 1338, 1907, 1748, 1891, 6187, 1574, 8486, 7943, 2388, 2014, 4829, 2344, 2666, 3231, 2323, 1491, 3433, 1144, 1616, 2119, 6940, 3104, 1485, 1381, 772, 6773, 4451, 2727, 977, 2472, 1895, 2366, 2105, 2179, 11984, 3148, 2084, 1429, 5426, 1719, 1843, 2942, 2726, 961, 2852, 2793, 6918, 1538, 2987, 378, 6813, 1980, 3638, 791, 1649, 2362, 1999, 4309, 2733, 1448, 1977, 4093, 780, 1710, 2525, 2187, 1418, 1731, 1985, 7412, 5355, 1405, 10254, 1078, 1557, 3700, 2310, 787, 4351, 8022, 3228, 2491, 1797, 2743, 1415, 1874, 4702, 1914, 1345, 1545, 1748, 6409, 2628, 1334, 1602, 1725, 2319, 2153, 1508, 1350, 3035, 1662, 1792, 2407, 2345, 2767, 3437, 1092, 2143, 1792, 3970, 5356, 1761, 1646, 3620, 447, 1582, 2879, 2178, 3976, 1122, 2301, 7004, 1085, 2507, 4950, 1743, 2368, 3071, 3521, 5718, 2394, 1354, 3585, 7755, 1270, 1138, 10314, 6098, 1963, 2406, 7931, 4753, 3623]\n",
      "wrong: 1.5b: [14719, 2350, 419, 32768, 3491, 32768, 2394, 14415, 19203, 5367, 7904, 11923, 9021, 32768, 1254, 32768, 24709, 21810, 29625, 8237, 32768, 1611, 5159, 9882, 5367, 11891, 32768, 30590, 2336, 1367, 579, 17010, 3601, 5938, 13872, 495, 32768, 5397, 7974, 32768, 29385, 3181, 32768, 5023, 9545, 17171, 11823, 4334, 4442, 18955, 25295, 32768, 3673, 2088, 3929, 26589, 8279, 16633, 1621, 16204, 6735, 25705, 31679, 8461, 9713, 4892, 4161, 14513, 1536, 436, 25615, 704, 11312, 32768, 3376, 23739, 32768, 3896, 9527, 32768, 1874, 5563, 7795, 19401, 32768, 32768, 2711, 8732, 601, 8453], 32b: [11106, 7554, 2703, 2412, 7193, 32768, 1303, 29171, 14922, 32768, 17199, 1558, 18748, 2392, 9284, 13145, 32768, 4615, 2575, 7027, 13307, 1108, 4678, 13991, 7219, 2964, 1346, 2008, 21080, 17927, 3022, 24379, 24876, 2440, 3364]\n"
     ]
    }
   ],
   "source": [
    "print('num of thoughts')\n",
    "print(f'right: 1.5b: {avg_s['right'][0]}, 32b: {avg_s['right'][1]}')\n",
    "print(f'wrong: 1.5b: {avg_s['wrong'][0]}, 32b: {avg_s['wrong'][1]}')\n",
    "print('length of thoughts')\n",
    "print(f'right: 1.5b: {avg_s_l['right'][0]}, 32b: {avg_s_l['right'][1]}')\n",
    "print(f'wrong: 1.5b: {avg_s_l['wrong'][1]}, 32b: {avg_s_l['wrong'][1]}')\n",
    "print('avg num of tokens')\n",
    "print(f'right: 1.5b: {num_tokens['right'][0]}, 32b: {num_tokens['right'][1]}')\n",
    "print(f'wrong: 1.5b: {num_tokens['wrong'][0]}, 32b: {num_tokens['wrong'][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg num of thoughts\n",
      "right: 1.5b: (18.90487804878049, 25.998747166996733), 32b: (13.98279569892473, 23.663313212313856)\n",
      "wrong: 1.5b: (138.3111111111111, 218.5495440623854), 32b: (138.11428571428573, 323.6392852216423)\n",
      "avg length of thoughts\n",
      "right: 1.5b: (255.4635688690364, 129.06428101599127), 32b: (295.160009790653, 128.87618247746545)\n",
      "wrong: 1.5b: (206.0119628321458, 148.19062182765992), 32b: (233.4757125271959, 144.00071867183073)\n",
      "avg num of tokens\n",
      "right: 1.5b: (3412.765853658537, 3329.6445732540105), 32b: (2964.404301075269, 2464.4364297158354)\n",
      "wrong: 1.5b: (13785.888888888889, 11578.366283187552), 32b: (11283.42857142857, 10064.738705814798)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('avg num of thoughts')\n",
    "print(f'right: 1.5b: {avg(avg_s['right'][0])}, 32b: {avg(avg_s['right'][1])}')\n",
    "print(f'wrong: 1.5b: {avg(avg_s['wrong'][0])}, 32b: {avg(avg_s['wrong'][1])}')\n",
    "print('avg length of thoughts')\n",
    "print(f'right: 1.5b: {avg(avg_s_l['right'][0])}, 32b: {avg(avg_s_l['right'][1])}')\n",
    "print(f'wrong: 1.5b: {avg(avg_s_l['wrong'][0])}, 32b: {avg(avg_s_l['wrong'][1])}')\n",
    "print('avg num of tokens')\n",
    "print(f'right: 1.5b: {avg(num_tokens['right'][0])}, 32b: {avg(num_tokens['right'][1])}')\n",
    "print(f'wrong: 1.5b: {avg(num_tokens['wrong'][0])}, 32b: {avg(num_tokens['wrong'][1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 统计32b正确，1.5错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59, 59)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = min(len(results_32b), len(results_1b))\n",
    "results_1b_2, results_32b_2 = [],[]\n",
    "for i in range(l):\n",
    "    r_32, r_1 = results_32b[i], results_1b[i]\n",
    "    r_32_right_flag = check_math_correctness(r_32['answer'], r_32['generated_answer'])\n",
    "    r_1_right_flag = check_math_correctness(r_1['answer'], r_1['generated_answer'])\n",
    "    if r_32['question']==r_1['question'] and (r_32_right_flag and not r_1_right_flag):\n",
    "        results_1b_2.append(r_1)\n",
    "        results_32b_2.append(r_32)\n",
    "len(results_1b_2), len(results_32b_2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sections_all_1b = segement(results_1b_2)\n",
    "filtered_sections_all_32b = segement(results_32b_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import __init__\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Qwen2ForCausalLM\n",
    "target_model_name = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-32B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(target_model_name)\n",
    "\n",
    "avg_s, avg_s_l,num_tokens = {'right': [[], []], 'wrong': [[], []]}, {'right': [[], []], 'wrong': [[], []]},{'right': [[], []], 'wrong': [[], []]}\n",
    "\n",
    "for s_1 in filtered_sections_all_1b:\n",
    "    key1 = 'right' if s_1['right'] else 'wrong'\n",
    "    # 统计 filtered_sections 的数量\n",
    "    avg_s[key1][0].append(len(s_1['filtered_sections']))\n",
    "   \n",
    "    # # **使用 tokenizer 计算 token 个数**\n",
    "    token_count_1b = [len(tokenizer.encode(ss, add_special_tokens=False)) for ss in s_1['filtered_sections']]\n",
    "   \n",
    "    # 统计 filtered_sections 的 token 数量\n",
    "    avg_s_l[key1][0].append(avg_l(token_count_1b)) # s_1['num_tokens']/avg_s[key1][0][-1]\n",
    "    num_tokens[key1][0].append(s_1['num_tokens'])\n",
    "   \n",
    "for s_32 in filtered_sections_all_32b:\n",
    "    key32 = 'right' if s_32['right'] else 'wrong'\n",
    "\n",
    "    # 统计 filtered_sections 的数量\n",
    "    avg_s[key32][1].append(len(s_32['filtered_sections']))\n",
    "\n",
    "    # # **使用 tokenizer 计算 token 个数**\n",
    "    token_count_32b = [len(tokenizer.encode(ss, add_special_tokens=False)) for ss in s_32['filtered_sections']]\n",
    "\n",
    "    # 统计 filtered_sections 的 token 数量\n",
    "    avg_s_l[key32][1].append(avg_l(token_count_32b)) # s_32['num_tokens']/avg_s[key32][1][-1]\n",
    "    num_tokens[key32][1].append(s_32['num_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5b wrong, 32b right\n",
      "num of thoughts\n",
      "right: 1.5b: [], 32b: [75, 17, 4, 21, 41, 40, 30, 7, 4, 34, 38, 171, 22, 96, 88, 25, 52, 15, 10, 15, 44, 16, 7, 48, 7, 7, 18, 33, 32, 30, 7, 16, 21, 7, 8, 8, 41, 6, 16, 26, 40, 16, 9, 49, 16, 120, 13, 9, 45, 10, 16, 23, 9, 17, 31, 46, 98, 6, 15]\n",
      "wrong: 1.5b: [318, 7, 2, 26, 390, 159, 169, 29, 35, 75, 125, 699, 121, 107, 142, 380, 57, 108, 27, 213, 620, 5, 2, 43, 133, 2, 31, 72, 432, 74, 29, 25, 178, 114, 31, 86, 172, 1336, 14, 20, 151, 43, 133, 98, 121, 189, 1, 1244, 234, 18, 26, 54, 171, 332, 163, 15, 181, 1, 38], 32b: []\n",
      "length of thoughts\n",
      "right: 1.5b: [], 32b: [124.6, 299.94117647058823, 174.25, 154.0952380952381, 107.0, 138.225, 293.93333333333334, 395.7142857142857, 633.0, 227.8235294117647, 96.26315789473684, 98.84795321637426, 292.54545454545456, 213.47916666666666, 68.19318181818181, 173.88, 131.0, 146.2, 291.6, 331.93333333333334, 123.4090909090909, 314.75, 226.71428571428572, 139.20833333333334, 337.2857142857143, 200.0, 274.27777777777777, 142.75757575757575, 147.59375, 254.83333333333334, 195.28571428571428, 304.25, 270.7142857142857, 235.0, 233.75, 449.125, 226.8048780487805, 297.3333333333333, 359.8125, 152.19230769230768, 178.375, 203.5, 486.0, 126.24489795918367, 201.4375, 99.81666666666666, 229.30769230769232, 219.55555555555554, 227.84444444444443, 435.0, 293.8125, 172.56521739130434, 319.77777777777777, 291.0, 113.51612903225806, 124.28260869565217, 79.12244897959184, 211.33333333333334, 406.46666666666664]\n",
      "wrong: 1.5b: [], 32b: []\n",
      "avg num of tokens\n",
      "right: 1.5b: [], 32b: [9347, 5100, 699, 3247, 4388, 5531, 8822, 2771, 2533, 7748, 3659, 16913, 6439, 20512, 6002, 4348, 6813, 2194, 2917, 4980, 5433, 5038, 1590, 6683, 2364, 1402, 4939, 4715, 4724, 7646, 1368, 4870, 5687, 1647, 1873, 3594, 9305, 1785, 5758, 3959, 7144, 3257, 4375, 6187, 3231, 11984, 2987, 1980, 10254, 4351, 4702, 3970, 2879, 4950, 3521, 5718, 7755, 1270, 6098]\n",
      "wrong: 1.5b: [14719, 2350, 419, 3491, 32768, 14415, 19203, 5367, 7904, 11923, 9021, 32768, 24709, 21810, 8237, 32768, 5159, 9882, 5367, 32768, 30590, 2336, 579, 5938, 13872, 495, 5397, 7974, 32768, 29385, 5023, 9545, 17171, 11823, 4334, 18955, 25295, 32768, 3673, 3929, 26589, 6735, 25705, 8461, 9713, 14513, 704, 32768, 32768, 3896, 9527, 7795, 19401, 32768, 32768, 2711, 8732, 601, 8453], 32b: []\n"
     ]
    }
   ],
   "source": [
    "print('1.5b wrong, 32b right')\n",
    "print('num of thoughts')\n",
    "print(f'right: 1.5b: {avg_s['right'][0]}, 32b: {avg_s['right'][1]}')\n",
    "print(f'wrong: 1.5b: {avg_s['wrong'][0]}, 32b: {avg_s['wrong'][1]}')\n",
    "print('length of thoughts')\n",
    "print(f'right: 1.5b: {avg_s_l['right'][0]}, 32b: {avg_s_l['right'][1]}')\n",
    "print(f'wrong: 1.5b: {avg_s_l['wrong'][1]}, 32b: {avg_s_l['wrong'][1]}')\n",
    "print('avg num of tokens')\n",
    "print(f'right: 1.5b: {num_tokens['right'][0]}, 32b: {num_tokens['right'][1]}')\n",
    "print(f'wrong: 1.5b: {num_tokens['wrong'][0]}, 32b: {num_tokens['wrong'][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5b wrong, 32b right\n",
      "avg num of thoughts\n",
      "right: 1.5b: (0, 0), 32b: (30.35593220338983, 30.926793118679036)\n",
      "wrong: 1.5b: (166.45762711864407, 254.364300476858), 32b: (0, 0)\n",
      "avg length of thoughts\n",
      "right: 1.5b: (0, 0), 32b: (232.14544287280705, 110.4314979477617)\n",
      "wrong: 1.5b: (176.5181525129746, 129.39334715042693), 32b: (0, 0)\n",
      "avg num of tokens\n",
      "right: 1.5b: (0, 0), 32b: (5185.6949152542375, 3488.88192597055)\n",
      "wrong: 1.5b: (14262.813559322034, 11184.377255455867), 32b: (0, 0)\n"
     ]
    }
   ],
   "source": [
    "print('1.5b wrong, 32b right')\n",
    "print('avg num of thoughts')\n",
    "print(f'right: 1.5b: {avg(avg_s['right'][0])}, 32b: {avg(avg_s['right'][1])}')\n",
    "print(f'wrong: 1.5b: {avg(avg_s['wrong'][0])}, 32b: {avg(avg_s['wrong'][1])}')\n",
    "print('avg length of thoughts')\n",
    "print(f'right: 1.5b: {avg(avg_s_l['right'][0])}, 32b: {avg(avg_s_l['right'][1])}')\n",
    "print(f'wrong: 1.5b: {avg(avg_s_l['wrong'][0])}, 32b: {avg(avg_s_l['wrong'][1])}')\n",
    "print('avg num of tokens')\n",
    "print(f'right: 1.5b: {avg(num_tokens['right'][0])}, 32b: {avg(num_tokens['right'][1])}')\n",
    "print(f'wrong: 1.5b: {avg(num_tokens['wrong'][0])}, 32b: {avg(num_tokens['wrong'][1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 统计1.5b 正确，32b错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = min(len(results_32b), len(results_1b))\n",
    "results_1b_2, results_32b_2 = [],[]\n",
    "for i in range(l):\n",
    "    r_32, r_1 = results_32b[i], results_1b[i]\n",
    "    r_32_right_flag = check_math_correctness(r_32['answer'], r_32['generated_answer'])\n",
    "    r_1_right_flag = check_math_correctness(r_1['answer'], r_1['generated_answer'])\n",
    "    if r_32['question']==r_1['question'] and (not r_32_right_flag and r_1_right_flag):\n",
    "        results_1b_2.append(r_1)\n",
    "        results_32b_2.append(r_32)\n",
    "len(results_1b_2), len(results_32b_2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sections_all_1b = segement(results_1b_2)\n",
    "filtered_sections_all_32b = segement(results_32b_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import __init__\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Qwen2ForCausalLM\n",
    "target_model_name = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-32B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(target_model_name)\n",
    "\n",
    "avg_s, avg_s_l,num_tokens = {'right': [[], []], 'wrong': [[], []]}, {'right': [[], []], 'wrong': [[], []]},{'right': [[], []], 'wrong': [[], []]}\n",
    "\n",
    "for s_1 in filtered_sections_all_1b:\n",
    "    key1 = 'right' if s_1['right'] else 'wrong'\n",
    "    # 统计 filtered_sections 的数量\n",
    "    avg_s[key1][0].append(len(s_1['filtered_sections']))\n",
    "   \n",
    "    # # **使用 tokenizer 计算 token 个数**\n",
    "    token_count_1b = [len(tokenizer.encode(ss, add_special_tokens=False)) for ss in s_1['filtered_sections']]\n",
    "   \n",
    "    # 统计 filtered_sections 的 token 数量\n",
    "    avg_s_l[key1][0].append(avg_l(token_count_1b)) # s_1['num_tokens']/avg_s[key1][0][-1]\n",
    "    num_tokens[key1][0].append(s_1['num_tokens'])\n",
    "   \n",
    "for s_32 in filtered_sections_all_32b:\n",
    "    key32 = 'right' if s_32['right'] else 'wrong'\n",
    "\n",
    "    # 统计 filtered_sections 的数量\n",
    "    avg_s[key32][1].append(len(s_32['filtered_sections']))\n",
    "\n",
    "    # # **使用 tokenizer 计算 token 个数**\n",
    "    token_count_32b = [len(tokenizer.encode(ss, add_special_tokens=False)) for ss in s_32['filtered_sections']]\n",
    "\n",
    "    # 统计 filtered_sections 的 token 数量\n",
    "    avg_s_l[key32][1].append(avg_l(token_count_32b)) # s_32['num_tokens']/avg_s[key32][1][-1]\n",
    "    num_tokens[key32][1].append(s_32['num_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5b right, 32b wrong\n",
      "num of thoughts\n",
      "right: 1.5b: [70, 37, 115, 44], 32b: []\n",
      "wrong: 1.5b: [], 32b: [96, 10, 55, 75]\n",
      "length of thoughts\n",
      "right: 1.5b: [95.81428571428572, 166.0, 126.0, 271.1363636363636], 32b: []\n",
      "wrong: 1.5b: [78.67708333333333, 240.9, 130.72727272727272, 198.84], 32b: [78.67708333333333, 240.9, 130.72727272727272, 198.84]\n",
      "avg num of tokens\n",
      "right: 1.5b: [6708, 6146, 14494, 11931], 32b: []\n",
      "wrong: 1.5b: [], 32b: [7554, 2412, 7193, 14922]\n"
     ]
    }
   ],
   "source": [
    "print('1.5b right, 32b wrong')\n",
    "print('num of thoughts')\n",
    "print(f'right: 1.5b: {avg_s['right'][0]}, 32b: {avg_s['right'][1]}')\n",
    "print(f'wrong: 1.5b: {avg_s['wrong'][0]}, 32b: {avg_s['wrong'][1]}')\n",
    "print('length of thoughts')\n",
    "print(f'right: 1.5b: {avg_s_l['right'][0]}, 32b: {avg_s_l['right'][1]}')\n",
    "print(f'wrong: 1.5b: {avg_s_l['wrong'][1]}, 32b: {avg_s_l['wrong'][1]}')\n",
    "print('avg num of tokens')\n",
    "print(f'right: 1.5b: {num_tokens['right'][0]}, 32b: {num_tokens['right'][1]}')\n",
    "print(f'wrong: 1.5b: {num_tokens['wrong'][0]}, 32b: {num_tokens['wrong'][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5b wrong, 32b right\n",
      "avg num of thoughts\n",
      "right: 1.5b: (66.5, 30.581857366746057), 32b: (0, 0)\n",
      "wrong: 1.5b: (0, 0), 32b: (59.0, 31.788362650504666)\n",
      "avg length of thoughts\n",
      "right: 1.5b: (164.73766233766233, 66.28219116583763), 32b: (0, 0)\n",
      "wrong: 1.5b: (0, 0), 32b: (162.2860890151515, 62.25504020803516)\n",
      "avg num of tokens\n",
      "right: 1.5b: (9819.75, 3517.2935884711133), 32b: (0, 0)\n",
      "wrong: 1.5b: (0, 0), 32b: (8020.25, 4471.810392615053)\n"
     ]
    }
   ],
   "source": [
    "print('1.5b wrong, 32b right')\n",
    "print('avg num of thoughts')\n",
    "print(f'right: 1.5b: {avg(avg_s['right'][0])}, 32b: {avg(avg_s['right'][1])}')\n",
    "print(f'wrong: 1.5b: {avg(avg_s['wrong'][0])}, 32b: {avg(avg_s['wrong'][1])}')\n",
    "print('avg length of thoughts')\n",
    "print(f'right: 1.5b: {avg(avg_s_l['right'][0])}, 32b: {avg(avg_s_l['right'][1])}')\n",
    "print(f'wrong: 1.5b: {avg(avg_s_l['wrong'][0])}, 32b: {avg(avg_s_l['wrong'][1])}')\n",
    "print('avg num of tokens')\n",
    "print(f'right: 1.5b: {avg(num_tokens['right'][0])}, 32b: {avg(num_tokens['right'][1])}')\n",
    "print(f'wrong: 1.5b: {avg(num_tokens['wrong'][0])}, 32b: {avg(num_tokens['wrong'][1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 统计1.5b错误，32b错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 31)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = min(len(results_32b), len(results_1b))\n",
    "results_1b_2, results_32b_2 = [],[]\n",
    "for i in range(l):\n",
    "    r_32, r_1 = results_32b[i], results_1b[i]\n",
    "    r_32_right_flag = check_math_correctness(r_32['answer'], r_32['generated_answer'])\n",
    "    r_1_right_flag = check_math_correctness(r_1['answer'], r_1['generated_answer'])\n",
    "    if r_32['question']==r_1['question'] and (not r_32_right_flag and not r_1_right_flag):\n",
    "        results_1b_2.append(r_1)\n",
    "        results_32b_2.append(r_32)\n",
    "len(results_1b_2), len(results_32b_2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sections_all_1b = segement(results_1b_2)\n",
    "filtered_sections_all_32b = segement(results_32b_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19905 > 16384). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import __init__\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Qwen2ForCausalLM\n",
    "target_model_name = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-32B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(target_model_name)\n",
    "\n",
    "avg_s, avg_s_l,num_tokens = {'right': [[], []], 'wrong': [[], []]}, {'right': [[], []], 'wrong': [[], []]},{'right': [[], []], 'wrong': [[], []]}\n",
    "\n",
    "for s_1 in filtered_sections_all_1b:\n",
    "    key1 = 'right' if s_1['right'] else 'wrong'\n",
    "    # 统计 filtered_sections 的数量\n",
    "    avg_s[key1][0].append(len(s_1['filtered_sections']))\n",
    "   \n",
    "    # # **使用 tokenizer 计算 token 个数**\n",
    "    token_count_1b = [len(tokenizer.encode(ss, add_special_tokens=False)) for ss in s_1['filtered_sections']]\n",
    "   \n",
    "    # 统计 filtered_sections 的 token 数量\n",
    "    avg_s_l[key1][0].append(avg_l(token_count_1b)) # s_1['num_tokens']/avg_s[key1][0][-1]\n",
    "    num_tokens[key1][0].append(s_1['num_tokens'])\n",
    "   \n",
    "for s_32 in filtered_sections_all_32b:\n",
    "    key32 = 'right' if s_32['right'] else 'wrong'\n",
    "\n",
    "    # 统计 filtered_sections 的数量\n",
    "    avg_s[key32][1].append(len(s_32['filtered_sections']))\n",
    "\n",
    "    # # **使用 tokenizer 计算 token 个数**\n",
    "    token_count_32b = [len(tokenizer.encode(ss, add_special_tokens=False)) for ss in s_32['filtered_sections']]\n",
    "\n",
    "    # 统计 filtered_sections 的 token 数量\n",
    "    avg_s_l[key32][1].append(avg_l(token_count_32b)) # s_32['num_tokens']/avg_s[key32][1][-1]\n",
    "    num_tokens[key32][1].append(s_32['num_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of thoughts\n",
      "right: 1.5b: [], 32b: []\n",
      "wrong: 1.5b: [106, 13, 94, 8, 115, 13, 100, 6, 214, 7, 58, 17, 291, 9, 4, 23, 141, 4, 198, 119, 37, 11, 3, 2, 233, 86, 12, 131, 477, 3, 92], 32b: [107, 6, 86, 7, 103, 1357, 208, 9, 136, 5, 55, 139, 1507, 11, 10, 32, 101, 3, 30, 72, 64, 8, 3, 7, 107, 76, 5, 101, 196, 5, 42]\n",
      "length of thoughts\n",
      "right: 1.5b: [], 32b: []\n",
      "wrong: 1.5b: [103.78504672897196, 450.3333333333333, 380.86046511627904, 185.57142857142858, 283.18446601941747, 24.146647015475313, 82.6826923076923, 173.0, 137.81617647058823, 478.0, 168.78181818181818, 94.56115107913669, 21.739880557398806, 419.3636363636364, 257.4, 219.4375, 131.7128712871287, 369.0, 155.9, 194.30555555555554, 112.78125, 370.25, 448.3333333333333, 286.7142857142857, 196.9626168224299, 235.77631578947367, 604.2, 241.36633663366337, 126.86734693877551, 487.6, 80.07142857142857], 32b: [103.78504672897196, 450.3333333333333, 380.86046511627904, 185.57142857142858, 283.18446601941747, 24.146647015475313, 82.6826923076923, 173.0, 137.81617647058823, 478.0, 168.78181818181818, 94.56115107913669, 21.739880557398806, 419.3636363636364, 257.4, 219.4375, 131.7128712871287, 369.0, 155.9, 194.30555555555554, 112.78125, 370.25, 448.3333333333333, 286.7142857142857, 196.9626168224299, 235.77631578947367, 604.2, 241.36633663366337, 126.86734693877551, 487.6, 80.07142857142857]\n",
      "avg num of tokens\n",
      "right: 1.5b: [], 32b: []\n",
      "wrong: 1.5b: [32768, 2394, 32768, 1254, 29625, 1611, 11891, 1367, 17010, 3601, 32768, 3181, 32768, 4442, 2088, 8279, 16633, 1621, 16204, 31679, 4892, 4161, 1536, 436, 25615, 11312, 3376, 23739, 32768, 1874, 5563], 32b: [11106, 2703, 32768, 1303, 29171, 32768, 17199, 1558, 18748, 2392, 9284, 13145, 32768, 4615, 2575, 7027, 13307, 1108, 4678, 13991, 7219, 2964, 1346, 2008, 21080, 17927, 3022, 24379, 24876, 2440, 3364]\n"
     ]
    }
   ],
   "source": [
    "print('num of thoughts')\n",
    "print(f'right: 1.5b: {avg_s['right'][0]}, 32b: {avg_s['right'][1]}')\n",
    "print(f'wrong: 1.5b: {avg_s['wrong'][0]}, 32b: {avg_s['wrong'][1]}')\n",
    "print('length of thoughts')\n",
    "print(f'right: 1.5b: {avg_s_l['right'][0]}, 32b: {avg_s_l['right'][1]}')\n",
    "print(f'wrong: 1.5b: {avg_s_l['wrong'][1]}, 32b: {avg_s_l['wrong'][1]}')\n",
    "print('avg num of tokens')\n",
    "print(f'right: 1.5b: {num_tokens['right'][0]}, 32b: {num_tokens['right'][1]}')\n",
    "print(f'wrong: 1.5b: {num_tokens['wrong'][0]}, 32b: {num_tokens['wrong'][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5b wrong, 32b wrong\n",
      "avg num of thoughts\n",
      "right: 1.5b: (0, 0), 32b: (0, 0)\n",
      "wrong: 1.5b: (84.74193548387096, 105.59813284698755), 32b: (148.32258064516128, 342.36722287124115)\n",
      "avg length of thoughts\n",
      "right: 1.5b: (0, 0), 32b: (0, 0)\n",
      "wrong: 1.5b: (262.14534376218126, 164.5740269550573), 32b: (242.66147039971776, 148.90751747128553)\n",
      "avg num of tokens\n",
      "right: 1.5b: (0, 0), 32b: (0, 0)\n",
      "wrong: 1.5b: (12878.193548387097, 12242.156584258622), 32b: (11704.483870967742, 10499.441118370394)\n"
     ]
    }
   ],
   "source": [
    "print('1.5b wrong, 32b wrong')\n",
    "print('avg num of thoughts')\n",
    "print(f'right: 1.5b: {avg(avg_s['right'][0])}, 32b: {avg(avg_s['right'][1])}')\n",
    "print(f'wrong: 1.5b: {avg(avg_s['wrong'][0])}, 32b: {avg(avg_s['wrong'][1])}')\n",
    "print('avg length of thoughts')\n",
    "print(f'right: 1.5b: {avg(avg_s_l['right'][0])}, 32b: {avg(avg_s_l['right'][1])}')\n",
    "print(f'wrong: 1.5b: {avg(avg_s_l['wrong'][0])}, 32b: {avg(avg_s_l['wrong'][1])}')\n",
    "print('avg num of tokens')\n",
    "print(f'right: 1.5b: {avg(num_tokens['right'][0])}, 32b: {avg(num_tokens['right'][1])}')\n",
    "print(f'wrong: 1.5b: {avg(num_tokens['wrong'][0])}, 32b: {avg(num_tokens['wrong'][1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 统计1.5正确，32b正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(406, 406)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = min(len(results_32b), len(results_1b))\n",
    "results_1b_2, results_32b_2 = [],[]\n",
    "for i in range(l):\n",
    "    r_32, r_1 = results_32b[i], results_1b[i]\n",
    "    r_32_right_flag = check_math_correctness(r_32['answer'], r_32['generated_answer'])\n",
    "    r_1_right_flag = check_math_correctness(r_1['answer'], r_1['generated_answer'])\n",
    "    if r_32['question']==r_1['question'] and (r_32_right_flag and r_1_right_flag):\n",
    "        results_1b_2.append(r_1)\n",
    "        results_32b_2.append(r_32)\n",
    "len(results_1b_2), len(results_32b_2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sections_all_1b = segement(results_1b_2)\n",
    "filtered_sections_all_32b = segement(results_32b_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import __init__\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Qwen2ForCausalLM\n",
    "target_model_name = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-32B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(target_model_name)\n",
    "\n",
    "avg_s, avg_s_l,num_tokens = {'right': [[], []], 'wrong': [[], []]}, {'right': [[], []], 'wrong': [[], []]},{'right': [[], []], 'wrong': [[], []]}\n",
    "\n",
    "for s_1 in filtered_sections_all_1b:\n",
    "    key1 = 'right' if s_1['right'] else 'wrong'\n",
    "    # 统计 filtered_sections 的数量\n",
    "    avg_s[key1][0].append(len(s_1['filtered_sections']))\n",
    "   \n",
    "    # # **使用 tokenizer 计算 token 个数**\n",
    "    token_count_1b = [len(tokenizer.encode(ss, add_special_tokens=False)) for ss in s_1['filtered_sections']]\n",
    "   \n",
    "    # 统计 filtered_sections 的 token 数量\n",
    "    avg_s_l[key1][0].append(avg_l(token_count_1b)) # s_1['num_tokens']/avg_s[key1][0][-1]\n",
    "    num_tokens[key1][0].append(s_1['num_tokens'])\n",
    "   \n",
    "for s_32 in filtered_sections_all_32b:\n",
    "    key32 = 'right' if s_32['right'] else 'wrong'\n",
    "\n",
    "    # 统计 filtered_sections 的数量\n",
    "    avg_s[key32][1].append(len(s_32['filtered_sections']))\n",
    "\n",
    "    # # **使用 tokenizer 计算 token 个数**\n",
    "    token_count_32b = [len(tokenizer.encode(ss, add_special_tokens=False)) for ss in s_32['filtered_sections']]\n",
    "\n",
    "    # 统计 filtered_sections 的 token 数量\n",
    "    avg_s_l[key32][1].append(avg_l(token_count_32b)) # s_32['num_tokens']/avg_s[key32][1][-1]\n",
    "    num_tokens[key32][1].append(s_32['num_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5b right, 32b right\n",
      "avg num of thoughts\n",
      "right: 1.5b: (18.435960591133004, 25.511604380505204), 32b: (11.60344827586207, 21.39420961050354)\n",
      "wrong: 1.5b: (0, 0), 32b: (0, 0)\n",
      "avg length of thoughts\n",
      "right: 1.5b: (256.357420164912, 129.21503140275453), 32b: (304.31729907181784, 128.80065452987168)\n",
      "wrong: 1.5b: (0, 0), 32b: (0, 0)\n",
      "avg num of tokens\n",
      "right: 1.5b: (3349.6428571428573, 3265.801907125861), 32b: (2641.6059113300494, 2089.4815081194038)\n",
      "wrong: 1.5b: (0, 0), 32b: (0, 0)\n"
     ]
    }
   ],
   "source": [
    "print('1.5b right, 32b right')\n",
    "print('avg num of thoughts')\n",
    "print(f'right: 1.5b: {avg(avg_s['right'][0])}, 32b: {avg(avg_s['right'][1])}')\n",
    "print(f'wrong: 1.5b: {avg(avg_s['wrong'][0])}, 32b: {avg(avg_s['wrong'][1])}')\n",
    "print('avg length of thoughts')\n",
    "print(f'right: 1.5b: {avg(avg_s_l['right'][0])}, 32b: {avg(avg_s_l['right'][1])}')\n",
    "print(f'wrong: 1.5b: {avg(avg_s_l['wrong'][0])}, 32b: {avg(avg_s_l['wrong'][1])}')\n",
    "print('avg num of tokens')\n",
    "print(f'right: 1.5b: {avg(num_tokens['right'][0])}, 32b: {avg(num_tokens['right'][1])}')\n",
    "print(f'wrong: 1.5b: {avg(num_tokens['wrong'][0])}, 32b: {avg(num_tokens['wrong'][1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 收集正确错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import save_all_results\n",
    "l = min(len(results_32b), len(results_1b))\n",
    "results = []\n",
    "for i in range(l):\n",
    "    r_32, r_1 = results_32b[i], results_1b[i]\n",
    "    r_32_right_flag = check_math_correctness(r_32['answer'], r_32['generated_answer'])\n",
    "    r_1_right_flag = check_math_correctness(r_1['answer'], r_1['generated_answer'])\n",
    "    if r_32['question']==r_1['question'] and ( r_32_right_flag and  r_1_right_flag):\n",
    "        # print(r_32.keys())\n",
    "        results.append({\n",
    "            'question': r_32['question'],\n",
    "            'true_answers': r_32['answer'],\n",
    "            '32b_ans': r_32['generated_answer'],\n",
    "            '32b_token_num': r_32['num_tokens'],\n",
    "            '1b_ans': r_1['generated_answer'],\n",
    "            '1b_token_num': r_1['num_tokens'],\n",
    "        })\n",
    "save_all_results('./32b_right_1b_right.jsonl', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9347 14719\n",
      "5100 2350\n",
      "699 419\n",
      "3247 3491\n",
      "4388 32768\n",
      "5531 14415\n",
      "8822 19203\n",
      "2771 5367\n",
      "2533 7904\n",
      "7748 11923\n",
      "3659 9021\n",
      "16913 32768\n",
      "6439 24709\n",
      "20512 21810\n",
      "6002 8237\n",
      "4348 32768\n",
      "6813 5159\n",
      "2194 9882\n",
      "2917 5367\n",
      "4980 32768\n",
      "5433 30590\n",
      "5038 2336\n",
      "1590 579\n",
      "6683 5938\n",
      "2364 13872\n",
      "1402 495\n",
      "4939 5397\n",
      "4715 7974\n",
      "4724 32768\n",
      "7646 29385\n",
      "1368 5023\n",
      "4870 9545\n"
     ]
    }
   ],
   "source": [
    "for r in results:\n",
    "    print(r['32b_token_num'], r['1b_token_num'])\n",
    "    r['32b_ans']\n",
    "    print(r['32b_ans'], r['1b_ans'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analysis_token_num(path1, path2):\n",
    "    result1 = read_saved_results(path1)\n",
    "    result2 = read_saved_results(path2)\n",
    "    l = min(len(result1), len(result2))\n",
    "\n",
    "    # 统计变量\n",
    "    path1_correct_path2_wrong = []\n",
    "    path1_wrong_path2_correct = []\n",
    "    path1_correct_tokens = []\n",
    "    path1_wrong_tokens = []\n",
    "    path2_tokens_on_path1_correct = []\n",
    "    path2_tokens_on_path1_wrong = []\n",
    "\n",
    "    for i in range(l):\n",
    "        r1, r2 = result1[i], result2[i]\n",
    "        generate_key1 = 'generated_text' if 'generated_text' in r1 else 'generated_answer'\n",
    "        generate_key2 = 'generated_text' if 'generated_text' in r2 else 'generated_answer'\n",
    "\n",
    "        right_flag1 = check_math_correctness(r1['answer'], r1[generate_key1])\n",
    "        right_flag2 = check_math_correctness(r1['answer'], r2[generate_key2])\n",
    "\n",
    "        num_tokens1 = r1['num_tokens']\n",
    "        num_tokens2 = r2['num_tokens']\n",
    "\n",
    "        if right_flag1 and not right_flag2:\n",
    "            path1_correct_path2_wrong.append(1)\n",
    "        if not right_flag1 and right_flag2:\n",
    "            path1_wrong_path2_correct.append(1)\n",
    "\n",
    "        if right_flag1:\n",
    "            path1_correct_tokens.append(num_tokens1)\n",
    "            path2_tokens_on_path1_correct.append(num_tokens2)\n",
    "        else:\n",
    "            path1_wrong_tokens.append(num_tokens1)\n",
    "            path2_tokens_on_path1_wrong.append(num_tokens2)\n",
    "\n",
    "    # 计算平均 token 数\n",
    "    def compute_avg(data):\n",
    "        return sum(data) / len(data) if data else 0\n",
    "\n",
    "    avg_tokens1_correct = compute_avg(path1_correct_tokens)\n",
    "    avg_tokens2_on_path1_correct = compute_avg(path2_tokens_on_path1_correct)\n",
    "    avg_tokens1_wrong = compute_avg(path1_wrong_tokens)\n",
    "    avg_tokens2_on_path1_wrong = compute_avg(path2_tokens_on_path1_wrong)\n",
    "\n",
    "    print(f\"Path1 正确但 Path2 错误的数量: {len(path1_correct_path2_wrong)}\")\n",
    "    print(f\"Path1 错误但 Path2 正确的数量: {len(path1_wrong_path2_correct)}\")\n",
    "    print(f\"Path1 在自己正确回答的题目上的平均 token 数: {avg_tokens1_correct:.2f}\")\n",
    "    print(f\"Path2 在 Path1 正确回答的题目上的平均 token 数: {avg_tokens2_on_path1_correct:.2f}\")\n",
    "    print(f\"Path1 在自己错误回答的题目上的平均 token 数: {avg_tokens1_wrong:.2f}\")\n",
    "    print(f\"Path2 在 Path1 错误回答的题目上的平均 token 数: {avg_tokens2_on_path1_wrong:.2f}\")\n",
    "\n",
    "def compare_token_num(path1, path2):\n",
    "    result1 = read_saved_results(path1)\n",
    "    result2 = read_saved_results(path2)\n",
    "    l = min(len(result1), len(result2))\n",
    "    for i in range(l):\n",
    "        r1,r2 = result1[i], result2[i]\n",
    "        generate_key = 'generated_text'\n",
    "        if generate_key not in r1.keys(): generate_key = 'generated_answer'\n",
    "        right_flag1 = check_math_correctness(r1['answer'], r1[generate_key])\n",
    "        generate_key = 'generated_text'\n",
    "        if generate_key not in r2.keys(): generate_key = 'generated_answer'\n",
    "        right_flag2 = check_math_correctness(r1['answer'], r2[generate_key])\n",
    "        if not right_flag1 and right_flag2:\n",
    "            print(r1['question'][:15])\n",
    "            print(f'# :{i+1}, {r1['num_tokens']}, {right_flag1}, {r2['num_tokens']}, {right_flag2}, correct_num: {len(r2['corrected_tokens'])}')\n",
    "\n",
    "def check_results(paths, show=False):\n",
    "    generate_key = 'generated_text' \n",
    "    results = [item for path in paths for item in read_saved_results(path)]\n",
    "    correct_num, all_time, all_tokens_num, num_8k, num_corr = 0,0,0,0,0\n",
    "    all_right_tokens_num, all_wrong_tokens_num, wrong_num = 0, 0 ,0\n",
    "    for r_i, r in enumerate(results):\n",
    "        if generate_key not in r.keys(): generate_key = 'generated_answer'\n",
    "        right_flag = check_math_correctness(r['answer'], r[generate_key])\n",
    "        all_time = all_time + r['generation_time']\n",
    "        all_tokens_num = all_tokens_num + r['num_tokens']\n",
    "        # num_corr = num_corr + sum([i['token_num'] for i in r['corrected_tokens']])\n",
    "        if not right_flag:\n",
    "            if show: print('#:',r['index'] if r['index'] is not None else r_i+1 , 'ans:' ,r['answer'], 'time:' ,r['generation_time'], 'num_tokens' ,r['num_tokens'], 'num_corr', len(r['corrected_tokens']))\n",
    "            if r['num_tokens'] > 8*1024: num_8k = num_8k +1\n",
    "            wrong_num = wrong_num+1\n",
    "            all_wrong_tokens_num = all_wrong_tokens_num+r['num_tokens']\n",
    "        else:\n",
    "            correct_num = correct_num + 1\n",
    "            all_right_tokens_num = all_right_tokens_num+r['num_tokens']\n",
    "            \n",
    "    # print(len(results), all_time, all_tokens_num)\n",
    "    print(f'num: {len(results)}, acc: {correct_num/len(results)}, all_time: {all_time}, tokens/s: {all_tokens_num/all_time}, token_num:{all_tokens_num/len(results)}, num_8k:{num_8k}, num_corr:{num_corr/all_tokens_num}')\n",
    "    print('wrong avg tokens', all_wrong_tokens_num/wrong_num, 'wrong_num', wrong_num, 'right avg tokens' ,all_right_tokens_num/correct_num, 'correct_num', correct_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#: 1 ans: 756 time: 5760.410681962967 num_tokens 24109 num_corr 60\n",
      "#: 2 ans: 150 time: 932.1434590816498 num_tokens 4434 num_corr 4\n",
      "#: 3 ans: 245 time: 5291.601450681686 num_tokens 22410 num_corr 46\n",
      "#: 4 ans: 392 time: 5999.1504373550415 num_tokens 26738 num_corr 70\n",
      "#: 5 ans: 459 time: 3864.1774096488953 num_tokens 16604 num_corr 47\n",
      "#: 6 ans: 033 time: 4941.935383558273 num_tokens 19594 num_corr 45\n",
      "#: 8 ans: 242 time: 4972.6334528923035 num_tokens 21697 num_corr 58\n",
      "#: 9 ans: 834 time: 2621.315885782242 num_tokens 10296 num_corr 33\n",
      "#: 10 ans: 550 time: 5822.069530963898 num_tokens 23397 num_corr 53\n",
      "#: 11 ans: 228 time: 3527.799068212509 num_tokens 15898 num_corr 52\n",
      "#: 12 ans: 289 time: 5546.274833917618 num_tokens 21428 num_corr 96\n",
      "#: 13 ans: 378 time: 6394.402061700821 num_tokens 27480 num_corr 81\n",
      "#: 17 ans: 180 time: 5123.989091873169 num_tokens 22638 num_corr 37\n",
      "#: 18 ans: 023 time: 5473.344890117645 num_tokens 22392 num_corr 34\n",
      "#: 19 ans: 220 time: 2735.0136892795563 num_tokens 12401 num_corr 20\n",
      "#: 20 ans: 188 time: 2197.66991686821 num_tokens 11009 num_corr 15\n",
      "#: 21 ans: 140 time: 4053.047815799713 num_tokens 17996 num_corr 49\n",
      "#: 22 ans: 125 time: 5233.002938985825 num_tokens 23171 num_corr 73\n",
      "#: 25 ans: 072 time: 5192.541455030441 num_tokens 23424 num_corr 78\n",
      "#: 26 ans: 841 time: 1125.1026058197021 num_tokens 5403 num_corr 9\n",
      "#: 27 ans: 192 time: 4219.608345985413 num_tokens 18475 num_corr 37\n",
      "#: 28 ans: 080 time: 3624.311557531357 num_tokens 16715 num_corr 37\n",
      "#: 29 ans: 244 time: 4537.230962276459 num_tokens 17418 num_corr 49\n",
      "#: 30 ans: 191 time: 2882.9137930870056 num_tokens 13127 num_corr 33\n",
      "#: 31 ans: 944 time: 6968.777136802673 num_tokens 32640 num_corr 37\n",
      "#: 32 ans: 235 time: 3604.15704202652 num_tokens 15078 num_corr 53\n",
      "#: 33 ans: 075 time: 4247.425406217575 num_tokens 18322 num_corr 45\n",
      "#: 34 ans: 125 time: 5861.853877782822 num_tokens 25179 num_corr 70\n",
      "#: 35 ans: 608 time: 3644.0463840961456 num_tokens 17328 num_corr 37\n",
      "#: 36 ans: 349 time: 5987.560253381729 num_tokens 26138 num_corr 43\n",
      "#: 38 ans: 607 time: 1063.4938988685608 num_tokens 4891 num_corr 7\n",
      "#: 40 ans: 106 time: 2952.4479274749756 num_tokens 13045 num_corr 21\n",
      "#: 41 ans: 051 time: 1797.600704908371 num_tokens 9008 num_corr 19\n",
      "#: 42 ans: 049 time: 1878.5154674053192 num_tokens 8805 num_corr 26\n",
      "#: 43 ans: 125 time: 3958.5747730731964 num_tokens 17012 num_corr 47\n",
      "#: 44 ans: 738 time: 5208.119018793106 num_tokens 23893 num_corr 61\n",
      "#: 46 ans: 144 time: 3477.519889831543 num_tokens 14681 num_corr 59\n",
      "#: 47 ans: 081 time: 1569.0431470870972 num_tokens 8243 num_corr 17\n",
      "#: 48 ans: 247 time: 4470.32485294342 num_tokens 20267 num_corr 24\n",
      "#: 49 ans: 167 time: 7690.522872924805 num_tokens 31427 num_corr 80\n",
      "#: 50 ans: 751 time: 5174.695562362671 num_tokens 20431 num_corr 76\n",
      "#: 51 ans: 363 time: 6727.312206983566 num_tokens 30897 num_corr 60\n",
      "#: 52 ans: 585 time: 7922.413763284683 num_tokens 32684 num_corr 82\n",
      "#: 53 ans: 250 time: 1462.2223672866821 num_tokens 5961 num_corr 18\n",
      "#: 54 ans: 273 time: 2119.471142053604 num_tokens 8658 num_corr 13\n",
      "#: 56 ans: 35 time: 4765.606787204742 num_tokens 20673 num_corr 52\n",
      "#: 57 ans: 928 time: 3393.678733110428 num_tokens 13704 num_corr 52\n",
      "#: 58 ans: 024 time: 3469.5331683158875 num_tokens 16117 num_corr 31\n",
      "#: 59 ans: 033 time: 4465.9814438819885 num_tokens 18301 num_corr 68\n",
      "#: 61 ans: 113 time: 5189.482335329056 num_tokens 22293 num_corr 53\n",
      "#: 62 ans: 371 time: 5772.838279724121 num_tokens 26487 num_corr 73\n",
      "#: 63 ans: 385 time: 5153.551939725876 num_tokens 23091 num_corr 39\n",
      "#: 64 ans: 110 time: 6399.196018457413 num_tokens 27903 num_corr 54\n",
      "#: 65 ans: 104 time: 1733.266607761383 num_tokens 8193 num_corr 17\n",
      "#: 66 ans: 721 time: 6838.893040180206 num_tokens 27899 num_corr 71\n",
      "#: 70 ans: 104 time: 4606.626324892044 num_tokens 19391 num_corr 39\n",
      "#: 73 ans: 197 time: 4814.47962975502 num_tokens 20610 num_corr 62\n",
      "#: 74 ans: 480 time: 3290.5745708942413 num_tokens 15021 num_corr 49\n",
      "#: 75 ans: 073 time: 4035.0026507377625 num_tokens 15882 num_corr 60\n",
      "#: 76 ans: 468 time: 3913.494626522064 num_tokens 16045 num_corr 57\n",
      "#: 77 ans: 601 time: 6028.536724805832 num_tokens 27517 num_corr 48\n",
      "#: 78 ans: 023 time: 4368.994109869003 num_tokens 19446 num_corr 37\n",
      "#: 80 ans: 211 time: 5121.072566986084 num_tokens 23786 num_corr 38\n",
      "#: 81 ans: 315 time: 4588.300475597382 num_tokens 17994 num_corr 74\n",
      "#: 82 ans: 236 time: 4720.4991106987 num_tokens 18916 num_corr 53\n",
      "#: 83 ans: 045 time: 4952.729556798935 num_tokens 23588 num_corr 33\n",
      "#: 85 ans: 080 time: 5584.558236837387 num_tokens 24685 num_corr 68\n",
      "#: 87 ans: 699 time: 4747.437609434128 num_tokens 24070 num_corr 66\n",
      "#: 88 ans: 127 time: 4140.331246376038 num_tokens 23151 num_corr 28\n",
      "#: 89 ans: 902 time: 1448.552520275116 num_tokens 7054 num_corr 22\n",
      "num: 90, acc: 0.2222222222222222, all_time: 335735.8491053581, tokens/s: 4.371246037355557, token_num:16306.488888888889, num_8k:65, num_corr:0.0\n",
      "wrong avg tokens 18923.7 wrong_num 70 right avg tokens 7146.25 correct_num 20\n"
     ]
    }
   ],
   "source": [
    "check_results(['/home/wxy320/ondemand/program/speculative_thinking/results/last/AIME_deepseek-32b_deepseek-1.5b_3.json',\n",
    "               ], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#: 1 ans: 756 time: 5812.8784096241 num_tokens 29985 num_corr 64\n",
      "#: 2 ans: 150 time: 4657.571393966675 num_tokens 23484 num_corr 40\n",
      "#: 3 ans: 245 time: 2225.4117362499237 num_tokens 12959 num_corr 28\n",
      "#: 4 ans: 392 time: 2875.3013627529144 num_tokens 16479 num_corr 39\n",
      "#: 6 ans: 033 time: 5444.34498167038 num_tokens 27946 num_corr 46\n",
      "#: 9 ans: 834 time: 1341.4393928050995 num_tokens 7032 num_corr 13\n",
      "#: 11 ans: 228 time: 2462.73859333992 num_tokens 13217 num_corr 32\n",
      "#: 12 ans: 289 time: 1713.7863729000092 num_tokens 8682 num_corr 12\n",
      "#: 13 ans: 378 time: 5555.348180532455 num_tokens 29762 num_corr 63\n",
      "#: 14 ans: 247 time: 1519.7115545272827 num_tokens 8526 num_corr 16\n",
      "#: 18 ans: 023 time: 3044.2661249637604 num_tokens 16989 num_corr 19\n",
      "#: 19 ans: 220 time: 1220.5234715938568 num_tokens 7028 num_corr 7\n",
      "#: 20 ans: 188 time: 2756.3587975502014 num_tokens 14932 num_corr 20\n",
      "#: 21 ans: 140 time: 3605.2792670726776 num_tokens 19297 num_corr 32\n",
      "#: 22 ans: 125 time: 1513.7599205970764 num_tokens 7775 num_corr 17\n",
      "#: 24 ans: 112 time: 2527.0749077796936 num_tokens 13625 num_corr 25\n",
      "#: 26 ans: 841 time: 1182.304703950882 num_tokens 6796 num_corr 10\n",
      "num: 24, acc: 0.2916666666666667, all_time: 56220.64365005493, tokens/s: 5.357409315247242, token_num:12549.875, num_8k:13, num_corr:0.0\n",
      "wrong avg tokens 15559.64705882353 wrong_num 17 right avg tokens 5240.428571428572 correct_num 7\n"
     ]
    }
   ],
   "source": [
    "check_results(['/home/wxy320/ondemand/program/speculative_thinking/results/AIME_deepseek-32b_deepseek-1.5b_new_0_90.json',\n",
    "               ], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#: 9 ans: 4 time: 1022.0049817562103 num_tokens 15409 num_corr 39\n",
      "num: 10, acc: 0.9, all_time: 4050.9482831954956, tokens/s: 8.238564816649227, token_num:3337.4, num_8k:1, num_corr:0.0\n",
      "wrong avg tokens 15409.0 wrong_num 1 right avg tokens 1996.111111111111 correct_num 9\n"
     ]
    }
   ],
   "source": [
    "check_results(['/home/wxy320/ondemand/program/speculative_thinking/results/MATH500_deepseek-32b_deepseek-1.5b_new_0_250.json',\n",
    "               ], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#: 10 ans: 4 time: 577.8739280700684 num_tokens 14719 num_corr 0\n",
      "#: 12 ans: \\frac{3}{56} time: 89.44219088554382 num_tokens 2350 num_corr 0\n",
      "#: 19 ans: 28 time: 1384.5067791938782 num_tokens 32768 num_corr 0\n",
      "#: 26 ans: 1,-2 time: 132.84535932540894 num_tokens 3491 num_corr 0\n",
      "#: 27 ans: 144 time: 1363.5159804821014 num_tokens 32768 num_corr 0\n",
      "#: 37 ans: 3, 5, 7 time: 91.09839940071106 num_tokens 2394 num_corr 0\n",
      "#: 42 ans: 17 time: 555.6910262107849 num_tokens 14415 num_corr 0\n",
      "#: 44 ans: 70 \\sqrt{2} time: 752.8760311603546 num_tokens 19203 num_corr 0\n",
      "#: 51 ans: 203 time: 204.41261982917786 num_tokens 5367 num_corr 0\n",
      "#: 64 ans: 10 time: 301.4226870536804 num_tokens 7904 num_corr 0\n",
      "#: 81 ans: 501 time: 458.16904187202454 num_tokens 11923 num_corr 0\n",
      "#: 83 ans: \\frac{3}{2} time: 344.1525957584381 num_tokens 9021 num_corr 0\n",
      "#: 97 ans: 1 \\pm \\sqrt{19} time: 1367.780562877655 num_tokens 32768 num_corr 0\n",
      "#: 98 ans: \\text{east} time: 47.863200664520264 num_tokens 1254 num_corr 0\n",
      "#: 101 ans: 145^\\circ time: 1363.43519282341 num_tokens 32768 num_corr 0\n",
      "#: 102 ans: 850 time: 995.4002861976624 num_tokens 24709 num_corr 0\n",
      "#: 104 ans: (3,4] time: 872.5779769420624 num_tokens 21810 num_corr 0\n",
      "#: 111 ans: 504 time: 1212.6594445705414 num_tokens 29625 num_corr 0\n",
      "#: 120 ans: -9 time: 312.54219913482666 num_tokens 8237 num_corr 0\n",
      "#: 121 ans: \\text{even} time: 1355.8357775211334 num_tokens 32768 num_corr 0\n",
      "#: 129 ans: 36 time: 61.035775661468506 num_tokens 1611 num_corr 0\n",
      "#: 139 ans: \\frac{1}{3} time: 196.3023488521576 num_tokens 5159 num_corr 0\n",
      "#: 143 ans: 28800 time: 377.76360511779785 num_tokens 9882 num_corr 0\n",
      "#: 151 ans: 1 time: 207.47449731826782 num_tokens 5367 num_corr 0\n",
      "#: 155 ans: 2\\sqrt{113} time: 463.0736801624298 num_tokens 11891 num_corr 0\n",
      "#: 157 ans: 30^\\circ time: 1370.5991733074188 num_tokens 32768 num_corr 0\n",
      "#: 169 ans: 19 time: 1273.094927072525 num_tokens 30590 num_corr 0\n",
      "#: 189 ans: 6 time: 88.9849328994751 num_tokens 2336 num_corr 0\n",
      "#: 199 ans: 10,\\!080 time: 51.990827322006226 num_tokens 1367 num_corr 0\n",
      "#: 201 ans: 40 time: 22.01841378211975 num_tokens 579 num_corr 0\n",
      "#: 205 ans: 0 time: 661.965677022934 num_tokens 17010 num_corr 0\n",
      "#: 206 ans: \\begin{pmatrix} -2 \\\\ -14 \\\\ -7 \\end{pmatrix} time: 136.56483125686646 num_tokens 3601 num_corr 0\n",
      "#: 211 ans: 10 time: 227.192720413208 num_tokens 5938 num_corr 0\n",
      "#: 215 ans: 110 time: 537.7400476932526 num_tokens 13872 num_corr 0\n",
      "#: 217 ans: 6 time: 19.011309385299683 num_tokens 495 num_corr 0\n",
      "#: 218 ans: 11,\\! 111,\\! 111,\\! 100 time: 1369.642172574997 num_tokens 32768 num_corr 0\n",
      "#: 220 ans: \\frac{1}{2} time: 207.6727418899536 num_tokens 5397 num_corr 0\n",
      "#: 223 ans: 54 time: 310.15176582336426 num_tokens 7974 num_corr 0\n",
      "#: 237 ans: 12 time: 1374.6689505577087 num_tokens 32768 num_corr 0\n",
      "#: 240 ans: 16 time: 1212.2632386684418 num_tokens 29385 num_corr 0\n",
      "#: 241 ans: 116 time: 122.41171550750732 num_tokens 3181 num_corr 0\n",
      "#: 243 ans: \\$32,\\!348 time: 1376.7893598079681 num_tokens 32768 num_corr 0\n",
      "#: 247 ans: 8 time: 193.04121732711792 num_tokens 5023 num_corr 0\n",
      "#: 249 ans: \\begin{pmatrix} -1 & 0 \\\\ 0 & -1 \\end{pmatrix} time: 399.23257088661194 num_tokens 9545 num_corr 0\n",
      "#: 265 ans: 1\\frac{4}{5} time: 671.1467471122742 num_tokens 17171 num_corr 0\n",
      "#: 268 ans: 180^\\circ time: 452.93306374549866 num_tokens 11823 num_corr 0\n",
      "#: 280 ans: 7\\pi time: 164.89210081100464 num_tokens 4334 num_corr 0\n",
      "#: 285 ans: 3 \\pm 2 \\sqrt{2} time: 169.0683786869049 num_tokens 4442 num_corr 0\n",
      "#: 286 ans: 440 time: 744.0804722309113 num_tokens 18955 num_corr 0\n",
      "#: 287 ans: \\frac{17}{21} time: 1021.7126474380493 num_tokens 25295 num_corr 0\n",
      "#: 292 ans: 63 time: 1372.217128753662 num_tokens 32768 num_corr 0\n",
      "#: 296 ans: 11 time: 140.02709650993347 num_tokens 3673 num_corr 0\n",
      "#: 299 ans: \\begin{pmatrix} -18 \\\\ -49 \\\\ 96 \\end{pmatrix} time: 79.34104895591736 num_tokens 2088 num_corr 0\n",
      "#: 302 ans: 5.4 \\text{ cents} time: 155.11944460868835 num_tokens 3929 num_corr 0\n",
      "#: 304 ans: 28 time: 1085.9042909145355 num_tokens 26589 num_corr 0\n",
      "#: 307 ans: 6 time: 312.4625587463379 num_tokens 8279 num_corr 0\n",
      "#: 309 ans: 1+2\\sqrt{3} time: 640.8706059455872 num_tokens 16633 num_corr 0\n",
      "#: 319 ans: \\frac{11+9a}{20} time: 60.58938002586365 num_tokens 1621 num_corr 0\n",
      "#: 325 ans: 120 time: 620.270848274231 num_tokens 16204 num_corr 0\n",
      "#: 327 ans: \\frac{9}{100} time: 256.19145584106445 num_tokens 6735 num_corr 0\n",
      "#: 328 ans: 15 time: 1035.232753753662 num_tokens 25705 num_corr 0\n",
      "#: 341 ans: 55^\\circ time: 1301.2735767364502 num_tokens 31679 num_corr 0\n",
      "#: 350 ans: 10080 time: 322.3257083892822 num_tokens 8461 num_corr 0\n",
      "#: 359 ans: \\text{ellipse} time: 373.961829662323 num_tokens 9713 num_corr 0\n",
      "#: 370 ans: 21 time: 186.01529240608215 num_tokens 4892 num_corr 0\n",
      "#: 380 ans: 12 time: 159.9349341392517 num_tokens 4161 num_corr 0\n",
      "#: 382 ans: \\frac{1}{2} time: 561.9892177581787 num_tokens 14513 num_corr 0\n",
      "#: 384 ans: x \\in [-2,7] time: 59.16363596916199 num_tokens 1536 num_corr 0\n",
      "#: 386 ans: 22 time: 16.798898220062256 num_tokens 436 num_corr 0\n",
      "#: 393 ans: \\sqrt{66} time: 1033.9468622207642 num_tokens 25615 num_corr 0\n",
      "#: 399 ans: \\text{Navin} time: 26.866779088974 num_tokens 704 num_corr 0\n",
      "#: 401 ans: \\frac{14}{3} time: 432.2985236644745 num_tokens 11312 num_corr 0\n",
      "#: 403 ans: 1 - 12i time: 1375.51172208786 num_tokens 32768 num_corr 0\n",
      "#: 420 ans: \\begin{pmatrix} -7 \\\\ 16 \\\\ 5 \\end{pmatrix} time: 129.4515426158905 num_tokens 3376 num_corr 0\n",
      "#: 423 ans: \\{1\\pm\\sqrt{5},-2\\} time: 953.9175131320953 num_tokens 23739 num_corr 0\n",
      "#: 426 ans: 3R^2 time: 1371.2014639377594 num_tokens 32768 num_corr 0\n",
      "#: 432 ans: \\left[ \\frac{\\pi^2}{8}, \\frac{5 \\pi^2}{4} \\right] time: 148.5179479122162 num_tokens 3896 num_corr 0\n",
      "#: 440 ans: x^8 + x^7 + x^6 + x^5 + x^4 + x^3 + x^2 + x + 1 time: 367.79940485954285 num_tokens 9527 num_corr 0\n",
      "#: 445 ans: \\frac{1}{2} time: 1377.2743577957153 num_tokens 32768 num_corr 0\n",
      "#: 457 ans: -2,1 time: 71.71126508712769 num_tokens 1874 num_corr 0\n",
      "#: 461 ans: 75^\\circ time: 212.30138087272644 num_tokens 5563 num_corr 0\n",
      "#: 467 ans: 8 \\pi time: 297.4228069782257 num_tokens 7795 num_corr 0\n",
      "#: 474 ans: 7 time: 771.182234287262 num_tokens 19401 num_corr 0\n",
      "#: 482 ans: 8n^2 + 4n + 1 time: 1367.0380945205688 num_tokens 32768 num_corr 0\n",
      "#: 486 ans: 13 time: 1369.0197429656982 num_tokens 32768 num_corr 0\n",
      "#: 487 ans: 216 time: 103.59409093856812 num_tokens 2711 num_corr 0\n",
      "#: 491 ans: 14 time: 336.3266625404358 num_tokens 8732 num_corr 0\n",
      "#: 492 ans: 64 time: 23.146773099899292 num_tokens 601 num_corr 0\n",
      "#: 495 ans: \\frac{1}{4} time: 324.3684997558594 num_tokens 8453 num_corr 0\n",
      "num: 500, acc: 0.822, all_time: 103890.68571519852, tokens/s: 25.41097868231503, token_num:5279.928, num_8k:51, num_corr:0.0\n",
      "wrong avg tokens 13936.078651685393 wrong_num 89 right avg tokens 3405.4817518248174 correct_num 411\n"
     ]
    }
   ],
   "source": [
    "check_results(['/home/wxy320/ondemand/program/speculative_thinking/results/MATH500_deepseek-1.5b_None_0.01.json'], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num: 500, acc: 0.93, all_time: 173507.94639635086, tokens/s: 10.220673097870845, token_num:3546.736, num_8k:16, num_corr:0.0\n",
      "wrong avg tokens 11283.42857142857 wrong_num 35 right avg tokens 2964.404301075269 correct_num 465\n"
     ]
    }
   ],
   "source": [
    "check_results('/home/wxy320/ondemand/program/speculative_thinking/results/MATH500_deepseek-32b_None_0.01.json', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#: 12 ans: \\frac{3}{56} time: 1180.6640040874481 num_tokens 20542 num_corr 48\n",
      "#: 19 ans: 28 time: 1111.8239932060242 num_tokens 18621 num_corr 42\n",
      "#: 26 ans: 1,-2 time: 150.99302506446838 num_tokens 3631 num_corr 4\n",
      "#: 27 ans: 144 time: 797.9254188537598 num_tokens 12570 num_corr 44\n",
      "#: 37 ans: 3, 5, 7 time: 222.4329538345337 num_tokens 4304 num_corr 5\n",
      "#: 51 ans: 203 time: 176.77258801460266 num_tokens 3582 num_corr 6\n",
      "#: 72 ans: 2516_8 time: 731.7306206226349 num_tokens 11225 num_corr 44\n",
      "#: 81 ans: 501 time: 665.2199766635895 num_tokens 12182 num_corr 33\n",
      "#: 95 ans: 80 time: 575.9955034255981 num_tokens 10187 num_corr 24\n",
      "#: 97 ans: 1 \\pm \\sqrt{19} time: 1432.8203518390656 num_tokens 24653 num_corr 52\n",
      "#: 98 ans: \\text{east} time: 143.71602129936218 num_tokens 2654 num_corr 8\n",
      "#: 101 ans: 145^\\circ time: 565.9280021190643 num_tokens 10148 num_corr 13\n",
      "#: 102 ans: 850 time: 574.1340522766113 num_tokens 11446 num_corr 17\n",
      "#: 104 ans: (3,4] time: 1457.0339226722717 num_tokens 23579 num_corr 39\n",
      "#: 111 ans: 504 time: 1947.7815916538239 num_tokens 31456 num_corr 56\n",
      "#: 127 ans: 1+274i time: 1437.6338436603546 num_tokens 24993 num_corr 59\n",
      "#: 139 ans: \\frac{1}{3} time: 202.39403200149536 num_tokens 4046 num_corr 5\n",
      "#: 155 ans: 2\\sqrt{113} time: 929.1013524532318 num_tokens 15981 num_corr 39\n",
      "#: 167 ans: \\frac{270}7\\text{ degrees} time: 831.7715113162994 num_tokens 15502 num_corr 43\n",
      "#: 169 ans: 19 time: 1082.6107640266418 num_tokens 16448 num_corr 63\n",
      "#: 177 ans: .0000672 time: 801.9889149665833 num_tokens 15700 num_corr 33\n",
      "#: 178 ans: 30^\\circ time: 1266.3236010074615 num_tokens 20493 num_corr 68\n",
      "#: 199 ans: 10,\\!080 time: 192.918701171875 num_tokens 3752 num_corr 12\n",
      "#: 205 ans: 0 time: 1884.4945030212402 num_tokens 28101 num_corr 102\n",
      "#: 206 ans: \\begin{pmatrix} -2 \\\\ -14 \\\\ -7 \\end{pmatrix} time: 93.66812062263489 num_tokens 2118 num_corr 2\n",
      "#: 214 ans: 2 time: 313.1653563976288 num_tokens 5668 num_corr 8\n",
      "#: 215 ans: 110 time: 292.20286297798157 num_tokens 4712 num_corr 12\n",
      "#: 217 ans: 6 time: 27.23903751373291 num_tokens 600 num_corr 0\n",
      "#: 218 ans: 11,\\! 111,\\! 111,\\! 100 time: 461.79641008377075 num_tokens 9316 num_corr 21\n",
      "#: 220 ans: \\frac{1}{2} time: 840.2524750232697 num_tokens 14365 num_corr 36\n",
      "#: 233 ans: \\frac{1997}{2} time: 497.94694328308105 num_tokens 9451 num_corr 14\n",
      "#: 241 ans: 116 time: 860.3029823303223 num_tokens 15023 num_corr 41\n",
      "#: 265 ans: 1\\frac{4}{5} time: 848.2729637622833 num_tokens 14569 num_corr 38\n",
      "#: 285 ans: 3 \\pm 2 \\sqrt{2} time: 201.38488030433655 num_tokens 3848 num_corr 7\n",
      "#: 287 ans: \\frac{17}{21} time: 1514.848400592804 num_tokens 25040 num_corr 59\n",
      "#: 299 ans: \\begin{pmatrix} -18 \\\\ -49 \\\\ 96 \\end{pmatrix} time: 139.27400994300842 num_tokens 2570 num_corr 0\n",
      "#: 304 ans: 28 time: 988.9609429836273 num_tokens 14920 num_corr 45\n",
      "#: 307 ans: 6 time: 711.08380818367 num_tokens 12701 num_corr 43\n",
      "#: 309 ans: 1+2\\sqrt{3} time: 1515.464747428894 num_tokens 23044 num_corr 93\n",
      "#: 319 ans: \\frac{11+9a}{20} time: 80.44375157356262 num_tokens 1578 num_corr 2\n",
      "#: 325 ans: 120 time: 1070.553956747055 num_tokens 17974 num_corr 50\n",
      "#: 327 ans: \\frac{9}{100} time: 296.66515731811523 num_tokens 5336 num_corr 10\n",
      "#: 341 ans: 55^\\circ time: 1878.2500607967377 num_tokens 29742 num_corr 43\n",
      "#: 352 ans: \\frac{1}{16} time: 424.15173745155334 num_tokens 8274 num_corr 6\n",
      "#: 366 ans: 8 time: 939.4935052394867 num_tokens 17388 num_corr 26\n",
      "#: 369 ans: 31 time: 50.70672249794006 num_tokens 1153 num_corr 1\n",
      "#: 370 ans: 21 time: 519.9280562400818 num_tokens 10201 num_corr 17\n",
      "#: 382 ans: \\frac{1}{2} time: 668.850161075592 num_tokens 10693 num_corr 31\n",
      "#: 384 ans: x \\in [-2,7] time: 78.62548995018005 num_tokens 1459 num_corr 0\n",
      "#: 386 ans: 22 time: 23.773098945617676 num_tokens 559 num_corr 0\n",
      "#: 393 ans: \\sqrt{66} time: 1298.5158157348633 num_tokens 19731 num_corr 69\n",
      "#: 394 ans: \\frac 34 time: 77.7542655467987 num_tokens 1608 num_corr 2\n",
      "#: 401 ans: \\frac{14}{3} time: 614.4096887111664 num_tokens 11057 num_corr 20\n",
      "#: 404 ans: 3 time: 386.8041262626648 num_tokens 6386 num_corr 26\n",
      "#: 420 ans: \\begin{pmatrix} -7 \\\\ 16 \\\\ 5 \\end{pmatrix} time: 255.85713577270508 num_tokens 4873 num_corr 9\n",
      "#: 423 ans: \\{1\\pm\\sqrt{5},-2\\} time: 1327.716822385788 num_tokens 20883 num_corr 57\n",
      "#: 433 ans: 331 time: 1230.6247556209564 num_tokens 20484 num_corr 43\n",
      "#: 442 ans: 550 time: 20.798059225082397 num_tokens 482 num_corr 0\n",
      "#: 445 ans: \\frac{1}{2} time: 1180.9237101078033 num_tokens 18546 num_corr 42\n",
      "#: 457 ans: -2,1 time: 120.16529059410095 num_tokens 2371 num_corr 2\n",
      "#: 460 ans: \\$18.90 time: 334.5362458229065 num_tokens 5747 num_corr 16\n",
      "#: 471 ans: 27648 time: 1435.4461410045624 num_tokens 21985 num_corr 70\n",
      "#: 479 ans: \\frac{2}{1005} time: 302.106240272522 num_tokens 6033 num_corr 11\n",
      "#: 482 ans: 8n^2 + 4n + 1 time: 1624.4723978042603 num_tokens 26301 num_corr 71\n",
      "#: 486 ans: 13 time: 367.5554883480072 num_tokens 6266 num_corr 19\n",
      "#: 491 ans: 14 time: 1311.5051219463348 num_tokens 17637 num_corr 78\n",
      "#: 492 ans: 64 time: 30.64808201789856 num_tokens 634 num_corr 0\n",
      "#: 496 ans: (2,12) \\cup (12,102) time: 291.258403301239 num_tokens 5679 num_corr 12\n",
      "num: 500, acc: 0.864, all_time: 135138.0871887207, tokens/s: 18.13417705533826, token_num:4901.236, num_8k:41, num_corr:0.0\n",
      "wrong avg tokens 11835.308823529413 wrong_num 68 right avg tokens 3809.761574074074 correct_num 432\n"
     ]
    }
   ],
   "source": [
    "check_results(['/home/wxy320/ondemand/program/speculative_thinking/results/MATH500_deepseek-32b_deepseek-1.5b_2.json'], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num: 500, acc: 0.836, all_time: 114736.51292514801, tokens/s: 22.358462311588426, token_num:5130.664, num_8k:51, num_corr:0.03515334467429557\n",
      "wrong avg tokens 12513.475609756097 wrong_num 82 right avg tokens 3682.3612440191387 correct_num 418\n"
     ]
    }
   ],
   "source": [
    "check_results('/home/wxy320/ondemand/program/speculative_thinking/results/MATH500_deepseek-7b_deepseek-1.5b.json', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#: 10 ans: 4 time: 1710.6111454963684 num_tokens 32768 num_corr 0\n",
      "#: 12 ans: \\frac{3}{56} time: 112.86040902137756 num_tokens 2934 num_corr 0\n",
      "#: 26 ans: 1,-2 time: 133.54089403152466 num_tokens 3486 num_corr 0\n",
      "#: 37 ans: 3, 5, 7 time: 185.07589149475098 num_tokens 4806 num_corr 0\n",
      "#: 95 ans: 80 time: 122.84628677368164 num_tokens 3189 num_corr 0\n",
      "#: 97 ans: 1 \\pm \\sqrt{19} time: 1355.6965217590332 num_tokens 28561 num_corr 0\n",
      "#: 98 ans: \\text{east} time: 197.4112319946289 num_tokens 5066 num_corr 0\n",
      "#: 101 ans: 145^\\circ time: 781.9172472953796 num_tokens 17974 num_corr 0\n",
      "#: 111 ans: 504 time: 1311.6970191001892 num_tokens 27436 num_corr 0\n",
      "#: 127 ans: 1+274i time: 643.4781391620636 num_tokens 15171 num_corr 0\n",
      "#: 155 ans: 2\\sqrt{113} time: 525.5147893428802 num_tokens 12651 num_corr 0\n",
      "#: 167 ans: \\frac{270}7\\text{ degrees} time: 558.7211472988129 num_tokens 13355 num_corr 0\n",
      "#: 199 ans: 10,\\!080 time: 43.37658166885376 num_tokens 1112 num_corr 0\n",
      "#: 205 ans: 0 time: 1400.1139018535614 num_tokens 29086 num_corr 0\n",
      "#: 206 ans: \\begin{pmatrix} -2 \\\\ -14 \\\\ -7 \\end{pmatrix} time: 140.10291004180908 num_tokens 3598 num_corr 0\n",
      "#: 218 ans: 11,\\! 111,\\! 111,\\! 100 time: 1060.3553278446198 num_tokens 23203 num_corr 0\n",
      "#: 220 ans: \\frac{1}{2} time: 200.04227423667908 num_tokens 5072 num_corr 0\n",
      "#: 233 ans: \\frac{1997}{2} time: 701.8752353191376 num_tokens 16335 num_corr 0\n",
      "#: 241 ans: 116 time: 101.42366480827332 num_tokens 2604 num_corr 0\n",
      "#: 265 ans: 1\\frac{4}{5} time: 949.7690925598145 num_tokens 21082 num_corr 0\n",
      "#: 285 ans: 3 \\pm 2 \\sqrt{2} time: 106.52362036705017 num_tokens 2733 num_corr 0\n",
      "#: 287 ans: \\frac{17}{21} time: 920.9021956920624 num_tokens 20631 num_corr 0\n",
      "#: 299 ans: \\begin{pmatrix} -18 \\\\ -49 \\\\ 96 \\end{pmatrix} time: 102.70998549461365 num_tokens 2634 num_corr 0\n",
      "#: 302 ans: 5.4 \\text{ cents} time: 166.5295650959015 num_tokens 4182 num_corr 0\n",
      "#: 309 ans: 1+2\\sqrt{3} time: 1451.3686866760254 num_tokens 29891 num_corr 0\n",
      "#: 319 ans: \\frac{11+9a}{20} time: 74.73701333999634 num_tokens 1926 num_corr 0\n",
      "#: 325 ans: 120 time: 1013.4339122772217 num_tokens 22318 num_corr 0\n",
      "#: 341 ans: 55^\\circ time: 319.3107523918152 num_tokens 7966 num_corr 0\n",
      "#: 359 ans: \\text{ellipse} time: 72.83320617675781 num_tokens 1863 num_corr 0\n",
      "#: 384 ans: x \\in [-2,7] time: 52.96456694602966 num_tokens 1357 num_corr 0\n",
      "#: 393 ans: \\sqrt{66} time: 1600.6779291629791 num_tokens 32239 num_corr 0\n",
      "#: 401 ans: \\frac{14}{3} time: 222.41257214546204 num_tokens 5655 num_corr 0\n",
      "#: 420 ans: \\begin{pmatrix} -7 \\\\ 16 \\\\ 5 \\end{pmatrix} time: 104.69920897483826 num_tokens 2683 num_corr 0\n",
      "#: 423 ans: \\{1\\pm\\sqrt{5},-2\\} time: 1633.7348573207855 num_tokens 32768 num_corr 0\n",
      "#: 445 ans: \\frac{1}{2} time: 1636.473718881607 num_tokens 32768 num_corr 0\n",
      "#: 457 ans: -2,1 time: 112.13817858695984 num_tokens 2877 num_corr 0\n",
      "#: 461 ans: 75^\\circ time: 619.0159020423889 num_tokens 14607 num_corr 0\n",
      "#: 492 ans: 64 time: 58.858553886413574 num_tokens 1513 num_corr 0\n",
      "num: 500, acc: 0.924, all_time: 84090.07145571709, tokens/s: 24.23278949255143, token_num:4075.474, num_8k:18, num_corr:0.0\n",
      "wrong avg tokens 12897.368421052632 wrong_num 38 right avg tokens 3349.8636363636365 correct_num 462\n"
     ]
    }
   ],
   "source": [
    "check_results('/home/wxy320/ondemand/program/speculative_thinking/results/MATH500_deepseek-7b_None_0.01.json', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num: 500, acc: 0.848, all_time: 139588.90592622757, tokens/s: 18.158520429550457, token_num:5069.456, num_8k:49, num_corr:0.03205669405159055\n",
      "wrong avg tokens 12938.263157894737 wrong_num 76 right avg tokens 3659.009433962264 correct_num 424\n"
     ]
    }
   ],
   "source": [
    "check_results('/home/wxy320/ondemand/program/speculative_thinking/results/MATH500_deepseek-32b_deepseek-1.5b_3.json', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expression \n",
      "# :10, 14719, False, 19048, True, correct_num: 77\n",
      "The coordinates\n",
      "# :42, 14415, False, 12749, True, correct_num: 20\n",
      "For $0 \\le x \\l\n",
      "# :44, 19203, False, 6798, True, correct_num: 8\n",
      "Let\n",
      "\\[x^8 + 3x^\n",
      "# :64, 7904, False, 3738, True, correct_num: 1\n",
      "Point $A$ lies \n",
      "# :83, 9021, False, 11672, True, correct_num: 32\n",
      "The expression \n",
      "# :120, 8237, False, 8116, True, correct_num: 27\n",
      "Let $f(x)$ be a\n",
      "# :121, 32768, False, 11510, True, correct_num: 25\n",
      "Suzanne walks f\n",
      "# :129, 1611, False, 10403, True, correct_num: 58\n",
      "A Senate commit\n",
      "# :143, 9882, False, 7717, True, correct_num: 19\n",
      "For every posit\n",
      "# :151, 5367, False, 4019, True, correct_num: 6\n",
      "Two runners, $A\n",
      "# :157, 32768, False, 7593, True, correct_num: 7\n",
      "Find the larges\n",
      "# :189, 2336, False, 6171, True, correct_num: 13\n",
      "If 4 daps = 7 y\n",
      "# :201, 579, False, 10682, True, correct_num: 31\n",
      "Twelve 1 by 1 s\n",
      "# :211, 5938, False, 3095, True, correct_num: 2\n",
      "The area of $\\t\n",
      "# :223, 7974, False, 10511, True, correct_num: 37\n",
      "Compute\n",
      "\\[\\frac\n",
      "# :237, 32768, False, 9239, True, correct_num: 18\n",
      "The parallelogr\n",
      "# :240, 29385, False, 10182, True, correct_num: 19\n",
      "An investment o\n",
      "# :243, 32768, False, 7949, True, correct_num: 20\n",
      "Twelve friends \n",
      "# :247, 5023, False, 5265, True, correct_num: 12\n",
      "Given\n",
      "\\[\\mathbf\n",
      "# :249, 9545, False, 5597, True, correct_num: 4\n",
      "In circle $J$, \n",
      "# :268, 11823, False, 3469, True, correct_num: 8\n",
      "In the diagram,\n",
      "# :280, 4334, False, 5529, True, correct_num: 15\n",
      "Steve says to J\n",
      "# :286, 18955, False, 7174, True, correct_num: 15\n",
      "Consider the re\n",
      "# :292, 32768, False, 2931, True, correct_num: 7\n",
      "The solutions o\n",
      "# :296, 3673, False, 9109, True, correct_num: 12\n",
      "$\\textbf{Juan's\n",
      "# :302, 3929, False, 5405, True, correct_num: 16\n",
      "Let $P(x)$ be a\n",
      "# :328, 25705, False, 5010, True, correct_num: 6\n",
      "In how many way\n",
      "# :350, 8461, False, 12511, True, correct_num: 34\n",
      "Determine if th\n",
      "# :359, 9713, False, 1997, True, correct_num: 0\n",
      "The data in the\n",
      "# :380, 4161, False, 4084, True, correct_num: 4\n",
      "Navin from Maur\n",
      "# :399, 704, False, 899, True, correct_num: 0\n",
      "A $90^\\circ$ ro\n",
      "# :403, 32768, False, 3503, True, correct_num: 4\n",
      "Let $G$ and $H$\n",
      "# :426, 32768, False, 6902, True, correct_num: 19\n",
      "Let\n",
      "\\[f(x) = (\\\n",
      "# :432, 3896, False, 4428, True, correct_num: 4\n",
      "Find the quotie\n",
      "# :440, 9527, False, 17346, True, correct_num: 25\n",
      "In the diagram \n",
      "# :461, 5563, False, 20471, True, correct_num: 73\n",
      "If $re^{i \\thet\n",
      "# :467, 7795, False, 7486, True, correct_num: 20\n",
      "Below is a magi\n",
      "# :474, 19401, False, 3947, True, correct_num: 6\n",
      "A steel sphere \n",
      "# :487, 2711, False, 9286, True, correct_num: 41\n",
      "Let $S$ be the \n",
      "# :495, 8453, False, 8191, True, correct_num: 16\n"
     ]
    }
   ],
   "source": [
    "compare_token_num('/home/wxy320/ondemand/program/speculative_thinking/results/MATH500_deepseek-1.5b_None_0.01.json', '/home/wxy320/ondemand/program/speculative_thinking/results/MATH500_deepseek-32b_deepseek-1.5b_2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The binary numb\n",
      "# :72, 6146, True, 11225, False, correct_num: 44\n",
      "In quadrilatera\n",
      "# :95, 14494, True, 10187, False, correct_num: 24\n",
      "Let $F(z)=\\frac\n",
      "# :127, 11931, True, 24993, False, correct_num: 59\n",
      "A square and a \n",
      "# :167, 16176, True, 15502, False, correct_num: 43\n",
      "Convert $\\frac{\n",
      "# :177, 9919, True, 15700, False, correct_num: 33\n",
      "In the diagram,\n",
      "# :178, 24017, True, 20493, False, correct_num: 68\n",
      "Compute\n",
      "\\[\\sum_\n",
      "# :214, 3344, True, 5668, False, correct_num: 8\n",
      "If $\\omega^{199\n",
      "# :233, 10868, True, 9451, False, correct_num: 14\n",
      "Simplify\n",
      "\\[\\cos\n",
      "# :352, 12449, True, 8274, False, correct_num: 6\n",
      "If $a$ and $b$ \n",
      "# :366, 5908, True, 17388, False, correct_num: 26\n",
      "Two sides of a \n",
      "# :369, 1353, True, 1153, False, correct_num: 1\n",
      "It's a well-kno\n",
      "# :394, 1170, True, 1608, False, correct_num: 2\n",
      "How many of the\n",
      "# :404, 6007, True, 6386, False, correct_num: 26\n",
      "If\n",
      "\\[f(n + 1) =\n",
      "# :433, 6614, True, 20484, False, correct_num: 43\n",
      "Karla drove her\n",
      "# :442, 2318, True, 482, False, correct_num: 0\n",
      "Fake gold brick\n",
      "# :460, 8545, True, 5747, False, correct_num: 16\n",
      "Let $a,$ $b,$ $\n",
      "# :471, 5627, True, 21985, False, correct_num: 70\n",
      "Find the sum of\n",
      "# :479, 7331, True, 6033, False, correct_num: 11\n",
      "What is the dom\n",
      "# :496, 2231, True, 5679, False, correct_num: 12\n"
     ]
    }
   ],
   "source": [
    "compare_token_num('/home/wxy320/ondemand/program/speculative_thinking/results/MATH500_deepseek-1.5b_None_0.01.json', '/home/wxy320/ondemand/program/speculative_thinking/results/MATH500_deepseek-32b_deepseek-1.5b_2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path1 正确但 Path2 错误的数量: 22\n",
      "Path1 错误但 Path2 正确的数量: 29\n",
      "Path1 在自己正确回答的题目上的平均 token 数: 3405.48\n",
      "Path2 在 Path1 正确回答的题目上的平均 token 数: 3761.62\n",
      "Path1 在自己错误回答的题目上的平均 token 数: 13936.08\n",
      "Path2 在 Path1 错误回答的题目上的平均 token 数: 11452.87\n"
     ]
    }
   ],
   "source": [
    "analysis_token_num('/home/wxy320/ondemand/program/speculative_thinking/results/MATH500_deepseek-1.5b_None_0.01.json', '/home/wxy320/ondemand/program/speculative_thinking/results/MATH500_deepseek-7b_deepseek-1.5b.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path1 正确但 Path2 错误的数量: 19\n",
      "Path1 错误但 Path2 正确的数量: 40\n",
      "Path1 在自己正确回答的题目上的平均 token 数: 3405.48\n",
      "Path2 在 Path1 正确回答的题目上的平均 token 数: 3753.10\n",
      "Path1 在自己错误回答的题目上的平均 token 数: 13936.08\n",
      "Path2 在 Path1 错误回答的题目上的平均 token 数: 10203.31\n"
     ]
    }
   ],
   "source": [
    "analysis_token_num('/home/wxy320/ondemand/program/speculative_thinking/results/MATH500_deepseek-1.5b_None_0.01.json', '/home/wxy320/ondemand/program/speculative_thinking/results/MATH500_deepseek-32b_deepseek-1.5b_2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, so I need to convert the rectangular coordinates (0, 3) to polar coordinates. Hmm, I remember that polar coordinates are represented as (r, θ), where r is the distance from the origin to the point, and θ is the angle made with the positive x-axis. \n",
      "\n",
      "Alright, let me think about how to do this step by step. I think the formulas involve converting x and y coordinates into r and θ. Maybe I can use the Pythagorean theorem? Because in a right triangle, the hypotenuse squared is equal to the sum of the squares of the other two sides. So, maybe r is the hypotenuse, and x and y are the legs.\n",
      "\n",
      "So, if I apply Pythagoras' theorem here, r squared should equal x squared plus y squared. Let me write that down:\n",
      "\n",
      "r² = x² + y²\n",
      "\n",
      "Given that x is 0 and y is 3, let me plug in those values:\n",
      "\n",
      "r² = 0² + 3²\n",
      "r² = 0 + 9\n",
      "r² = 9\n",
      "\n",
      "Okay, so if I take the square root of both sides to solve for r, I get:\n",
      "\n",
      "r = √9\n",
      "r = 3\n",
      "\n",
      "Alright, so r is 3. That seems straightforward. Now, I need to find θ. I think θ is the angle whose tangent is equal to y over x, right? Because in a right triangle, the tangent of an angle is the opposite side over the adjacent side.\n",
      "\n",
      "So, θ = arctan(y/x)\n",
      "\n",
      "But wait, if x is 0, then we have y over 0, which is undefined. Hmm, arctan of infinity? That makes sense because when x is 0 and y is positive, the point is on the positive y-axis, right?\n",
      "\n",
      "So, arctan(y/x) would be arctan(3/0), which is like arctan(infinity). Hmm, I remember that as θ approaches 90 degrees from the left (since it's in the second quadrant), the angle approaches π/2. Since our point is (0, 3), it's on the positive y-axis, which is exactly 90 degrees or π/2 radians.\n",
      "\n",
      "Therefore, θ is π/2. Let me just write that down:\n",
      "\n",
      "θ = π/2\n",
      "\n",
      "Wait a second, just to make sure I'm not messing anything up. Sometimes, especially with negative numbers or quadrants, I tend to get confused. Let's think: the point (0, 3) is in the first quadrant, but on the y-axis, which is actually considered the boundary between quadrants I and II. I know that when x is positive and y is positive, we are in quadrant I, but when x is 0 and y is positive, it's on the positive y-axis, which is a bit of an edge case.\n",
      "\n",
      "But wait, arctan(y/x) is undefined because x is 0, so we have to consider the signs. Since both x and y are positive, but wait, in our case, x is 0 and y is positive, so the point is in the positive y-axis. Since it's exactly on the axis, is θ = π/2 correct? Or should it be 0?\n",
      "\n",
      "Hmm, no, wait. In polar coordinates, angles are measured from the positive x-axis, going counterclockwise. So, if the point is directly up along the y-axis, then that should correspond to θ = π/2, which is 90 degrees. So yes, that seems correct. Because if we think of a point (0, 3), it's at the top of the circle, which is π/2.\n",
      "\n",
      "But just to double-check: if the angle is 0, we're on the positive x-axis. If it's π/2, that's the top of the unit circle. So that makes sense. \n",
      "\n",
      "Wait, but actually, I remember sometimes it's confusing whether θ can be between π and 3π/2, but in this case, since x is 0 and y is positive, θ is just π/2. So I think that's correct.\n",
      "\n",
      "So, just summarizing: r is 3 because (0,3) is 3 units away from the origin, and θ is π/2 because the point is along the positive y-axis. Therefore, the polar coordinates should be (3, π/2).\n",
      "\n",
      "Let me just make sure if that seems right. If I convert back from polar to rectangular, (3, π/2), x should be 3cos(π/2) and y should be 3sin(π/2). Cos(π/2) is 0, and sin(π/2) is 1, so x is 0 and y is 3, which matches our original point. Therefore, that's correct.\n",
      "\n",
      "Wait, but maybe another way to see θ is by considering the signs of x and y. In this case, x is 0 and y is positive, so we can recall that in polar coordinates, when x is 0 and y is positive, θ is π/2. If x is 0 and y is negative, θ would be 3π/2, right? So yes, for our case, it's π/2.\n",
      "\n",
      "Alright, so just to recap the steps: for rectangular coordinates (x, y) = (0, 3), we calculate r as √(0² + 3²) = 3, and θ as arctan(3/0) which is π/2.\n",
      "\n",
      "Hence, the polar coordinates are (3, π/2). \n",
      "\n",
      "I think that should be the correct answer. I can't think of any mistakes here; I've double-checked each step. So I feel confident about this.\n",
      "\n",
      "**Final Answer**\n",
      "The polar coordinates of the point $(0,3)$ are $\\boxed{(3, \\frac{\\pi}{2})}$.\n",
      "</think>\n",
      "\n",
      "To convert the rectangular coordinates $(0, 3)$ to polar coordinates, we start by using the formulas for converting from rectangular to polar coordinates. \n",
      "\n",
      "The radius $r$ is calculated using the Pythagorean theorem:\n",
      "\\[\n",
      "r = \\sqrt{x^2 + y^2}\n",
      "\\]\n",
      "Substituting $x = 0$ and $y = 3$:\n",
      "\\[\n",
      "r = \\sqrt{0^2 + 3^2} = \\sqrt{9} = 3\n",
      "\\]\n",
      "\n",
      "Next, we find the angle $\\theta$ using the tangent function:\n",
      "\\[\n",
      "\\theta = \\arctan\\left(\\frac{y}{x}\\right)\n",
      "\\]\n",
      "However, since $x = 0$, the expression $\\frac{y}{x}$ is undefined. In such cases, we consider the position of the point. The point $(0, 3)$ is on the positive y-axis, which corresponds to an angle of $\\frac{\\pi}{2}$ radians.\n",
      "\n",
      "Thus, the polar coordinates are:\n",
      "\\[\n",
      "(r, \\theta) = \\left(3, \\frac{\\pi}{2}\\right)\n",
      "\\]\n",
      "\n",
      "We can verify this by converting back to rectangular coordinates using:\n",
      "\\[\n",
      "x = r \\cos \\theta = 3 \\cos \\left(\\frac{\\pi}{2}\\right) = 0\n",
      "\\]\n",
      "\\[\n",
      "y = r \\sin \\theta = 3 \\sin \\left(\\frac{\\pi}{2}\\right) = 3\n",
      "\\]\n",
      "\n",
      "Both $x$ and $y$ match the original rectangular coordinates, confirming the correctness of the polar coordinates.\n",
      "\n",
      "### Final Answer\n",
      "The polar coordinates of the point $(0, 3)$ are $\\boxed{(3, \\frac{\\pi}{2})}$.\n"
     ]
    }
   ],
   "source": [
    "result1 = read_saved_results('/home/wxy320/ondemand/program/speculative_thinking/results/MATH500_deepseek-1.5b_None_0.01.json')\n",
    "result2 = read_saved_results('/home/wxy320/ondemand/program/speculative_thinking/results/MATH500_deepseek-32b_deepseek-1.5b_2.json')\n",
    "\n",
    "print(result2[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def load_kpl_files_from_folder(folder_path):\n",
    "    \"\"\"读取指定文件夹下所有.kpl文件的数据\"\"\"\n",
    "    data_dict = []  # 用字典存储 {文件名: 数据}\n",
    "    \n",
    "    # 遍历文件夹下所有文件\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.pkl'):  # 只处理 .kpl 文件\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            try:\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    data = pickle.load(file)  # 反序列化\n",
    "                    data_dict.append(data)  # 存入字典\n",
    "            except Exception as e:\n",
    "                print(f\"读取文件 {file_name} 时出错: {e}\")\n",
    "    \n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = load_kpl_files_from_folder('/scratch/pbsjobs/wxy320/speculative/deepseek-32b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_probabilities, num_p = None, []\n",
    "for data in data_dict:\n",
    "    if avg_probabilities is None:\n",
    "        avg_probabilities = data['avg_probabilities']\n",
    "    else: \n",
    "        avg_probabilities = avg_probabilities + data['avg_probabilities']\n",
    "    for d in data['record_tokens']:\n",
    "        num_p.append(len(d['token']))\n",
    "# avg_probabilities = avg_probabilities/len(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1084\n",
      "begin ['okay', 'alright']\n",
      "[382] ['.\\n\\n'] ['4', 'now', 'hmm']\n",
      "[2055, 2160, 4710, 6771] [' So', ' Is', ' \\n\\n', ' Let'] ['app', 'so', 'let']\n",
      "[1939] ['?\\n\\n'] ['wait']\n",
      "[382] ['.\\n\\n'] ['i', 'but', 'just', 'alternatively']\n",
      "[382] ['.\\n\\n'] ['i', 'so']\n",
      "[2533] [']\\n\\n'] ['to', 'we', 'listing', 'therefore']\n"
     ]
    }
   ],
   "source": [
    "reasoning_words = ['H', 'Okay', 'Alright', ' Hmm', ' Let', ' Right', ' Alright', 'Let', ' let', ' Therefore', \n",
    "                   'Right', 'Let', 'Wait', 'Yeah', 'Alright', 'So',' Let', 'But',' But', ' So', ' Let', \n",
    "                   'Alternatively', ' However', ' Since',' now',]\n",
    "reasoning_words = ['h', 'okay', 'alright', 'hmm', 'let', 'right', 'therefore', 'wait', 'yeah', 'but', 'so', 'alternatively']\n",
    "data = data_dict[3]\n",
    "print(len(data['record_tokens']))\n",
    "for i in range(len(data['record_tokens'])):\n",
    "    d = data['record_tokens'][i]\n",
    "    # print(d['token'])\n",
    "    ts = [ t.lower() for t in d['token'] ]\n",
    "    # print(ts)\n",
    "    for t in reasoning_words:\n",
    "        if t in ts:\n",
    "            if i ==0: print('begin', ts)\n",
    "            else:\n",
    "                print(data['record_tokens'][i-1]['token_id'], data['record_tokens'][i-1]['token'], ts)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token:   ,Index: 220, Probability: 1.60803, Rank: 1\n",
      "token: , ,Index: 11, Probability: 1.34385, Rank: 2\n",
      "token: 2 ,Index: 17, Probability: 1.04405, Rank: 3\n",
      "token: 1 ,Index: 16, Probability: 0.87791, Rank: 4\n",
      "token:  the ,Index: 279, Probability: 0.74955, Rank: 5\n",
      "token:  = ,Index: 284, Probability: 0.55900, Rank: 6\n",
      "token: . ,Index: 13, Probability: 0.55187, Rank: 7\n",
      "token:  is ,Index: 374, Probability: 0.54100, Rank: 8\n",
      "token: 3 ,Index: 18, Probability: 0.48716, Rank: 9\n",
      "token: 0 ,Index: 15, Probability: 0.48245, Rank: 10\n",
      "token:  \\ ,Index: 1124, Probability: 0.45517, Rank: 11\n",
      "token:  - ,Index: 481, Probability: 0.44690, Rank: 12\n",
      "token:  + ,Index: 488, Probability: 0.41471, Rank: 13\n",
      "token: .\n",
      "\n",
      " ,Index: 382, Probability: 0.37956, Rank: 14\n",
      "token: ) ,Index: 8, Probability: 0.36792, Rank: 15\n",
      "token: 4 ,Index: 19, Probability: 0.36083, Rank: 16\n",
      "token: 5 ,Index: 20, Probability: 0.32443, Rank: 17\n",
      "token:  \\( ,Index: 17767, Probability: 0.25291, Rank: 18\n",
      "token: 6 ,Index: 21, Probability: 0.25053, Rank: 19\n",
      "token:  of ,Index: 315, Probability: 0.23038, Rank: 20\n"
     ]
    }
   ],
   "source": [
    "import __init__\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B')\n",
    "# 1. 将数组展平为 (152064,)\n",
    "flat_probs = avg_probabilities.flatten()\n",
    "\n",
    "# 2. 获取从小到大的排序索引\n",
    "sorted_indices = np.argsort(flat_probs)\n",
    "\n",
    "# 3. 计算排名（从大到小，最大值排名 1）\n",
    "ranks = np.empty_like(sorted_indices)\n",
    "ranks[sorted_indices] = np.arange(1, len(flat_probs) + 1)[::-1]\n",
    "\n",
    "# 输出前 10 个数据及其排名\n",
    "for i in range(20):\n",
    "    print(f\"token: {tokenizer.decode(sorted_indices[-(i+1)])} ,Index: {sorted_indices[-(i+1)]}, Probability: {flat_probs[sorted_indices[-(i+1)]]:.5f}, Rank: {ranks[sorted_indices[-(i+1)]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 你的数值列表\n",
    "data = num_p\n",
    "\n",
    "# 创建直方图\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.histplot(num_p, bins=20, color='blue', edgecolor='black')\n",
    "\n",
    "# 获取 bin 的边界\n",
    "bins = np.histogram_bin_edges(num_p, bins=10)\n",
    "\n",
    "# 在每个 bin 顶部添加计数值，并在 x 轴标注 bin 范围\n",
    "for patch, left_edge, right_edge in zip(ax.patches, bins[:-1], bins[1:]):\n",
    "    height = patch.get_height()  # 获取柱子的高度（频数）\n",
    "    if height > 0:  # 只标注非零的 bin\n",
    "        ax.text(patch.get_x() + patch.get_width() / 2, height, f'{int(height)}', \n",
    "                ha='center', va='bottom', fontsize=10, color='black')\n",
    "\n",
    "# 添加标题和标签\n",
    "plt.xlabel(\"num\", fontsize=12)\n",
    "plt.ylabel(\"count\", fontsize=12)\n",
    "\n",
    "# 显示图表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_probabilities_numpy = loaded_data[\"avg_probabilities\"]\n",
    "filtered_tokens_list = loaded_data[\"filtered_tokens\"]\n",
    "\n",
    "print(avg_probabilities_numpy)\n",
    "print(filtered_tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [09:14<00:00, 69.37s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import __init__\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Qwen2ForCausalLM\n",
    "target_model_name = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-32B'\n",
    "target_model = AutoModelForCausalLM.from_pretrained(\n",
    "    target_model_name, torch_dtype=torch.float16,device_map=\"auto\", low_cpu_mem_usage=True, attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(target_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, so I have this physics problem here. Let me read it again.\\n\\nJen wants to throw a softball with the same force as Jack throws a baseball. The softball has a mass of 200 grams, and the baseball has a mass of 150 grams. I need to find the ratio of acceleration of Jen's ball to Jack's. And they want the answer as a fraction in its lowest terms. Hmm, okay.\\n\\nAlright, so first, I should recall the formula that relates force, mass, and acceleration. From Newton's second law, I remember that F equals mass times acceleration, right? So F = m * a. Yeah, that's F equals ma.\\n\\nSo, if I rearrange that formula to solve for acceleration, a, that would be a = F / m. That makes sense because force is equal to mass times acceleration, so acceleration is force divided by mass.\\n\\nSo, in this problem, both Jen and Jack are exerting the same force on the ball, right? Because it says Jen wants to throw a softball with the same force as Jack throws a baseball. So, F_jen = F_jack.\\n\\nWait, hold on. The problem doesn't specify whether the force is the same magnitude or perhaps the same power or something else. But given it just says the same force, I think it's safe to assume that both forces are equal. So, yeah, F_jen = F_jack.\\n\\nSo, since the force is the same, let's denote it as F for both. Then, the acceleration of each object would be inversely proportional to their respective masses. Because if mass increases, acceleration decreases if force stays the same, by Newton's second law.\\n\\nOkay, so let me denote acceleration of Jen as a_jen and acceleration of Jack as a_jack. So,\\n\\na_jen = F / m_jen,\\n\\na_jack = F / m_jack.\\n\\nGiven that m_jen is 200 grams and m_jack is 150 grams.\\n\\nWait, I need the ratio of acceleration of Jen's ball to Jack's. That would be (a_jen) / (a_jack). So, let me write that as:\\n\\nRatio = (F / m_jen) / (F / m_jack).\\n\\nSimplify that. Let's see, F cancels out. So that becomes (1 / m_jen) / (1 / m_jack) which is (1 / m_jen) * (m_jack / 1) which simplifies to m_jack / m_jen.\\n\\nSo the ratio of acceleration is just the mass of Jen's ball divided by the mass of Jack's ball.\\n\\nSo, plugging in the numbers: m_jen is 200 grams, m_jack is 150 grams.\\n\\nSo, Ratio = 200 / 150.\\n\\nNow, let me simplify that fraction. 200 divided by 150.\\n\\nDivide numerator and denominator by 50: 200 ÷ 50 = 4, 150 ÷ 50 = 3.\\n\\nSo, it's 4/3.\\n\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepseek_1b_path = '/home/wxy320/ondemand/program/speculative_thinking/results/MATH500_deepseek-32b_deepseek-1.5b_2.json'\n",
    "results_1b = read_saved_results(deepseek_1b_path)\n",
    "question = results_1b[393]['generated_text'].split('Hmm, is that right? Let me double-check.')[0]\n",
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text:\n",
      " Wait, hold on. So, the ratio is 4/3? That would mean that Jen's acceleration is 4/3 times Jack's. But since Jen's ball is heavier, wouldn't it have a lower acceleration? So, wait\n",
      "__________________\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(question, return_tensors='pt').to(target_model.device)\n",
    "output_ids = target_model.generate(\n",
    "    input_ids,\n",
    "    temperature=0.6,\n",
    "    top_k=0,\n",
    "    top_p=0.95,\n",
    "    max_length=input_ids.shape[1] + 50,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "# Decode response\n",
    "response_text = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "print('response_text:\\n', response_text)\n",
    "print('__________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek_32b_path = '/home/wxy320/ondemand/program/speculative_thinking/results/MATH500_deepseek-32b_None_0.01.json'\n",
    "deepseek_1b_path = '/home/wxy320/ondemand/program/speculative_thinking/results/MATH500_deepseek-1.5b_None_0.01.json'\n",
    "\n",
    "results_32b = read_saved_results(deepseek_32b_path)\n",
    "results_1b = read_saved_results(deepseek_1b_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59, 59)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = min(len(results_32b), len(results_1b))\n",
    "results_1b_2, results_32b_2 = [],[]\n",
    "for i in range(l):\n",
    "    r_32, r_1 = results_32b[i], results_1b[i]\n",
    "    r_32_right_flag = check_math_correctness(r_32['answer'], r_32['generated_answer'])\n",
    "    r_1_right_flag = check_math_correctness(r_1['answer'], r_1['generated_answer'])\n",
    "    if r_32['question']==r_1['question'] and (r_32_right_flag and not r_1_right_flag):\n",
    "        results_1b_2.append(r_1)\n",
    "        results_32b_2.append(r_32)\n",
    "len(results_1b_2), len(results_32b_2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Let $p(x)$ be a polynomial of degree 5 such that\\n\\\\[p(n) = \\\\frac{n}{n^2 - 1}\\\\]for $n = 2,$ 3, 4, $\\\\dots,$ 7.  Find $p(8).$',\n",
       " 'generated_answer': \"Okay, so I have this problem where I need to find p(8), where p(x) is a polynomial of degree 5. The polynomial satisfies the equation p(n) = n / (n² - 1) for n = 2, 3, 4, 5, 6, 7. Hmm, that's interesting. Let me think about how to approach this.\\n\\nFirst, I know that if a polynomial p(x) of degree 5 is given to satisfy certain conditions, like here, it passes through specific points. These points are given for n from 2 to 7. So, I have six points here: (2, 2/(4-1)), (3, 3/(9-1)), ..., (7, 7/(49-1)). That's 6 points in total. Since p(x) is a degree 5 polynomial, it's uniquely determined by these six points, right? So, maybe I can construct the polynomial using these points and then evaluate it at x=8.\\n\\nBut constructing a degree 5 polynomial with six points might be tedious. I wonder if there's a smarter way to do this. Maybe using the concept of Lagrange interpolation or something similar. Or perhaps, I can consider the difference between p(x) and the given function f(n) = n/(n² -1). Hmm.\\n\\nWait, let me think about another approach. I remember that for polynomials that agree with a rational function at several points, sometimes we can use the concept of constructing a polynomial that has roots at those points or something like that. Maybe I can define another polynomial that relates to p(x) and f(x) = n/(n² -1).\\n\\nLet me define a new function q(x) = (x² - 1)p(x) - x. Then, for each n from 2 to 7, q(n) = (n² - 1)p(n) - n = (n² -1)(n/(n² -1)) - n = n - n = 0. So, q(n) = 0 for n = 2, 3, 4, 5, 6, 7. That means that q(x) has roots at x = 2, 3, 4, 5, 6, 7. So, q(x) is a polynomial that has these six roots, meaning it can be written as q(x) = k(x - 2)(x - 3)(x - 4)(x - 5)(x - 6)(x - 7), where k is some constant.\\n\\nSo, q(x) = k(x - 2)(x - 3)(x - 4)(x - 5)(x - 6)(x - 7). But what is q(x) in terms of x? Remember, q(x) = (x² - 1)p(x) - x. So, I can write:\\n\\n(x² - 1)p(x) - x = k(x - 2)(x - 3)(x - 4)(x - 5)(x - 6)(x - 7).\\n\\nNow, I need to find k. To find k, I can plug in a value for x that isn't one of 2, 3, 4, 5, 6, 7. Since p(x) is a degree 5 polynomial, (x² -1)p(x) is a degree 7 polynomial. The right-hand side is also a degree 7 polynomial, so their difference is a degree 7 polynomial.\\n\\nBut if I can find another point where I can compute both sides, I can solve for k. Let's pick x=0, because it's not one of the roots and might simplify things.\\n\\nSo, let's plug in x=0 into the equation:\\n\\n(0² - 1)p(0) - 0 = k(0 - 2)(0 - 3)(0 - 4)(0 - 5)(0 - 6)(0 - 7).\\n\\nSimplify the left side: (-1)p(0).\\n\\nRight side: k*(-2)(-3)(-4)(-5)(-6)(-7). Let's compute that.\\n\\nFirst, multiply the terms:\\n\\n(-2)(-3) = 6,\\n\\n6*(-4) = -24,\\n\\n-24*(-5) = 120,\\n\\n120*(-6) = -720,\\n\\n-720*(-7) = 5040.\\n\\nSo, the right side is k*5040.\\n\\nLeft side is (-1)p(0). So, we have:\\n\\n- p(0) = k*5040.\\n\\nTherefore, k = - p(0)/5040.\\n\\nBut I don't know p(0) yet. Hmm, maybe I can find p(0) in another way. Let me see.\\n\\nAlternatively, perhaps I can use another value for x. Maybe x=1? Let me try x=1.\\n\\nPlug in x=1:\\n\\n(1² - 1)p(1) - 1 = k(1 - 2)(1 - 3)(1 - 4)(1 - 5)(1 - 6)(1 - 7).\\n\\nLeft side: (0)p(1) -1 = -1.\\n\\nRight side: k*(-1)(-2)(-3)(-4)(-5)(-6).\\n\\nCompute the product:\\n\\n(-1)(-2) = 2,\\n\\n2*(-3) = -6,\\n\\n-6*(-4) = 24,\\n\\n24*(-5) = -120,\\n\\n-120*(-6) = 720.\\n\\nSo, right side is k*720.\\n\\nTherefore, equation is -1 = 720k => k = -1/720.\\n\\nAh, that's helpful. So, k is -1/720. Therefore, now we can write:\\n\\n(x² - 1)p(x) - x = (-1/720)(x - 2)(x - 3)(x - 4)(x - 5)(x - 6)(x - 7).\\n\\nSo, now, if I want to find p(8), let's plug x=8 into this equation.\\n\\nCompute left side: (8² - 1)p(8) - 8.\\n\\nRight side: (-1/720)(8 - 2)(8 - 3)(8 - 4)(8 - 5)(8 - 6)(8 - 7).\\n\\nCompute both sides.\\n\\nFirst, left side:\\n\\n(64 - 1)p(8) - 8 = 63p(8) - 8.\\n\\nRight side:\\n\\n(-1/720)(6)(5)(4)(3)(2)(1).\\n\\nCompute the product:\\n\\n6*5=30,\\n\\n30*4=120,\\n\\n120*3=360,\\n\\n360*2=720,\\n\\n720*1=720.\\n\\nSo, right side is (-1/720)*720 = -1.\\n\\nSo, equation is 63p(8) - 8 = -1.\\n\\nSolve for p(8):\\n\\n63p(8) = -1 + 8 = 7.\\n\\nSo, p(8) = 7/63 = 1/9.\\n\\nTherefore, the answer is 1/9.\\n\\nWait, let me double-check my steps to make sure I didn't make a mistake.\\n\\nFirst, defined q(x) = (x² -1)p(x) - x. It has roots at x=2,3,4,5,6,7, so q(x) = k(x-2)...(x-7). Then, found k by plugging in x=1, which gave k = -1/720.\\n\\nThen, plugging in x=8, we have 63p(8) -8 = -1, so 63p(8)=7, so p(8)=1/9. Seems correct.\\n\\nAlternatively, maybe I can think of this problem in terms of polynomial interpolation. Since p(x) agrees with f(n)=n/(n² -1) at n=2,...,7, and it's a degree 5 polynomial, perhaps I can construct it as a sum of basis polynomials.\\n\\nBut that might take longer. The method I used seems solid, so probably the answer is 1/9.\\n\\n**Final Answer**\\n\\\\boxed{\\\\dfrac{1}{9}}\\n</think>\\n\\nGiven a polynomial \\\\( p(x) \\\\) of degree 5 such that \\\\( p(n) = \\\\frac{n}{n^2 - 1} \\\\) for \\\\( n = 2, 3, 4, \\\\ldots, 7 \\\\), we need to find \\\\( p(8) \\\\).\\n\\nFirst, we define a new function \\\\( q(x) = (x^2 - 1)p(x) - x \\\\). This function \\\\( q(x) \\\\) has roots at \\\\( x = 2, 3, 4, 5, 6, 7 \\\\), so it can be written as:\\n\\\\[ q(x) = k(x - 2)(x - 3)(x - 4)(x - 5)(x - 6)(x - 7) \\\\]\\nwhere \\\\( k \\\\) is a constant.\\n\\nTo find \\\\( k \\\\), we substitute \\\\( x = 1 \\\\) into the equation:\\n\\\\[ (1^2 - 1)p(1) - 1 = k(1 - 2)(1 - 3)(1 - 4)(1 - 5)(1 - 6)(1 - 7) \\\\]\\nSimplifying both sides, we get:\\n\\\\[ -1 = k(-1)(-2)(-3)(-4)(-5)(-6) \\\\]\\n\\\\[ -1 = k \\\\cdot 720 \\\\]\\nThus, \\\\( k = -\\\\frac{1}{720} \\\\).\\n\\nNext, we use \\\\( x = 8 \\\\) to find \\\\( p(8) \\\\):\\n\\\\[ (8^2 - 1)p(8) - 8 = -\\\\frac{1}{720}(8 - 2)(8 - 3)(8 - 4)(8 - 5)(8 - 6)(8 - 7) \\\\]\\nSimplifying both sides, we get:\\n\\\\[ 63p(8) - 8 = -\\\\frac{1}{720} \\\\cdot 6 \\\\cdot 5 \\\\cdot 4 \\\\cdot 3 \\\\cdot 2 \\\\cdot 1 \\\\]\\n\\\\[ 63p(8) - 8 = -1 \\\\]\\nSolving for \\\\( p(8) \\\\):\\n\\\\[ 63p(8) = 7 \\\\]\\n\\\\[ p(8) = \\\\frac{7}{63} = \\\\frac{1}{9} \\\\]\\n\\nThus, the answer is:\\n\\\\[\\n\\\\boxed{\\\\dfrac{1}{9}}\\n\\\\]\",\n",
       " 'answer': '\\\\frac{3}{56}',\n",
       " 'corrected_tokens': [],\n",
       " 'generation_time': 89.44219088554382,\n",
       " 'num_tokens': 2350}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_1b_2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = results_1b_2[1]['generated_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_p = s.split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, so I have this problem where I need to find p(8), where p(x) is a polynomial of degree 5. The polynomial satisfies the equation p(n) = n / (n² - 1) for n = 2, 3, 4, 5, 6, 7. Hmm, that's interesting. Let me think about how to approach this.\n",
      "First, I know that if a polynomial p(x) of degree 5 is given to satisfy certain conditions, like here, it passes through specific points. These points are given for n from 2 to 7. So, I have six points here: (2, 2/(4-1)), (3, 3/(9-1)), ..., (7, 7/(49-1)). That's 6 points in total. Since p(x) is a degree 5 polynomial, it's uniquely determined by these six points, right? So, maybe I can construct the polynomial using these points and then evaluate it at x=8.\n",
      "But constructing a degree 5 polynomial with six points might be tedious. I wonder if there's a smarter way to do this. Maybe using the concept of Lagrange interpolation or something similar. Or perhaps, I can consider the difference between p(x) and the given function f(n) = n/(n² -1). Hmm.\n",
      "Wait, let me think about another approach. I remember that for polynomials that agree with a rational function at several points, sometimes we can use the concept of constructing a polynomial that has roots at those points or something like that. Maybe I can define another polynomial that relates to p(x) and f(x) = n/(n² -1).\n",
      "Let me define a new function q(x) = (x² - 1)p(x) - x. Then, for each n from 2 to 7, q(n) = (n² - 1)p(n) - n = (n² -1)(n/(n² -1)) - n = n - n = 0. So, q(n) = 0 for n = 2, 3, 4, 5, 6, 7. That means that q(x) has roots at x = 2, 3, 4, 5, 6, 7. So, q(x) is a polynomial that has these six roots, meaning it can be written as q(x) = k(x - 2)(x - 3)(x - 4)(x - 5)(x - 6)(x - 7), where k is some constant.\n",
      "So, q(x) = k(x - 2)(x - 3)(x - 4)(x - 5)(x - 6)(x - 7). But what is q(x) in terms of x? Remember, q(x) = (x² - 1)p(x) - x. So, I can write:\n",
      "(x² - 1)p(x) - x = k(x - 2)(x - 3)(x - 4)(x - 5)(x - 6)(x - 7).\n",
      "Now, I need to find k. To find k, I can plug in a value for x that isn't one of 2, 3, 4, 5, 6, 7. Since p(x) is a degree 5 polynomial, (x² -1)p(x) is a degree 7 polynomial. The right-hand side is also a degree 7 polynomial, so their difference is a degree 7 polynomial.\n",
      "But if I can find another point where I can compute both sides, I can solve for k. Let's pick x=0, because it's not one of the roots and might simplify things.\n",
      "So, let's plug in x=0 into the equation:\n",
      "(0² - 1)p(0) - 0 = k(0 - 2)(0 - 3)(0 - 4)(0 - 5)(0 - 6)(0 - 7).\n",
      "Simplify the left side: (-1)p(0).\n",
      "Right side: k*(-2)(-3)(-4)(-5)(-6)(-7). Let's compute that.\n",
      "First, multiply the terms:\n",
      "(-2)(-3) = 6,\n",
      "6*(-4) = -24,\n",
      "-24*(-5) = 120,\n",
      "120*(-6) = -720,\n",
      "-720*(-7) = 5040.\n",
      "So, the right side is k*5040.\n",
      "Left side is (-1)p(0). So, we have:\n",
      "- p(0) = k*5040.\n",
      "Therefore, k = - p(0)/5040.\n",
      "But I don't know p(0) yet. Hmm, maybe I can find p(0) in another way. Let me see.\n",
      "Alternatively, perhaps I can use another value for x. Maybe x=1? Let me try x=1.\n",
      "Plug in x=1:\n",
      "(1² - 1)p(1) - 1 = k(1 - 2)(1 - 3)(1 - 4)(1 - 5)(1 - 6)(1 - 7).\n",
      "Left side: (0)p(1) -1 = -1.\n",
      "Right side: k*(-1)(-2)(-3)(-4)(-5)(-6).\n",
      "Compute the product:\n",
      "(-1)(-2) = 2,\n",
      "2*(-3) = -6,\n",
      "-6*(-4) = 24,\n",
      "24*(-5) = -120,\n",
      "-120*(-6) = 720.\n",
      "So, right side is k*720.\n",
      "Therefore, equation is -1 = 720k => k = -1/720.\n",
      "Ah, that's helpful. So, k is -1/720. Therefore, now we can write:\n",
      "(x² - 1)p(x) - x = (-1/720)(x - 2)(x - 3)(x - 4)(x - 5)(x - 6)(x - 7).\n",
      "So, now, if I want to find p(8), let's plug x=8 into this equation.\n",
      "Compute left side: (8² - 1)p(8) - 8.\n",
      "Right side: (-1/720)(8 - 2)(8 - 3)(8 - 4)(8 - 5)(8 - 6)(8 - 7).\n",
      "Compute both sides.\n",
      "First, left side:\n",
      "(64 - 1)p(8) - 8 = 63p(8) - 8.\n",
      "Right side:\n",
      "(-1/720)(6)(5)(4)(3)(2)(1).\n",
      "Compute the product:\n",
      "6*5=30,\n",
      "30*4=120,\n",
      "120*3=360,\n",
      "360*2=720,\n",
      "720*1=720.\n",
      "So, right side is (-1/720)*720 = -1.\n",
      "So, equation is 63p(8) - 8 = -1.\n",
      "Solve for p(8):\n",
      "63p(8) = -1 + 8 = 7.\n",
      "So, p(8) = 7/63 = 1/9.\n",
      "Therefore, the answer is 1/9.\n",
      "Wait, let me double-check my steps to make sure I didn't make a mistake.\n",
      "First, defined q(x) = (x² -1)p(x) - x. It has roots at x=2,3,4,5,6,7, so q(x) = k(x-2)...(x-7). Then, found k by plugging in x=1, which gave k = -1/720.\n",
      "Then, plugging in x=8, we have 63p(8) -8 = -1, so 63p(8)=7, so p(8)=1/9. Seems correct.\n",
      "Alternatively, maybe I can think of this problem in terms of polynomial interpolation. Since p(x) agrees with f(n)=n/(n² -1) at n=2,...,7, and it's a degree 5 polynomial, perhaps I can construct it as a sum of basis polynomials.\n",
      "But that might take longer. The method I used seems solid, so probably the answer is 1/9.\n",
      "**Final Answer**\n",
      "\\boxed{\\dfrac{1}{9}}\n",
      "</think>\n",
      "Given a polynomial \\( p(x) \\) of degree 5 such that \\( p(n) = \\frac{n}{n^2 - 1} \\) for \\( n = 2, 3, 4, \\ldots, 7 \\), we need to find \\( p(8) \\).\n",
      "First, we define a new function \\( q(x) = (x^2 - 1)p(x) - x \\). This function \\( q(x) \\) has roots at \\( x = 2, 3, 4, 5, 6, 7 \\), so it can be written as:\n",
      "\\[ q(x) = k(x - 2)(x - 3)(x - 4)(x - 5)(x - 6)(x - 7) \\]\n",
      "where \\( k \\) is a constant.\n",
      "To find \\( k \\), we substitute \\( x = 1 \\) into the equation:\n",
      "\\[ (1^2 - 1)p(1) - 1 = k(1 - 2)(1 - 3)(1 - 4)(1 - 5)(1 - 6)(1 - 7) \\]\n",
      "Simplifying both sides, we get:\n",
      "\\[ -1 = k(-1)(-2)(-3)(-4)(-5)(-6) \\]\n",
      "\\[ -1 = k \\cdot 720 \\]\n",
      "Thus, \\( k = -\\frac{1}{720} \\).\n",
      "Next, we use \\( x = 8 \\) to find \\( p(8) \\):\n",
      "\\[ (8^2 - 1)p(8) - 8 = -\\frac{1}{720}(8 - 2)(8 - 3)(8 - 4)(8 - 5)(8 - 6)(8 - 7) \\]\n",
      "Simplifying both sides, we get:\n",
      "\\[ 63p(8) - 8 = -\\frac{1}{720} \\cdot 6 \\cdot 5 \\cdot 4 \\cdot 3 \\cdot 2 \\cdot 1 \\]\n",
      "\\[ 63p(8) - 8 = -1 \\]\n",
      "Solving for \\( p(8) \\):\n",
      "\\[ 63p(8) = 7 \\]\n",
      "\\[ p(8) = \\frac{7}{63} = \\frac{1}{9} \\]\n",
      "Thus, the answer is:\n",
      "\\[\n",
      "\\boxed{\\dfrac{1}{9}}\n",
      "\\]\n"
     ]
    }
   ],
   "source": [
    "for s_i in s_p:\n",
    "    print(s_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " First, I know that if a polynomial p(x) \n",
      "response_text:\n",
      " First, I know that p(x) is a degree 5 polynomial.\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " But constructing a degree 5 polynomial w\n",
      "response_text:\n",
      " But wait, constructing a 5th degree polynomial from scratch using six points\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " Wait, let me think about another approac\n",
      "response_text:\n",
      " Wait, let's think about f(n) = n/(n² -\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " Let me define a new function q(x) = (x² \n",
      "response_text:\n",
      " Let me write down f(n) as n/(n² -1)\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " So, q(x) = k(x - 2)(x - 3)(x - 4)(x - 5)\n",
      "response_text:\n",
      " Now, since p(x) is a degree 5 polynomial, let's\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " (x² - 1)p(x) - x = k(x - 2)(x - 3)(x - 4\n",
      "response_text:\n",
      " (x² - 1)p(x) - x = k(x - \n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " Now, I need to find k. To find k, I can \n",
      "response_text:\n",
      " Therefore, (x² - 1)p(x) = x + k\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " But if I can find another point where I \n",
      "response_text:\n",
      " Wait, but if I can find a specific x that simplifies the equation\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " So, let's plug in x=0 into the equation:\n",
      "response_text:\n",
      " So, when x=0:\n",
      "\n",
      "Left-hand side (LHS): (\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " (0² - 1)p(0) - 0 = k(0 - 2)(0 - 3)(0 - 4\n",
      "response_text:\n",
      " (0² - 1)p(0) - 0 = k\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " Simplify the left side: (-1)p(0).\n",
      "response_text:\n",
      " Simplify the left side: (-1)p(0) - 0\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " Right side: k*(-2)(-3)(-4)(-5)(-6)(-7). \n",
      "response_text:\n",
      " Right side: k(-2)(-3)(-4)(-5\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " First, multiply the terms:\n",
      "response_text:\n",
      " First, compute the product of the constants:\n",
      "\n",
      "(-2)*(-3)\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " (-2)(-3) = 6,\n",
      "response_text:\n",
      " (-2)(-3) = 6\n",
      "\n",
      "6*(-4) =\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " 6*(-4) = -24,\n",
      "response_text:\n",
      " 6*(-4) = -24,\n",
      "\n",
      "-24*(-5)\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " -24*(-5) = 120,\n",
      "response_text:\n",
      " -24*(-5) = 120,\n",
      "\n",
      "120\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " 120*(-6) = -720,\n",
      "response_text:\n",
      " 120*(-6) = -720,\n",
      "\n",
      "-72\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " -720*(-7) = 5040.\n",
      "response_text:\n",
      " -720*(-7) = 5040.\n",
      "\n",
      "So\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " So, the right side is k*5040.\n",
      "response_text:\n",
      " So, the right side is k*5040.\n",
      "\n",
      "Left side\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " Left side is (-1)p(0). So, we have:\n",
      "response_text:\n",
      " So, the equation becomes:\n",
      "\n",
      "(-1)p(0) = 5\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " - p(0) = k*5040.\n",
      "response_text:\n",
      " (-1)p(0) = 5040k.\n",
      "\n",
      "But\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " Therefore, k = - p(0)/5040.\n",
      "response_text:\n",
      " So, p(0) = -5040k.\n",
      "\n",
      "But\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " But I don't know p(0) yet. Hmm, maybe I \n",
      "response_text:\n",
      " Hmm, but I don't know p(0). Maybe I can find\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " Alternatively, perhaps I can use another\n",
      "response_text:\n",
      " Wait, another idea: since p(x) is a polynomial of degree \n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " Plug in x=1:\n",
      "response_text:\n",
      " Plug in x=1 into the equation:\n",
      "\n",
      "(1² -1)p\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " (1² - 1)p(1) - 1 = k(1 - 2)(1 - 3)(1 - 4\n",
      "response_text:\n",
      " (1² -1)p(1) - 1 = k(\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " Left side: (0)p(1) -1 = -1.\n",
      "response_text:\n",
      " Left side: (1 -1)p(1) -1 = \n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " Right side: k*(-1)(-2)(-3)(-4)(-5)(-6).\n",
      "response_text:\n",
      " Right side: k*(-1)(-2)(-3)(-4\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " Compute the product:\n",
      "response_text:\n",
      " Compute that:\n",
      "\n",
      "Multiply the terms:\n",
      "\n",
      "(-1)(-2)=2,\n",
      "\n",
      "\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " (-1)(-2) = 2,\n",
      "response_text:\n",
      " (-1)(-2)=2,\n",
      "\n",
      "2*(-3)=-6,\n",
      "\n",
      "\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " 2*(-3) = -6,\n",
      "response_text:\n",
      " 2*(-3) = -6,\n",
      "\n",
      "-6*(-4) = \n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " -6*(-4) = 24,\n",
      "response_text:\n",
      " -6*(-4) = 24,\n",
      "\n",
      "24*(-5)\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " 24*(-5) = -120,\n",
      "response_text:\n",
      " 24*(-5) = -120,\n",
      "\n",
      "-120\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " -120*(-6) = 720.\n",
      "response_text:\n",
      " -120*(-6) = 720.\n",
      "\n",
      "So,\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " So, right side is k*720.\n",
      "response_text:\n",
      " So, right side is k*720.\n",
      "\n",
      "Therefore, -1\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " Therefore, equation is -1 = 720k => k = \n",
      "response_text:\n",
      " Thus, -1 = k*720.\n",
      "\n",
      "So, k =\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " Ah, that's helpful. So, k is -1/720. The\n",
      "response_text:\n",
      " Wait, so k is -1/720. Let me check\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " (x² - 1)p(x) - x = (-1/720)(x - 2)(x - 3\n",
      "response_text:\n",
      " q(x) = (x² -1)p(x) - x =\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " So, now, if I want to find p(8), let's p\n",
      "response_text:\n",
      " So, to find p(x), we can rearrange:\n",
      "\n",
      "p(x)\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " Compute left side: (8² - 1)p(8) - 8.\n",
      "response_text:\n",
      " So, (8² -1)p(8) -8 = (-\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " Right side: (-1/720)(8 - 2)(8 - 3)(8 - 4\n",
      "response_text:\n",
      " Right side: (-1/720)(8 - 2)(\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " Compute both sides.\n",
      "response_text:\n",
      " Compute right side first:\n",
      "\n",
      "(8 - 2) = 6,\n",
      "\n",
      "\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " First, left side:\n",
      "response_text:\n",
      " Left side: (64 - 1)p(8) - \n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " (64 - 1)p(8) - 8 = 63p(8) - 8.\n",
      "response_text:\n",
      " (64 - 1)p(8) - 8 = \n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " Right side:\n",
      "response_text:\n",
      " Right side:\n",
      "\n",
      "(-1/720)(6)(5)(4\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " (-1/720)(6)(5)(4)(3)(2)(1).\n",
      "response_text:\n",
      " (-1/720)(6)(5)(4)(3)(\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " Compute the product:\n",
      "response_text:\n",
      " Compute the product in the numerator:\n",
      "\n",
      "6*5 = 30,\n",
      "\n",
      "\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " 6*5=30,\n",
      "response_text:\n",
      " 6*5=30,\n",
      "\n",
      "30*4=120\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " 30*4=120,\n",
      "response_text:\n",
      " 30*4=120,\n",
      "\n",
      "120*3=\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " 120*3=360,\n",
      "response_text:\n",
      " 120*3=360,\n",
      "\n",
      "360*2\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " 360*2=720,\n",
      "response_text:\n",
      " 360*2=720,\n",
      "\n",
      "720*1\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " 720*1=720.\n",
      "response_text:\n",
      " 720*1=720.\n",
      "\n",
      "So, right side is\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " So, right side is (-1/720)*720 = -1.\n",
      "response_text:\n",
      " So, right side is (-1/720)*720\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " So, equation is 63p(8) - 8 = -1.\n",
      "response_text:\n",
      " So, we have:\n",
      "\n",
      "63p(8) - 8 =\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " Solve for p(8):\n",
      "response_text:\n",
      " Solving for p(8):\n",
      "\n",
      "63p(8) = -\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " 63p(8) = -1 + 8 = 7.\n",
      "response_text:\n",
      " 63p(8) = -1 + 8 = 7\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " So, p(8) = 7/63 = 1/9.\n",
      "response_text:\n",
      " Thus, p(8) = 7 / 63 = \n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " Therefore, the answer is 1/9.\n",
      "response_text:\n",
      " So, the answer is 1/9.\n",
      "\n",
      "Wait, let me double\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " Wait, let me double-check my steps to ma\n",
      "response_text:\n",
      " Wait, that seems straightforward. Let me verify my steps.\n",
      "\n",
      "1. Defined\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " First, defined q(x) = (x² -1)p(x) - x. I\n",
      "response_text:\n",
      " 1. Defined q(x) = (x² -1)p(x)\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " Then, plugging in x=8, we have 63p(8) -8\n",
      "response_text:\n",
      " Then, to find p(8), plugged in x=8 into q\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " Alternatively, maybe I can think of this\n",
      "response_text:\n",
      " Alternatively, let me think if there's another way to compute k. For\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " But that might take longer. The method I\n",
      "response_text:\n",
      " But that might be more complicated. The approach I took earlier seems straightforward.\n",
      "\n",
      "\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " **Final Answer**\n",
      "\\boxed{\\dfrac{1}{9}}\n",
      "</\n",
      "response_text:\n",
      " **Final Answer**\n",
      "The value of \\( p(8) \\) is\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " Given a polynomial \\( p(x) \\) of degree \n",
      "response_text:\n",
      " To find \\( p(8) \\) where \\( p(x) \\\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " First, we define a new function \\( q(x) \n",
      "response_text:\n",
      " First, we define a new polynomial \\( q(x) = (x^\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " To find \\( k \\), we substitute \\( x = 1 \n",
      "response_text:\n",
      " To find \\( k \\), we evaluate \\( q(x) \\) at\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " Next, we use \\( x = 8 \\) to find \\( p(8)\n",
      "response_text:\n",
      " Now, substituting \\( x = 8 \\) into the equation for\n",
      "__________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      " Thus, the answer is:\n",
      "\\[\n",
      "\\boxed{\\dfrac{1}\n",
      "response_text:\n",
      " Thus, the final answer is:\n",
      "\\[\n",
      "\\boxed{\\dfrac{\n",
      "__________________\n",
      "question:\n",
      " \n",
      "response_text:\n",
      " Given a polynomial \\( p(x) \\) of degree 5 such that\n",
      "__________________\n"
     ]
    }
   ],
   "source": [
    "question = ''\n",
    "for i in range(len(s_p)):\n",
    "    s_i = s_p[i]\n",
    "    question = question + s_i+'\\n\\n'\n",
    "    input_ids = tokenizer.encode(question, return_tensors='pt').to(target_model.device)\n",
    "    output_ids = target_model.generate(\n",
    "        input_ids,\n",
    "        temperature=0.6,\n",
    "        top_k=0,\n",
    "        top_p=0.95,\n",
    "        max_length=input_ids.shape[1] + 15,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    # Decode response\n",
    "    response_text = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "    print('question:\\n', (s_p[i+1][:40] if i + 1 < len(s_p) else '')) # question[-40:] + \n",
    "    print('response_text:\\n', response_text)\n",
    "    print('__________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5468\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the distance, in units, between the points $(2, -6)$ and $(-4, 3)$? Express your answer in simplest radical form. Please reason step by step, and put your final answer within \\\\boxed{{}}. Alright, so I have this problem here where I need to find the distance between two points on a coordinate plane. The points are (2, -6) and (-4, 3). Hmm, okay, let me think about how to approach this. I remember something about the distance formula from my geometry class. I think it involves the coordinates of the two points. \\n\\nFirst, let me recall the distance formula. If I have two points, say (x1, y1) and (x2, y2), the distance between them is calculated using the square root of [(x2 - x1) squared plus (y2 - y1) squared]. So, it's like taking the differences in the x-coordinates and y-coordinates, squaring them, adding them together, and then taking the square root of that sum. That should give me the straight-line distance between the two points. \\n\\nAlright, so applying that to the points given here: (2, -6) and (-4, 3). Let me label the points to make it clearer. Let\\u2019s say (x1, y1) is (2, -6) and (x2, y2) is (-4, 3). \\n\\nNow, I need to find the differences in the x and y coordinates. So, starting with the x-coordinates: x2 - x1 is -4 - 2. Wait, is that right? Hold on, x2 is -4 and x1 is 2. So, subtracting x1 from x2 would be -4 - 2, which is -6. Hmm, but actually, in the distance formula, it doesn't matter if it's positive or negative because when we square it, the negative will become positive. So, maybe I should just compute the absolute difference to make it easier. \\n\\nBut wait, let me double-check. The formula is (x2 - x1)^2, so if x2 is less than x1, the result will be negative, but when squared, it becomes positive. So, whether I do -4 - 2 or 2 - (-4), the squared term will be the same. Let me compute both ways to be sure.\\n\\nFirst, x2 - x1: -4 - 2 = -6. Then, y2 - y1: 3 - (-6) = 3 + 6 = 9. So, squaring these, (-6)^2 is 36, and 9^2 is 81. Then, adding them together: 36 + 81 = 117. So, the distance is the square root of 117.\\n\\nBut wait, 117 doesn't look like a perfect square. Let me see if it can be simplified. Breaking down 117: 117 divided by 9 is 13, because 9 times 13 is 117. So, 117 is 9 times 13. Therefore, sqrt(117) can be written as sqrt(9*13), which is sqrt(9)*sqrt(13), and since sqrt(9) is 3, that becomes 3*sqrt(13). \\n\\nSo, putting it all together, the distance between the two points is 3 times the square root of 13 units. Let me just recap to make sure I didn't make any mistakes. I took the coordinates, subtracted them, squared each difference, added them, and then took the square root. That seems correct.\\n\\nJust to double-check, I can visualize the points on the coordinate plane. The first point is at (2, -6), which is in the fourth quadrant, and the second point is at (-4, 3), which is in the second quadrant. The distance between them should be a straight line across the plane, not along the x or y-axis. So, the calculation seems reasonable because it's not a horizontal or vertical line; it's diagonal, which would result in a distance that's a combination of horizontal and vertical distances.\\n\\nAlternatively, I can think of the horizontal distance between them as the difference in the x-coordinates, which is 2 - (-4) = 6 units. The vertical distance is the difference in the y-coordinates, which is 3 - (-6) = 9 units. So, the horizontal distance is 6, the vertical distance is 9. Then, the distance between the points would be the hypotenuse of a right-angled triangle with legs of 6 and 9. \\n\\nUsing the Pythagorean theorem, which states that in a right-angled triangle, the square of the hypotenuse is equal to the sum of the squares of the other two sides. So, hypotenuse squared is 6^2 + 9^2 = 36 + 81 = 117. Therefore, the hypotenuse is sqrt(117), which simplifies to 3*sqrt(13). \\n\\nOkay, so both methods give me the same result. That makes me feel confident that 3*sqrt(13) is the correct distance. Just to make sure, I can also compute sqrt(117) using a calculator. Let me do that. \\n\\nCalculating sqrt(117): 117 is between 100 and 121, so sqrt(117) should be between 10 and 11. Let me see: 10.8 squared is 116.64, which is just under 117. So, sqrt(117) is approximately 10.8167. Then, 3*sqrt(13) would be approximately 3*3.6055512755, which is approximately 10.8166538265. So, that matches up with the decimal approximation of sqrt(117). \\n\\nTherefore, both the exact value and the approximate decimal value check out. So, I think I can safely conclude that the distance between the points (2, -6) and (-4, 3) is 3*sqrt(13) units.\\n\\n**Final Answer**\\nThe distance between the points is \\\\boxed{3\\\\sqrt{13}} units.\\n</think>\\n\\nTo find the distance between the points \\\\((2, -6)\\\\) and \\\\((-4, 3)\\\\), we use the distance formula. The distance formula is given by:\\n\\n\\\\[\\n\\\\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\\n\\\\]\\n\\nLabeling the points as \\\\((x_1, y_1) = (2, -6)\\\\) and \\\\((x_2, y_2) = (-4, 3)\\\\), we calculate the differences in the coordinates:\\n\\n\\\\[\\nx_2 - x_1 = -4 - 2 = -6\\n\\\\]\\n\\\\[\\ny_2 - y_1 = 3 - (-6) = 3 + 6 = 9\\n\\\\]\\n\\nNext, we square these differences:\\n\\n\\\\[\\n(-6)^2 = 36\\n\\\\]\\n\\\\[\\n9^2 = 81\\n\\\\]\\n\\nAdding these squared differences:\\n\\n\\\\[\\n36 + 81 = 117\\n\\\\]\\n\\nTaking the square root of the sum:\\n\\n\\\\[\\n\\\\sqrt{117}\\n\\\\]\\n\\nWe simplify \\\\(\\\\sqrt{117}\\\\) by noting that 117 factors into \\\\(9 \\\\times 13\\\\):\\n\\n\\\\[\\n\\\\sqrt{117} = \\\\sqrt{9 \\\\times 13} = \\\\sqrt{9} \\\\times \\\\sqrt{13} = 3\\\\sqrt{13}\\n\\\\]\\n\\nThus, the distance between the points is \\\\(\\\\boxed{3\\\\sqrt{13}}\\\\) units.\"\n",
    "print(len(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2326\n"
     ]
    }
   ],
   "source": [
    "print(len(question+response_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import read_saved_results\n",
    "datas = read_saved_results('/home/wxy320/ondemand/program/speculative_thinking/results/MATH500_Qwen-math-1.5b_None_0.01.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Please reason step by step, and put your final answer within \\\\boxed{{}}',\n",
       " ' $\\\\overline{BC}$ is parallel to the segment through $A$, and $AB = BC$',\n",
       " ' What is the number of degrees represented by $x$?\\n\\n[asy]\\ndraw((0,0)--(10,0));\\ndraw((0,3)--(10,3));\\ndraw((2,3)--(8,0));\\ndraw((2,3)--(4,0));\\nlabel(\"$A$\",(2,3),N);\\nlabel(\"$B$\",(4,0),S);\\nlabel(\"$C$\",(8,0),S);\\nlabel(\"$124^{\\\\circ}$\",(2,3),SW);\\nlabel(\"$x^{\\\\circ}$\",(4',\n",
       " '5,3),S);\\n[/asy]<think>\\nPlease reason step by step, and put your final answer within \\\\boxed{{}}',\n",
       " ' $\\\\overline{BC}$ is parallel to the segment through $A$, and $AB = BC$',\n",
       " ' What is the number of degrees represented by $x$?\\n\\n[asy]\\ndraw((0,0)--(10,0));\\ndraw((0,3)--(10,3));\\ndraw((2,3)--(8,0));\\ndraw((2,3)--(4,0));\\nlabel(\"$A$\",(2,3),N);\\nlabel(\"$B$\",(4,0),S);\\nlabel(\"$C$\",(8,0),S);\\nlabel(\"$124^{\\\\circ}$\",(2,3),SW);\\nlabel(\"$x^{\\\\circ}$\",(4',\n",
       " \"5,3),S);\\n[/asy]\\nLet's solve the problem step-by-step\",\n",
       " '\\n\\n1',\n",
       " ' **Understand the Geometry:**\\n   - We have a triangle \\\\(ABC\\\\) with \\\\(AB = BC\\\\)',\n",
       " '\\n   - The angle at \\\\(A\\\\) is \\\\(124^\\\\circ\\\\)',\n",
       " '\\n   - Since \\\\(AB = BC\\\\), triangle \\\\(ABC\\\\) is isosceles with \\\\(\\\\angle BAC = \\\\angle BCA = x\\\\)',\n",
       " '\\n   - The sum of the angles in a triangle is \\\\(180^\\\\circ\\\\)',\n",
       " '\\n\\n2',\n",
       " ' **Calculate the Angles:**\\n   - The angle at \\\\(B\\\\) can be found using the fact that the sum of the angles in triangle \\\\(ABC\\\\) is \\\\(180^\\\\circ\\\\)',\n",
       " '\\n   - Since \\\\(\\\\angle BAC = x\\\\) and \\\\(\\\\angle BCA = x\\\\), we have:\\n     \\\\[\\n     x + x + 124^\\\\circ = 180^\\\\circ\\n     \\\\]\\n   - Simplifying, we get:\\n     \\\\[\\n     2x + 124^\\\\circ = 180^\\\\circ\\n     \\\\]\\n   - Solving for \\\\(x\\\\):\\n     \\\\[\\n     2x = 180^\\\\circ - 124^\\\\circ\\n     \\\\]\\n     \\\\[\\n     2x = 56^\\\\circ\\n     \\\\]\\n     \\\\[\\n     x = 28^\\\\circ\\n     \\\\]\\n\\nSo, the value of \\\\(x\\\\) is \\\\(28^\\\\circ\\\\)',\n",
       " \"\\n\\nLet's confirm this with Python and sympy:\\n\\n```python\\nimport sympy as sp\\n\\n# Define the variable\\nx = sp\",\n",
       " \"symbols('x')\\n\\n# Equation based on the sum of angles in the triangle\\nequation = sp\",\n",
       " 'Eq(2*x + 124, 180)\\n\\n# Solve the equation\\nsolution = sp',\n",
       " 'solve(equation, x)[0]\\n\\n# Print the solution\\nprint(solution)\\n```\\n```output\\n28\\n```\\nThe value of \\\\(x\\\\) is \\\\(\\\\boxed{28}\\\\)',\n",
       " '']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "i = 18\n",
    "question =\"Please reason step by step, and put your final answer within \\\\boxed{{}}. \" +datas[i]['question']+\"<think>\\n\"+datas[i]['generated_answer']\n",
    "s_p = question.split('.')\n",
    "s_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the smallest positive perfect cube that can be written as the sum of three consecutive integers? Please reason step by step, and put your final answer within \\boxed{{}}. Okay, so I need to find the smallest positive perfect cube that can be written as the sum of three consecutive integers. Hmm, let's break this down step by step. \n",
      "\n",
      "First, let's clarify what is meant by consecutive integers. Consecutive integers are numbers that follow each other in order without any gaps. So, if I denote the first integer as n, then the next two consecutive integers would be n+1 and n+2. Therefore, the sum of these three integers would be n + (n+1) + (n+2).\n",
      "\n",
      "Let me write that down:\n",
      "\n",
      "Sum = n + (n + 1) + (n + 2)\n",
      "\n",
      "Simplifying this expression:\n",
      "\n",
      "Sum = n + n + 1 + n + 2\n",
      "Sum = 3n + 3\n",
      "\n",
      "So, the sum simplifies to 3n + 3. Hmm, that's 3(n + 1). Interesting. So, the sum is three times (n + 1). \n",
      "\n",
      "Now, the problem states that this sum must be a perfect cube. A perfect cube is a number that can be expressed as some integer raised to the power of three. So, we're looking for a perfect cube, let's call it C, such that C = 3(n + 1). \n",
      "\n",
      "Since C is a perfect cube, it can be written as k\\u00b3 where k is a positive integer. So, we have:\n",
      "\n",
      "k\\u00b3 = 3(n + 1)\n",
      "\n",
      "Which means that 3 divides k\\u00b3. Since 3 is a prime number, if 3 divides k\\u00b3, then 3 must divide k. So, let me write that down: k = 3m where m is an integer. \n",
      "\n",
      "Substituting back into the equation:\n",
      "\n",
      "(3m)\\u00b3 = 3(n + 1)\n",
      "27m\\u00b3 = 3(n + 1)\n",
      "\n",
      "Divide both sides by 3:\n",
      "\n",
      "9m\\u00b3 = n + 1\n",
      "\n",
      "So, n + 1 = 9m\\u00b3, which implies that n = 9m\\u00b3 - 1.\n",
      "\n",
      "Therefore, the three consecutive integers are:\n",
      "\n",
      "First integer: n = 9m\\u00b3 - 1\n",
      "Second integer: n + 1 = 9m\\u00b3\n",
      "Third integer: n + 2 = 9m\\u00b3 + 1\n",
      "\n",
      "Let me check with m = 1:\n",
      "\n",
      "n = 9(1)\\u00b3 - 1 = 9 - 1 = 8\n",
      "So, the three consecutive integers are 8, 9, 10.\n",
      "\n",
      "\n",
      "__________________\n",
      "What is the smallest positive perfect cube that can be written as the sum of three consecutive integers? Please reason step by step, and put your final answer within \\boxed{{}}. Okay, so I need to find the smallest positive perfect cube that can be written as the sum of three consecutive integers. Hmm, let's break this down step by step. \n",
      "\n",
      "First, let's clarify what is meant by consecutive integers. Consecutive integers are numbers that follow each other in order without any gaps. So, if I denote the first integer as n, then the next two consecutive integers would be n+1 and n+2. Therefore, the sum of these three integers would be n + (n+1) + (n+2).\n",
      "\n",
      "Let me write that down:\n",
      "\n",
      "Sum = n + (n + 1) + (n + 2)\n",
      "\n",
      "Simplifying this expression:\n",
      "\n",
      "Sum = n + n + 1 + n + 2\n",
      "Sum = 3n + 3\n",
      "\n",
      "So, the sum simplifies to 3n + 3. Hmm, that's 3(n + 1). Interesting. So, the sum is three times (n + 1). \n",
      "\n",
      "Now, the problem states that this sum must be a perfect cube. A perfect cube is a number that can be expressed as some integer raised to the power of three. So, we're looking for a perfect cube, let's call it C, such that C = 3(n + 1). \n",
      "\n",
      "Since C is a perfect cube, it can be written as k\\u00b3 where k is a positive integer. So, we have:\n",
      "\n",
      "k\\u00b3 = 3(n + 1)\n",
      "\n",
      "Which means that 3 divides k\\u00b3. Since 3 is a prime number, if 3 divides k\\u00b3, then 3 must divide k. So, let me write that down: k = 3m where m is an integer. \n",
      "\n",
      "Substituting back into the equation:\n",
      "\n",
      "(3m)\\u00b3 = 3(n + 1)\n",
      "27m\\u00b3 = 3(n + 1)\n",
      "\n",
      "Divide both sides by 3:\n",
      "\n",
      "9m\\u00b3 = n + 1\n",
      "\n",
      "So, n + 1 = 9m\\u00b3, which implies that n = 9m\\u00b3 - 1.\n",
      "\n",
      "Therefore, the three consecutive integers are:\n",
      "\n",
      "First integer: n = 9m\\u00b3 - 1\n",
      "Second integer: n + 1 = 9m\\u00b3\n",
      "Third integer: n + 2 = 9m\\u00b3 + 1\n",
      "\n",
      "Let me check with m = 1:\n",
      "\n",
      "n = 9(1)\\u00b3 - 1 = 9 - 1 = 8\n",
      "So, the three consecutive integers are 8, 9, 10.\n",
      "\n",
      "To find the smallest positive perfect cube that can be\n",
      "[{'token_id': [1249, 32313, 71486], 'probs': [0.3065232038497925, 0.47901102900505066, 0.21446575224399567], 'token': ['To', 'Okay', 'Alright']}, {'token_id': [1477], 'probs': [1.0], 'token': [' find']}, {'token_id': [279], 'probs': [1.0], 'token': [' the']}, {'token_id': [24632], 'probs': [1.0], 'token': [' smallest']}, {'token_id': [6785], 'probs': [1.0], 'token': [' positive']}, {'token_id': [4727], 'probs': [1.0], 'token': [' perfect']}, {'token_id': [23739], 'probs': [1.0], 'token': [' cube']}, {'token_id': [429], 'probs': [1.0], 'token': [' that']}, {'token_id': [646], 'probs': [1.0], 'token': [' can']}, {'token_id': [387], 'probs': [1.0], 'token': [' be']}]\n",
      "__________________\n"
     ]
    }
   ],
   "source": [
    "from explore_diff2 import decoding_chat\n",
    "\n",
    "question = \"What is the smallest positive perfect cube that can be written as the sum of three consecutive integers? Please reason step by step, and put your final answer within \\\\boxed{{}}. Okay, so I need to find the smallest positive perfect cube that can be written as the sum of three consecutive integers. Hmm, let's break this down step by step. \\n\\nFirst, let's clarify what is meant by consecutive integers. Consecutive integers are numbers that follow each other in order without any gaps. So, if I denote the first integer as n, then the next two consecutive integers would be n+1 and n+2. Therefore, the sum of these three integers would be n + (n+1) + (n+2).\\n\\nLet me write that down:\\n\\nSum = n + (n + 1) + (n + 2)\\n\\nSimplifying this expression:\\n\\nSum = n + n + 1 + n + 2\\nSum = 3n + 3\\n\\nSo, the sum simplifies to 3n + 3. Hmm, that's 3(n + 1). Interesting. So, the sum is three times (n + 1). \\n\\nNow, the problem states that this sum must be a perfect cube. A perfect cube is a number that can be expressed as some integer raised to the power of three. So, we're looking for a perfect cube, let's call it C, such that C = 3(n + 1). \\n\\nSince C is a perfect cube, it can be written as k\\\\u00b3 where k is a positive integer. So, we have:\\n\\nk\\\\u00b3 = 3(n + 1)\\n\\nWhich means that 3 divides k\\\\u00b3. Since 3 is a prime number, if 3 divides k\\\\u00b3, then 3 must divide k. So, let me write that down: k = 3m where m is an integer. \\n\\nSubstituting back into the equation:\\n\\n(3m)\\\\u00b3 = 3(n + 1)\\n27m\\\\u00b3 = 3(n + 1)\\n\\nDivide both sides by 3:\\n\\n9m\\\\u00b3 = n + 1\\n\\nSo, n + 1 = 9m\\\\u00b3, which implies that n = 9m\\\\u00b3 - 1.\\n\\nTherefore, the three consecutive integers are:\\n\\nFirst integer: n = 9m\\\\u00b3 - 1\\nSecond integer: n + 1 = 9m\\\\u00b3\\nThird integer: n + 2 = 9m\\\\u00b3 + 1\\n\\nLet me check with m = 1:\\n\\nn = 9(1)\\\\u00b3 - 1 = 9 - 1 = 8\\nSo, the three consecutive integers are 8, 9, 10.\\n\\n\" #Sum: 8 + 9 + 10 = 27\\n\\n27 is indeed a perfect cube (3\\\\u00b3). So, that seems to work. \"\n",
    "response_text, corrected_tokens, num_tokens, record_tokens, avg_probabilities = decoding_chat(\n",
    "    target_model, tokenizer, question, '', \n",
    "    temperature=0.7, topk=0, topp=0.95, max_tokens=10\n",
    ")\n",
    "print(question)\n",
    "print('__________________')\n",
    "print(question+response_text)\n",
    "print(record_tokens)\n",
    "print('__________________')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lceg2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
