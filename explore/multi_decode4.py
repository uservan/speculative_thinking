import __init__
import torch
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer, Qwen2ForCausalLM
import json
from utils.data_utils import load_data,save_all_results, read_saved_results, save_results
from utils.utils import *
from tqdm import tqdm
import time
import argparse
import pickle
from generate import generate_with_partial_kv, generate_hf
import copy
from concurrent.futures import ThreadPoolExecutor, as_completed
from utils.qwen_math_parser import *

def check_math_correctness(ref, generation):
    if not find_box(generation): return False
    answer = strip_answer_string(ref)
    pred = extract_answer(generation)
    pred = strip_answer_string(pred)
    return math_equal(pred, answer)

def sentiment_analysis(text, positive_words, negative_words):
    positive_count = 0
    negative_count = 0
    last_pos_index = -1
    last_neg_index = -1

    text = text.lower()  # è½¬æ¢ä¸ºå°å†™ä»¥ç¡®ä¿åŒ¹é…ä¸åŒºåˆ†å¤§å°å†™
    for word in positive_words:
        if word in text:
            positive_count += text.count(word)  # ç»Ÿè®¡å‡ºç°æ¬¡æ•°
            last_pos_index = max(last_pos_index, text.rfind(word))  # è®°å½•æœ€åå‡ºç°çš„ä½ç½®

    for word in negative_words:
        if word in text:
            negative_count += text.count(word)  # ç»Ÿè®¡å‡ºç°æ¬¡æ•°
            last_neg_index = max(last_neg_index, text.rfind(word))  # è®°å½•æœ€åå‡ºç°çš„ä½ç½®

    if positive_count > negative_count:
        return 1  # æ­£å‘
    elif negative_count > positive_count:
        return -1  # è´Ÿå‘
    elif positive_count == negative_count and positive_count > 0:
        return -1 # 1 if last_pos_index > last_neg_index else -1  # å–æœ€é åçš„è¯ç±»åˆ«
    else:
        return 0  # ä¸­ç«‹

def load_train_data(choose_data_name):
    split = 'train' if choose_data_name not in ['MATH500'] else 'test'
    choose_data_name = dataset_names[choose_data_name]
    choose_data = load_data(choose_data_name, 'huggingface')
    choose_data = choose_data[split]
    if 'MATH500' in choose_data_name or 'aime' in choose_data_name:
        new_column_names = {"problem": "question"}
    choose_data = choose_data.rename_columns(new_column_names)
    return choose_data

def contains_keywords(text, keywords):
    return any(keyword in text.lower() for keyword in keywords)

def speculative_generate(
    speculative_model, target_model, tokenizer, input_ids, help_think_word_ids,help_recap_words_ids,
    max_tokens=100, max_target_tokens=10, temperature=1.0, top_k=50, top_p=0.95
):
    try_correct_num = 0
    spec_kv=None
    tgt_kv=None
    prompt_len = input_ids.shape[1]
    generated_ids = input_ids  # ç›´æ¥å­˜å‚¨ token ids
    correct_tokens = []
    begin,change_flag = False, False
    negative_sent_num, recap_after_negtive_num, original_recap_token_num, begin_token_num, add_each_recap = 0, 10, 100, 100, 50
    recap_token_num = original_recap_token_num
    while generated_ids.shape[1] < max_tokens:  # **ä¸å†æ‰‹åŠ¨æ£€æŸ¥ max_tokens**
        if not begin:
            # **Step 1: Speculative Model é€ä¸ªç”Ÿæˆ 1 ä¸ª token**
            new_ids, spec_kv = generate_with_partial_kv(
                speculative_model, tokenizer, generated_ids, spec_kv,
                max_new_tokens=1, temperature=temperature, top_k=top_k, top_p=top_p
            )
            # **æ›´æ–° token åºåˆ—**
            generated_ids = new_ids # torch.cat([generated_ids, new_ids[:, -1:]], dim=-1)
            # **Step 2: ä»…è§£ç æ–° token å¹¶æ£€æŸ¥æ˜¯å¦è§¦å‘ Target Model**
            decoded_text = tokenizer.decode(new_ids[0, -1:], skip_special_tokens=True)
            
        if begin or any(trigger in decoded_text for trigger in TRIGGER_TOKENS):
            # 1. åˆ¤æ–­è´Ÿå‘æ¬¡æ•°æ˜¯å¦è¿‡å¤š
            # 2. åˆ¤æ–­æ˜¯å¦beginæ˜¯å¦éœ€è¦å¤„ç†
            # 3. åˆ¤æ–­æ˜¯å¦å‡ºç°åç›¸
            if begin:
                change_tokens = begin_token_num
                begin = False
                change_flag = True
            # elif negative_sent_num >= recap_after_negtive_num:
            #     generated_ids = torch.cat([generated_ids, help_recap_words_ids], dim=-1)
            #     change_tokens = recap_token_num
            #     change_flag = True
            #     negative_sent_num = 0
            #     recap_token_num, recap_after_negtive_num= min(recap_token_num + add_each_recap, 500), min(recap_after_negtive_num+15, 50)
            else:
                if help_think_word_ids is not None:
                    cache_generated_ids = torch.cat([generated_ids, help_think_word_ids], dim=-1)
                else:
                    cache_generated_ids = generated_ids
                spe_new_ids, spec_kv_candidate = generate_with_partial_kv(
                    speculative_model, tokenizer, cache_generated_ids, copy.deepcopy(spec_kv),
                    max_new_tokens=max_target_tokens, temperature=temperature, top_k=top_k, top_p=top_p
                )
                spe_decoded_text = tokenizer.decode(spe_new_ids[0,-max_target_tokens:], skip_special_tokens=True)
                spe_sent = sentiment_analysis(spe_decoded_text, TARGET_VALIDATION_KEYWORDS['positive'], TARGET_VALIDATION_KEYWORDS['negative']+TARGET_VALIDATION_KEYWORDS['verify'])
                if spe_sent != 0:
                    try_correct_num = try_correct_num+1
                    # **Step 3: ç›®æ ‡æ¨¡å‹ç”Ÿæˆ max_target_tokens ä¸ª token**
                    tgt_new_ids, tgt_kv_candidate = generate_with_partial_kv(
                        target_model, tokenizer, cache_generated_ids, copy.deepcopy(tgt_kv),
                        max_new_tokens=max_target_tokens, temperature=temperature, top_k=top_k, top_p=top_p
                    )
                    # **è§£ç  Target Model ç”Ÿæˆçš„æ–‡æœ¬**
                    tgt_decoded_text = tokenizer.decode(tgt_new_ids[0,-max_target_tokens:], skip_special_tokens=True)
                    tgt_sent = sentiment_analysis(tgt_decoded_text, TARGET_VALIDATION_KEYWORDS['positive'], TARGET_VALIDATION_KEYWORDS['negative']+TARGET_VALIDATION_KEYWORDS['verify'])
                    if True: #(spe_sent<0 and tgt_sent >=0) or (spe_sent>0 and tgt_sent<0):
                        generated_ids = tgt_new_ids # torch.cat([cache_generated_ids, tgt_new_ids[:, :]], dim=-1)  # âœ… æ¥å— Target Model ç»“æœ
                        tgt_kv = tgt_kv_candidate  # âœ… åªæœ‰åœ¨æ¥å— Target Model ç»“æœæ—¶æ‰æ›´æ–° `tgt_kv`
                        decode_text = tgt_decoded_text
                        correct_tokens.append({
                            'pos': cache_generated_ids.shape[1]-prompt_len, 'token_num':max_target_tokens,
                            'traget':tgt_decoded_text, 'speculative':spe_decoded_text})
                        final_sent=tgt_sent
                    else:
                        generated_ids = spe_new_ids # torch.cat([cache_generated_ids, tgt_new_ids[:, :]], dim=-1)  # âœ… æ¥å— Target Model ç»“æœ
                        spec_kv = spec_kv_candidate  # âœ… åªæœ‰åœ¨æ¥å— Target Model ç»“æœæ—¶æ‰æ›´æ–° `tgt_kv`
                        decode_text = spe_decoded_text
                        final_sent=spe_sent
                    if final_sent < 0: negative_sent_num = negative_sent_num+1
                    if contains_keywords(decode_text, TARGET_VALIDATION_KEYWORDS['verify']):
                        change_tokens = original_recap_token_num
                        change_flag = True
                        # negative_sent_num = 0
            # if change_flag:
            #     try_correct_num = try_correct_num+1
            #     tgt_new_ids, tgt_kv_candidate = generate_with_partial_kv(
            #         target_model, tokenizer, generated_ids, copy.deepcopy(tgt_kv_candidate),
            #         max_new_tokens=change_tokens, temperature=temperature, top_k=top_k, top_p=top_p
            #     )
            #     tgt_decoded_text = tokenizer.decode(tgt_new_ids[0,-change_tokens:], skip_special_tokens=True)
            #     generated_ids = tgt_new_ids
            #     tgt_kv = tgt_kv_candidate
            #     correct_tokens.append({
            #         'pos': generated_ids.shape[1]-prompt_len, 'token_num':change_tokens,
            #         'traget':tgt_decoded_text, 'speculative':spe_decoded_text})
            #     change_flag = False
        
        if tgt_kv is not None and tgt_kv[0][0].shape[2] > len(generated_ids[0]):
            print("wrong")
            
        # **Step 5: ç»ˆæ­¢æ¡ä»¶**
        # last_token_id = generated_ids[0, -1].item()
        if tokenizer.eos_token_id in generated_ids[0, -max_target_tokens:].tolist():  # âœ… è®© `generate()` å¤„ç†ç»ˆæ­¢
            break

    # **æœ€ç»ˆè§£ç æ–‡æœ¬**
    generated_text = tokenizer.decode(generated_ids[0,prompt_len:], skip_special_tokens=True)
    return generated_text, len(generated_ids[0])-prompt_len, correct_tokens, try_correct_num


def parse_args(args=None):
    parser = argparse.ArgumentParser()
    parser.add_argument('--start', type=int, default=0)
    parser.add_argument('--end', type=int, default=90)
    parser.add_argument('--dataset', type=str, default='AIME')
    parser.add_argument('--target_model', type=str, default='deepseek-32b')
    parser.add_argument('--speculative_model', type=str, default='deepseek-1.5b') 
    parser.add_argument('--speculative_k', type=int, default=20)
    parser.add_argument('--accept_prob', type=float, default=0.01)
    parser.add_argument('--temperature', type=float, default=0.6)
    parser.add_argument('--topp', type=float, default=0.95)
    parser.add_argument('--topk', type=float, default=0)
    parser.add_argument('--max_tokens', type=int, default=1024*32)
    parser.add_argument('--seed', type=int, default=42)
    return parser.parse_args(args)

dataset_names = {
    'MATH500': "qq8933/MATH500",
    'AIME': "AI-MO/aimo-validation-aime"
}

models_names = {
    'deepseek-32b': "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
    'deepseek-1.5b': "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
     'deepseek-7b': "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
    'Qwen-math-1.5b':'Qwen/Qwen2.5-Math-1.5B'
}


if __name__ == '__main__':
    args = parse_args()
    seed_everything(args.seed)
    # accept_prob = args.accept_prob
    # # **åŠ è½½ Hugging Face æ¨¡å‹ï¼ˆæ”¯æŒ KV Cache + å¤š GPUï¼‰**
    # target_model_name = models_names[args.target_model]  
    # target_model = AutoModelForCausalLM.from_pretrained(
    #     target_model_name, torch_dtype=torch.float16,device_map="auto", low_cpu_mem_usage=True, attn_implementation="flash_attention_2",
    # )
    # tokenizer = AutoTokenizer.from_pretrained(target_model_name)
    # speculative_model = None
    # if args.speculative_model:
    #     speculative_model_name = models_names[args.speculative_model] 
    #     speculative_model = AutoModelForCausalLM.from_pretrained(
    #         speculative_model_name,  torch_dtype=torch.float16, device_map="auto", low_cpu_mem_usage=True,attn_implementation="flash_attention_2",
    #     )
   

# **è§¦å‘ Token & å…³é”®è¯**
TRIGGER_TOKENS = {"\n\n"}  # é‡åˆ°è¿™äº› token è§¦å‘ Target Model
TARGET_VALIDATION_KEYWORDS = {'verify':['verify', 'think again', 'recap', 'check'],
                              "negative":['but', 'wait', "alternatively", 'hold on','another'],
                              "positive":['yeah','yes','final answer','confident']}  # ç›®æ ‡å…³é”®å­—
help_think_word = None # '\n\n'
system_prompt = None
device = "cuda" if torch.cuda.is_available() else "cpu"
# **åŠ è½½ Speculative Model å’Œ Target Model**
target_model_name = models_names[args.target_model]  # ç›®æ ‡æ¨¡å‹

tokenizer = AutoTokenizer.from_pretrained(target_model_name)
if tokenizer.pad_token_id is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id 
target_model = [ AutoModelForCausalLM.from_pretrained(target_model_name, torch_dtype=torch.float16, device_map="auto") for i in range(1)]
speculative_model = [None]
if args.speculative_model is not None: 
    speculative_model_name = models_names[args.speculative_model]  # Speculative Model (æ›´å°çš„æ¨¡å‹)
    speculative_model = [AutoModelForCausalLM.from_pretrained(speculative_model_name, torch_dtype=torch.float16, device_map="auto") for i in range(1)]
help_think_word_ids = None if help_think_word is None else tokenizer([help_think_word], return_tensors="pt").input_ids.to("cuda")
# Let me summarize and recap to make sure I didn't make any mistakes
help_recap_words_ids = tokenizer(["Let me shortly summarize and check previous thoughts to make sure I didn't make any mistakes"], return_tensors="pt").input_ids.to("cuda")
datasets = args.dataset.split(',')

start,end = args.start, args.end

for dataset in datasets:
    math500_dataset = load_train_data(dataset).select(range(start,end))
    output_file = f"./results/{dataset}_{args.target_model}_{args.speculative_model}_new_{start}_{end}.json"

    results = read_saved_results(output_file)
    idxs = {r['index'] for r in results}
    remaining_data = math500_dataset# .select(range(len(math500_dataset)))

    def process_sample(idx, sample):
        """å¤„ç†å•ä¸ªæ ·æœ¬ï¼Œå¹¶è¿”å›ç´¢å¼•ï¼Œç¡®ä¿é¡ºåº"""
        question, answer = sample["question"], sample["answer"]
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({
            "role": "user",
            "content": "Please reason step by step, and put your final answer within \\boxed{{}}. " + question + ' <think>\n'
        })
        input_ids = tokenizer.apply_chat_template(messages, return_tensors="pt").to(device)

        start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
        speculative_model_, target_model_ = speculative_model[0], target_model[0]
        if speculative_model_ is not None:
           generated_text, num_tokens, correct_tokens,try_correct_num = speculative_generate(
                speculative_model_, target_model_, tokenizer, input_ids, help_think_word_ids,help_recap_words_ids,
                max_target_tokens=args.speculative_k, max_tokens=args.max_tokens,
                temperature=args.temperature, top_p=args.topp
            )
        else:
            prompt_len = input_ids.shape[1]
            generated_ids_hf = generate_hf(target_model_, tokenizer, input_ids, args.max_tokens,
                        temperature=args.temperature, top_p=args.topp)
            generated_text = tokenizer.decode(generated_ids_hf[0,prompt_len:], skip_special_tokens=True)
            num_tokens, correct_tokens,try_correct_num = None, None, None
        end_time = time.time()  # è®°å½•ç»“æŸæ—¶é—´
        generation_time = end_time - start_time  # è®¡ç®—ç”Ÿæˆæ—¶é—´
        return {
            "index": idx,  # è®°å½•åŸå§‹ç´¢å¼•ï¼Œç¡®ä¿æ’åº
            "question": question,
            "generated_text": generated_text,
            "answer": answer,
            "corrected_tokens": correct_tokens,
            "generation_time": generation_time,
            "num_tokens": num_tokens,
            "try_correct_num": try_correct_num
        }

    # çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
    max_workers = min(4, os.cpu_count())  # é™åˆ¶æœ€å¤§çº¿ç¨‹æ•°ï¼Œé˜²æ­¢è¶…è½½
    print(f"ğŸš€ è‡ªåŠ¨åˆ†é…çº¿ç¨‹æ•°: {max_workers}")
    # max_workers = 8  # æ ¹æ®æ˜¾å­˜æƒ…å†µè°ƒæ•´
    results_list = []  # ç”¨äºå­˜å‚¨è¿”å›çš„ future ç»“æœ
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(process_sample, idx, sample): idx for idx, sample in enumerate(remaining_data) if idx not in idxs}

        for future in tqdm(as_completed(futures), total=len(futures)):
            try:
                result = future.result()
                right_flag = check_math_correctness(result['answer'], result['generated_text'])
                print(start+result['idx']+1, ": ", right_flag)
                results_list.append(result)  # å…ˆå­˜å‚¨ï¼Œä¿è¯ç»“æœå®Œæ•´
            except Exception as e:
                print(f"Error processing sample {futures[future]}: {e}")

    # **ç¡®ä¿æœ€ç»ˆç»“æœæŒ‰ç´¢å¼•æ’åº**
    results_list = sorted(results_list, key=lambda x: x["index"])

    # **æŒ‰é¡ºåºå­˜å…¥ results å¹¶ä¿å­˜**
    for res in results_list:
        results.append(res)
        save_results(output_file, res)

print("æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")



# # **åˆå§‹åŒ–è¾“å…¥**
# prompt = "Once upon a time"
# input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to("cuda")
# help_think_word_ids = tokenizer([help_think_word], return_tensors="pt").input_ids.to("cuda")
# # **æ‰§è¡Œ Speculative Mode ç”Ÿæˆ**
# generated_text = speculative_generate(
#     speculative_model, target_model, tokenizer, input_ids, help_think_word_ids,
#     max_target_tokens=10
# )

# print("Final Generated Text:", generated_text)